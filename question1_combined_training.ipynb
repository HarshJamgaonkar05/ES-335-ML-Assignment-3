{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33d2237e",
   "metadata": {},
   "source": [
    "# Next-Word Prediction using MLP - Complete Training\n",
    "\n",
    "## Both Category I (Shakespeare) and Category II (Linux Kernel)\n",
    "\n",
    "### Instructions:\n",
    "1. Upload both `shakespeare_processed.pkl` and `linux_kernel_processed.pkl` to Kaggle dataset\n",
    "2. Update DATA_PATHS below if needed\n",
    "3. Enable GPU (Settings ‚Üí Accelerator ‚Üí GPU T4 x2)\n",
    "4. Run all cells\n",
    "\n",
    "### What This Notebook Does:\n",
    "- Trains Shakespeare model (Category I - Natural Language)\n",
    "- Trains Linux Kernel model (Category II - Structured Text)\n",
    "- Generates all visualizations and reports\n",
    "- Saves all models with checkpointing\n",
    "\n",
    "### Expected Time:\n",
    "- Total: 4-8 hours with GPU (both models)\n",
    "- Shakespeare: 2-4 hours\n",
    "- Linux Kernel: 2-4 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095b6fe",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546f95e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cc4fd6",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Define Model Architecture and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7175e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.LongTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class NextWordMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_length, hidden_dim, activation='relu', dropout=0.5):\n",
    "        super(NextWordMLP, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        input_dim = context_length * embedding_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded.view(embedded.size(0), -1)\n",
    "        \n",
    "        h1 = self.activation(self.fc1(embedded))\n",
    "        h1 = self.dropout(h1)\n",
    "        \n",
    "        h2 = self.activation(self.fc2(h1))\n",
    "        h2 = self.dropout(h2)\n",
    "        \n",
    "        output = self.fc3(h2)\n",
    "        return output\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        return self.embedding.weight.data.cpu().numpy()\n",
    "\n",
    "print(\"Model architecture defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def generate_text(model, seed_text, num_words, word_to_idx, idx_to_word, context_length, temperature=1.0, is_code=False):\n",
    "    model.eval()\n",
    "    \n",
    "    if is_code:\n",
    "        words = seed_text if isinstance(seed_text, list) else seed_text.split()\n",
    "    else:\n",
    "        words = seed_text.lower().split()\n",
    "    \n",
    "    generated = words.copy()\n",
    "    \n",
    "    for _ in range(num_words):\n",
    "        context = generated[-context_length:] if len(generated) >= context_length else generated\n",
    "        context_indices = [word_to_idx.get(w, word_to_idx['<UNK>']) for w in context]\n",
    "        \n",
    "        if len(context_indices) < context_length:\n",
    "            context_indices = [word_to_idx['<START>']] * (context_length - len(context_indices)) + context_indices\n",
    "        \n",
    "        context_tensor = torch.LongTensor([context_indices]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(context_tensor) / temperature\n",
    "            probs = F.softmax(output, dim=1)\n",
    "            predicted_idx = torch.multinomial(probs, 1).item()\n",
    "        \n",
    "        next_word = idx_to_word[predicted_idx]\n",
    "        if next_word == '<END>':\n",
    "            break\n",
    "        generated.append(next_word)\n",
    "    \n",
    "    return ' '.join(generated)\n",
    "\n",
    "print(\"Training and generation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed5e0f",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Shakespeare Model (Category I - Natural Language)\n",
    "\n",
    "## Task 1.1 & 1.2 - Shakespeare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7ea0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAKESPEARE_DATA_PATH = '/kaggle/input/shakespeare-processed/shakespeare_processed.pkl'\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING SHAKESPEARE DATASET (Category I)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "with open(SHAKESPEARE_DATA_PATH, 'rb') as f:\n",
    "    shakespeare_data = pickle.load(f)\n",
    "\n",
    "shak_X = shakespeare_data['X']\n",
    "shak_y = shakespeare_data['y']\n",
    "shak_vocab = shakespeare_data['vocab']\n",
    "shak_word_to_idx = shakespeare_data['word_to_idx']\n",
    "shak_idx_to_word = shakespeare_data['idx_to_word']\n",
    "shak_context_length = shakespeare_data['context_length']\n",
    "shak_vocab_stats = shakespeare_data['vocab_stats']\n",
    "\n",
    "print(\"\\nSHAKESPEARE DATASET STATISTICS (Task 1.1)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Vocabulary size: {shak_vocab_stats['vocab_size']:,}\")\n",
    "print(f\"Total training samples: {len(shak_X):,}\")\n",
    "print(f\"Context length: {shak_context_length}\")\n",
    "print(f\"Total words in corpus: {shak_vocab_stats['total_words']:,}\")\n",
    "print(f\"<UNK> percentage: {shak_vocab_stats.get('unk_percentage', 0):.2f}%\")\n",
    "\n",
    "print(\"\\n10 Most frequent words:\")\n",
    "for word, count in shak_vocab_stats['most_common']:\n",
    "    print(f\"  {word}: {count:,}\")\n",
    "\n",
    "print(\"\\n10 Least frequent words:\")\n",
    "for word, count in shak_vocab_stats['least_common']:\n",
    "    print(f\"  {word}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeda192",
   "metadata": {},
   "outputs": [],
   "source": [
    "shak_X_train, shak_X_val, shak_y_train, shak_y_val = train_test_split(\n",
    "    shak_X, shak_y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nData Split:\")\n",
    "print(f\"Training samples: {len(shak_X_train):,}\")\n",
    "print(f\"Validation samples: {len(shak_X_val):,}\")\n",
    "\n",
    "shak_train_dataset = TextDataset(shak_X_train, shak_y_train)\n",
    "shak_val_dataset = TextDataset(shak_X_val, shak_y_val)\n",
    "\n",
    "batch_size = 256\n",
    "shak_train_loader = DataLoader(shak_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "shak_val_loader = DataLoader(shak_val_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"\\nBatch size: {batch_size}\")\n",
    "print(f\"Training batches: {len(shak_train_loader)}\")\n",
    "print(f\"Validation batches: {len(shak_val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0793c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 512\n",
    "ACTIVATION = 'relu'\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "shak_model = NextWordMLP(\n",
    "    vocab_size=len(shak_vocab),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    context_length=shak_context_length,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    activation=ACTIVATION\n",
    "    dropout=0.5).to(device)\n",
    "\n",
    "shak_criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "shak_optimizer = optim.Adam(shak_model.parameters(), lr=LEARNING_RATE)\n",
    "shak_optimizer = optim.Adam(shak_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "shak_scheduler = optim.lr_scheduler.ReduceLROnPlateau(shak_optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "print(\"\\nSHAKESPEARE MODEL ARCHITECTURE (Task 1.2)\")\n",
    "print(\"=\"*70)\n",
    "print(shak_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in shak_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTRAINING SHAKESPEARE MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "shak_history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "shak_best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "PATIENCE = 10\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(shak_model, shak_train_loader, shak_criterion, shak_optimizer, device)\n",
    "    val_loss, val_acc = validate(shak_model, shak_val_loader, shak_criterion, device)\n",
    "    shak_scheduler.step(val_loss)\n",
    "    \n",
    "    shak_history['train_loss'].append(train_loss)\n",
    "    shak_history['train_acc'].append(train_acc)\n",
    "    shak_history['val_loss'].append(val_loss)\n",
    "    shak_history['val_acc'].append(val_acc)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    if val_loss < shak_best_val_loss:\n",
    "        shak_best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': shak_model.state_dict(),\n",
    "            'optimizer_state_dict': shak_optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'vocab': shak_vocab,\n",
    "            'word_to_idx': shak_word_to_idx,\n",
    "            'idx_to_word': shak_idx_to_word,\n",
    "            'context_length': shak_context_length,\n",
    "            'embedding_dim': EMBEDDING_DIM,\n",
    "            'hidden_dim': HIDDEN_DIM,\n",
    "            'activation': ACTIVATION\n",
    "        }, 'shakespeare_best_model.pth')\n",
    "        print(f\"  Saved best model\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': shak_model.state_dict(),\n",
    "            'optimizer_state_dict': shak_optimizer.state_dict(),\n",
    "            'history': shak_history\n",
    "        }, f'shakespeare_checkpoint_epoch_{epoch+1}.pth')\n",
    "        print(f\"  Checkpoint saved\")\n",
    "\n",
    "print(\"\\nShakespeare training completed!\")\n",
    "print(f\"Best Validation Loss: {shak_best_val_loss:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {shak_history['val_acc'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabaec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Shakespeare training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(shak_history['train_loss'], label='Train Loss')\n",
    "ax1.plot(shak_history['val_loss'], label='Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Shakespeare - Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(shak_history['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(shak_history['val_acc'], label='Validation Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Shakespeare - Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('shakespeare_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5296dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shakespeare text generation\n",
    "checkpoint = torch.load('shakespeare_best_model.pth')\n",
    "shak_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "shak_model.eval()\n",
    "\n",
    "print(\"\\n SHAKESPEARE TEXT GENERATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_seeds = [\"to be or not to\", \"what is the\", \"i am\", \"the king of\"]\n",
    "\n",
    "for seed in test_seeds:\n",
    "    print(f\"\\nSeed: '{seed}'\")\n",
    "    for temp in [0.5, 1.0, 1.5]:\n",
    "        generated = generate_text(shak_model, seed, 20, shak_word_to_idx, shak_idx_to_word, \n",
    "                                 shak_context_length, temp, is_code=False)\n",
    "        print(f\"  T={temp}: {generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b0c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shakespeare embedding visualization (Task 1.3)\n",
    "print(\"\\n SHAKESPEARE EMBEDDING VISUALIZATION (Task 1.3)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "shak_embeddings = shak_model.get_embeddings()\n",
    "\n",
    "# Select words to visualize\n",
    "words_to_viz = [w for w, c in shak_vocab_stats['most_common'][:50]]\n",
    "interesting = ['king', 'queen', 'love', 'hate', 'good', 'evil', 'man', 'woman', 'life', 'death']\n",
    "words_to_viz.extend([w for w in interesting if w in shak_word_to_idx])\n",
    "words_to_viz = list(set(words_to_viz))[:100]\n",
    "\n",
    "word_indices = [shak_word_to_idx[w] for w in words_to_viz]\n",
    "word_embeddings = shak_embeddings[word_indices]\n",
    "\n",
    "print(\"Applying t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(word_embeddings)\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5)\n",
    "for i, word in enumerate(words_to_viz):\n",
    "    plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=8, alpha=0.7)\n",
    "\n",
    "plt.title('t-SNE Visualization of Word Embeddings (Shakespeare)', fontsize=16)\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('shakespeare_embeddings_tsne.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\" Visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a5b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Shakespeare artifacts\n",
    "with open('shakespeare_training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(shak_history, f)\n",
    "\n",
    "print(\"\\n SHAKESPEARE MODEL COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"Saved files:\")\n",
    "print(\"  ‚Ä¢ shakespeare_best_model.pth\")\n",
    "print(\"  ‚Ä¢ shakespeare_training_curves.png\")\n",
    "print(\"  ‚Ä¢ shakespeare_embeddings_tsne.png\")\n",
    "print(\"  ‚Ä¢ shakespeare_training_history.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6aef8e",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Linux Kernel Model (Category II - Structured Text)\n",
    "\n",
    "## Task 1.1 & 1.2 - Linux Kernel Code Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f55514",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LINUX_DATA_PATH = '/kaggle/input/linux-kernel-processed/linux_kernel_processed.pkl'\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING LINUX KERNEL DATASET (Category II)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "with open(LINUX_DATA_PATH, 'rb') as f:\n",
    "    linux_data = pickle.load(f)\n",
    "\n",
    "linux_X = linux_data['X']\n",
    "linux_y = linux_data['y']\n",
    "linux_vocab = linux_data['vocab']\n",
    "linux_word_to_idx = linux_data['word_to_idx']\n",
    "linux_idx_to_word = linux_data['idx_to_word']\n",
    "linux_context_length = linux_data['context_length']\n",
    "linux_vocab_stats = linux_data['vocab_stats']\n",
    "\n",
    "print(\"\\n LINUX KERNEL DATASET STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Vocabulary size: {linux_vocab_stats['vocab_size']:,}\")\n",
    "print(f\"Total training samples: {len(linux_X):,}\")\n",
    "print(f\"Context length: {linux_context_length}\")\n",
    "print(f\"Total tokens in corpus: {linux_vocab_stats['total_tokens']:,}\")\n",
    "print(f\"<UNK> percentage: {linux_vocab_stats.get('unk_percentage', 0):.2f}%\")\n",
    "\n",
    "print(\"\\n10 Most frequent tokens:\")\n",
    "for token, count in linux_vocab_stats['most_common']:\n",
    "    print(f\"  {repr(token)}: {count:,}\")\n",
    "\n",
    "print(\"\\n10 Least frequent tokens:\")\n",
    "for token, count in linux_vocab_stats['least_common']:\n",
    "    print(f\"  {repr(token)}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b90d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "linux_X_train, linux_X_val, linux_y_train, linux_y_val = train_test_split(\n",
    "    linux_X, linux_y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\n Data Split:\")\n",
    "print(f\"Training samples: {len(linux_X_train):,}\")\n",
    "print(f\"Validation samples: {len(linux_X_val):,}\")\n",
    "\n",
    "linux_train_dataset = TextDataset(linux_X_train, linux_y_train)\n",
    "linux_val_dataset = TextDataset(linux_X_val, linux_y_val)\n",
    "\n",
    "linux_train_loader = DataLoader(linux_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "linux_val_loader = DataLoader(linux_val_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"\\nBatch size: {batch_size}\")\n",
    "print(f\"Training batches: {len(linux_train_loader)}\")\n",
    "print(f\"Validation batches: {len(linux_val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b09fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "linux_model = NextWordMLP(\n",
    "    vocab_size=len(linux_vocab),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    context_length=linux_context_length,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    activation=ACTIVATION\n",
    ").to(device)\n",
    "\n",
    "linux_criterion = nn.CrossEntropyLoss()\n",
    "linux_optimizer = optim.Adam(linux_model.parameters(), lr=LEARNING_RATE)\n",
    "linux_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    linux_optimizer, mode='min', factor=0.5, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n LINUX KERNEL MODEL ARCHITECTURE (Task 1.2)\")\n",
    "print(\"=\"*70)\n",
    "print(linux_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in linux_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8638a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n TRAINING LINUX KERNEL MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "linux_history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "linux_best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(linux_model, linux_train_loader, linux_criterion, linux_optimizer, device)\n",
    "    val_loss, val_acc = validate(linux_model, linux_val_loader, linux_criterion, device)\n",
    "    linux_scheduler.step(val_loss)\n",
    "    \n",
    "    linux_history['train_loss'].append(train_loss)\n",
    "    linux_history['train_acc'].append(train_acc)\n",
    "    linux_history['val_loss'].append(val_loss)\n",
    "    linux_history['val_acc'].append(val_acc)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    if val_loss < linux_best_val_loss:\n",
    "        linux_best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': linux_model.state_dict(),\n",
    "            'optimizer_state_dict': linux_optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'vocab': linux_vocab,\n",
    "            'word_to_idx': linux_word_to_idx,\n",
    "            'idx_to_word': linux_idx_to_word,\n",
    "            'context_length': linux_context_length,\n",
    "            'embedding_dim': EMBEDDING_DIM,\n",
    "            'hidden_dim': HIDDEN_DIM,\n",
    "            'activation': ACTIVATION\n",
    "        }, 'linux_kernel_best_model.pth')\n",
    "        print(f\"  ‚úì Saved best model\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\n Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': linux_model.state_dict(),\n",
    "            'optimizer_state_dict': linux_optimizer.state_dict(),\n",
    "            'history': linux_history\n",
    "        }, f'linux_kernel_checkpoint_epoch_{epoch+1}.pth')\n",
    "        print(f\"  ‚úì Checkpoint saved\")\n",
    "\n",
    "print(\"\\n Linux Kernel training completed!\")\n",
    "print(f\"Best Validation Loss: {linux_best_val_loss:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {linux_history['val_acc'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b2d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(linux_history['train_loss'], label='Train Loss')\n",
    "ax1.plot(linux_history['val_loss'], label='Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Linux Kernel - Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(linux_history['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(linux_history['val_acc'], label='Validation Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Linux Kernel - Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('linux_kernel_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1969ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('linux_kernel_best_model.pth')\n",
    "linux_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "linux_model.eval()\n",
    "\n",
    "print(\"\\n LINUX KERNEL CODE GENERATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_seeds = [\n",
    "    ['struct', 'task_struct', '*'],\n",
    "    ['if', '('],\n",
    "    ['int', 'ret', '='],\n",
    "    ['static', 'void']\n",
    "]\n",
    "\n",
    "for seed in test_seeds:\n",
    "    print(f\"\\nSeed: {' '.join(seed)}\")\n",
    "    for temp in [0.5, 1.0, 1.5]:\n",
    "        generated = generate_text(linux_model, seed, 20, linux_word_to_idx, linux_idx_to_word, \n",
    "                                 linux_context_length, temp, is_code=True)\n",
    "        print(f\"  T={temp}: {generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc75c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n LINUX KERNEL EMBEDDING VISUALIZATION (Task 1.3)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "linux_embeddings = linux_model.get_embeddings()\n",
    "\n",
    "tokens_to_viz = [t for t, c in linux_vocab_stats['most_common'][:50]]\n",
    "interesting = ['if', 'else', 'for', 'while', 'return', 'struct', 'int', 'void',\n",
    "               'static', 'const', 'unsigned', 'char', '*', '{', '}', '(', ')']\n",
    "tokens_to_viz.extend([t for t in interesting if t in linux_word_to_idx])\n",
    "tokens_to_viz = list(set(tokens_to_viz))[:100]\n",
    "\n",
    "token_indices = [linux_word_to_idx[t] for t in tokens_to_viz]\n",
    "token_embeddings = linux_embeddings[token_indices]\n",
    "\n",
    "print(\"Applying t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(token_embeddings)\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5)\n",
    "for i, token in enumerate(tokens_to_viz):\n",
    "    plt.annotate(token, (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=8, alpha=0.7)\n",
    "\n",
    "plt.title('t-SNE Visualization of Token Embeddings (Linux Kernel Code)', fontsize=16)\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('linux_kernel_embeddings_tsne.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úì Visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0351089",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('linux_kernel_training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(linux_history, f)\n",
    "\n",
    "print(\"\\n LINUX KERNEL MODEL COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"Saved files:\")\n",
    "print(\"  ‚Ä¢ linux_kernel_best_model.pth\")\n",
    "print(\"  ‚Ä¢ linux_kernel_training_curves.png\")\n",
    "print(\"  ‚Ä¢ linux_kernel_embeddings_tsne.png\")\n",
    "print(\"  ‚Ä¢ linux_kernel_training_history.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee830d5d",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Comparative Analysis (Task 1.5)\n",
    "\n",
    "## Compare Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f66f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARATIVE ANALYSIS - SHAKESPEARE VS LINUX KERNEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n Dataset Comparison:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Metric':<30} {'Shakespeare':<20} {'Linux Kernel':<20}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Vocabulary Size':<30} {len(shak_vocab):>19,} {len(linux_vocab):>19,}\")\n",
    "print(f\"{'Training Samples':<30} {len(shak_X):>19,} {len(linux_X):>19,}\")\n",
    "print(f\"{'Context Length':<30} {shak_context_length:>19} {linux_context_length:>19}\")\n",
    "print(f\"{'<UNK> Percentage':<30} {shak_vocab_stats.get('unk_percentage', 0):>18.2f}% {linux_vocab_stats.get('unk_percentage', 0):>18.2f}%\")\n",
    "\n",
    "print(\"\\n Model Performance:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Metric':<30} {'Shakespeare':<20} {'Linux Kernel':<20}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Best Val Loss':<30} {shak_best_val_loss:>19.4f} {linux_best_val_loss:>19.4f}\")\n",
    "print(f\"{'Final Val Accuracy':<30} {shak_history['val_acc'][-1]:>18.2f}% {linux_history['val_acc'][-1]:>18.2f}%\")\n",
    "print(f\"{'Epochs Trained':<30} {len(shak_history['train_loss']):>19} {len(linux_history['train_loss']):>19}\")\n",
    "print(f\"{'Parameters':<30} {sum(p.numel() for p in shak_model.parameters()):>19,} {sum(p.numel() for p in linux_model.parameters()):>19,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Shakespeare loss\n",
    "ax1.plot(shak_history['train_loss'], label='Train', alpha=0.7)\n",
    "ax1.plot(shak_history['val_loss'], label='Val', alpha=0.7)\n",
    "ax1.set_title('Shakespeare - Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Linux loss\n",
    "ax2.plot(linux_history['train_loss'], label='Train', alpha=0.7, color='orange')\n",
    "ax2.plot(linux_history['val_loss'], label='Val', alpha=0.7, color='red')\n",
    "ax2.set_title('Linux Kernel - Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Shakespeare accuracy\n",
    "ax3.plot(shak_history['train_acc'], label='Train', alpha=0.7)\n",
    "ax3.plot(shak_history['val_acc'], label='Val', alpha=0.7)\n",
    "ax3.set_title('Shakespeare - Accuracy')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Accuracy (%)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Linux accuracy\n",
    "ax4.plot(linux_history['train_acc'], label='Train', alpha=0.7, color='orange')\n",
    "ax4.plot(linux_history['val_acc'], label='Val', alpha=0.7, color='red')\n",
    "ax4.set_title('Linux Kernel - Accuracy')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Accuracy (%)')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparison_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Comparison plot saved as 'comparison_training_curves.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d65b3c",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f11d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ ALL TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ Shakespeare Model (Category I):\")\n",
    "print(\"   ‚Ä¢ shakespeare_best_model.pth\")\n",
    "print(\"   ‚Ä¢ shakespeare_training_curves.png\")\n",
    "print(\"   ‚Ä¢ shakespeare_embeddings_tsne.png\")\n",
    "print(\"   ‚Ä¢ shakespeare_training_history.pkl\")\n",
    "\n",
    "print(\"\\n‚úÖ Linux Kernel Model (Category II):\")\n",
    "print(\"   ‚Ä¢ linux_kernel_best_model.pth\")\n",
    "print(\"   ‚Ä¢ linux_kernel_training_curves.png\")\n",
    "print(\"   ‚Ä¢ linux_kernel_embeddings_tsne.png\")\n",
    "print(\"   ‚Ä¢ linux_kernel_training_history.pkl\")\n",
    "\n",
    "print(\"\\n‚úÖ Comparison:\")\n",
    "print(\"   ‚Ä¢ comparison_training_curves.png\")\n",
    "\n",
    "print(\"\\nüìã Tasks Completed:\")\n",
    "print(\"   ‚úì Task 1.1: Preprocessing (both datasets)\")\n",
    "print(\"   ‚úì Task 1.2: Model training (both models)\")\n",
    "print(\"   ‚úì Task 1.3: Embedding visualization (both models)\")\n",
    "print(\"   ‚úì Task 1.5: Comparative analysis\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"   ‚Ä¢ Download all saved files from Kaggle\")\n",
    "print(\"   ‚Ä¢ Use models for Task 1.4 (Streamlit app)\")\n",
    "print(\"   ‚Ä¢ Write detailed analysis for assignment report\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Good luck with your assignment! üçÄ\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
