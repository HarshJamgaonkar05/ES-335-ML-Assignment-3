<UNK>
<START>
<END>
<NEWLINE>
*
=
{
if
}
the
*/
/*
struct
return
int
to
static
is
void
a
0;
of
for
and
unsigned
long
-
in
#include
==
we
be
that
&&
goto
it
this
ret
else
&
on
<
!=
char
not
+
case
#endif
||
are
with
This
The
ret;
#ifdef
const
break;
from
by
task_struct
If
1;
/**
return;
or
#define
NULL;
can
inline
will
0)
};
as
task
function
have
>
an
all
while
\
},
We
|
0,
which
bool
+=
0
-EINVAL;
do
CPU
has
u64
when
flags);
set
so
only
out;
no
must
at
but
cpu
__user
new
err
any
file
"
i
extern
need
called
data
(i
should
buffer
time
>=
up
:
one
used
lock
0);
rq
error
?
event
current
before
then
continue;
i;
1
after
state
use
page
may
size_t
flags;
i++)
its
number
err;
into
timer
value
NULL,
|=
CTL_INT,
size
u32
module
-ENOMEM;
#else
enum
was
(ret)
there
1)
next
already
false;
__init
tasks
RCU
call
interrupt
don't
1);
/
<<
seq_file
check
kernel
list
other
1,
get
just
NULL);
true;
perf_event
memory
trace_array
code
first
idle
cpu);
GFP_KERNEL);
Copyright
(unsigned
out:
than
switch
more
been
user
thread
cgroup
cpu)
<=
loff_t
because
group
same
-EFAULT;
entry
(ret
ssize_t
out
make
update
work
pointer
might
free
process
system
(C)
grace
caller
start
list_head
bit
our
*,
sure
end
does
being
Returns
cpu,
irq
also
see
last
now
flags,
trace
want
still
-1;
count
read
NULL
It
device
type
period
A
each
rcu_read_unlock();
*p,
irq_desc
error;
wait
cpu;
they
here
result
it.
some
retval
resource
&=
load
program
context
either
since
2
you
under
without
flags
p
len
>>
seq_printf(m,
0644,
(void
rcu_node
events
write
inode
CPUs
returns
-=
until
functions
pages
interrupts
*p)
node
list)
CONFIG_SMP
it's
*m,
In
rcu_read_lock();
#
NULL)
.mode
about
running
GNU
General
run
done
(err)
Public
where
pid
name
See
.procname
even
could
can't
timespec
ktime_t
handler
allocate
delta
parent
another
tracer
disabled
change
default:
i,
pt_regs
cpuset
stop
.data
License
For
through
whether
address
needs
clock
.maxlen
.proc_handler
avoid
cpumask
ring
cfs_rq
structure
we're
between
ring_buffer
try
specified
signal
doesn't
*rq,
copy
string
ftrace_ops
kprobe
#if
cannot
always
mask
callbacks
using
(struct
would
mutex
*event)
given
trace_iterator
Check
register
*buf,
When
during
off
int,
rcu_head
callback
take
notifier
worker
never
back
two
old
active
them
add
cgroup_subsys_state
domain
#ifndef
pending
sizeof(struct
hash
filter
zero
*data)
head
",
length
both
rcu_state
Return
associated
flags)
stack
flag
target
Free
record
tracing
*buffer,
allow
test
Make
offset
us
such
wake
define
their
version
command
jiffies
handle
values
means
per
space
SEQ_printf(m,
were
held
len;
task_group
calls
critical
No
dentry
point
p,
notifier_block
size;
retval;
*inode,
Note
ensure
hold
*event,
mode
ip,
later
via
priority
.name
currently
{}
create
modify
#undef
these
default
keep
enabled
hrtimer
Called
%d\n",
*p;
find
waiting
local
trace_seq
ftrace_event_call
access
go
clock_event_device
full
too
");
count;
buffer,
*ppos)
path
sched_entity
'\0';
*)
addr
perf_event_context
allocated
longer
size,
out_unlock;
otherwise
(1
clocksource
state.
order
%
tick
trigger
chain
Set
array
counter
file_operations
delta;
release
remove
waiter
safe
-EPERM;
cred
ops
ftrace_event_file
.read
IRQ
Update
profiling
cpus
possible
rcu_data
disable
len,
So
%d
root
valid
Software
threads
.open
against
rt_mutex
owner
know
reset
p);
image
queue
work_struct
lock.
audit
local_irq_restore(flags);
cnt,
going
*info)
.llseek
support
gcov_info
key
CPU.
match
irq_data
move
following
single
-EBUSY;
irq,
passed
(error)
n
Do
sleep
field
reader
console
current;
time.
happen
But
different
clear
*file)
*file,
buffer.
returned
data;
task.
true
completion
locks
*lock)
lock,
holding
count,
reference
2;
bits
*lock,
type,
what
__weak
rt_rq
sched_domain
<linux/slab.h>
Must
*tr,
ring_buffer_event
<linux/sched.h>
atomic_t
0))
loop
All
gets
Only
wakeup
Don't
name,
write,
disabled.
line
online
read-side
.release
*rq)
__sched
nothing
terms
set,
level
true);
contains
false);
%s\n",
like
val
*tr)
(!ret)
2)
There
period.
entries
here.
*v)
context.
-ENODEV;
local_irq_save(flags);
s64
found
your
within
section
(0)
On
success,
timeout
ctl_table
ring_buffer_per_cpu
index
long)
based
own
-1)
request
addr,
acquire
runtime
least
t
map
race
global
message
base
<linux/module.h>
and/or
needed
second
every
css
queued
hardware
Return:
boot
child
due
syscall
*attr,
<linux/export.h>
__read_mostly
tree
put
available
above
specific
calling
(char
trace_seq_printf(s,
breakpoint
probe
actually
tmp
locking
*entry;
exit
prevent
guaranteed
barrier
setting
To
way
timers
previous
details.
uses
seq_puts(m,
preempt_enable();
(flags
%s
*name,
Ingo
(;;)
over
allowed
max
i);
Remove
j
&flags);
failed
print
val,
quiescent
mode,
empty
buf
restart
'
*pos)
mm_struct
audit_log_format(ab,
irqs
val;
variable
implementation
bytes
*regs)
workqueue
symbol
enable
again
published
multiple
kthread
registered
event,
list.
long,
control
*s,
left
*tr
*iter,
(len
*data,
)
buf,
there's
buffers
audit_context
futex
redistribute
pass
store
preempt_disable();
changed
*tsk)
pc);
something
those
--
block
blocked
care
yet
*str)
Inc.,
info
core
ftrace
*filp,
sizeof(int),
software;
printk(KERN_ERR
fields
held.
*p
*event;
ptr
interval
normal
part
works
suspend
state,
Since
descriptor
pool
*desc)
userspace
here,
cputime_t
-1
id
.func
worker_pool
instead
updated
fail
res
range
right
function.
down
tmp;
once
visible
1))
broadcast
object
kdb
many
subsystem
set.
internal
doing
real
true,
len);
*ops,
Initialize
uprobe
workqueue_struct
Molnar
Wait
force
argument
total
now,
nodes
Find
*buf)
let
*work)
woken
result;
pid,
irq_domain
<linux/init.h>
really
cause
smp_mb();
(cpu
information
stored
n;
task,
source
idx
*iter)
required
As
top
isn't
rc
signals
routine
out_unlock:
len)
print_line_t
added
seq_read,
0444,
user_namespace
*dev,
*rq
class
format
iterator
lock_list
existing
"PM:
cnt;
did
preemption
Use
pos
cpumask_var_t
*mod)
now;
.extra1
css_set
timekeeper
CTL_DIR,
You
corresponding
*arg)
list);
data,
pid_t
trace_entry
unlock;
ctx);
held_lock
f->op,
Now
unused
initialize
started
Add
size)
p)
released
32
success
inside
event);
files
kobject
proc_dointvec,
buffer_page
implied
code.
scheduler
directly
freed
allocation
created
skip
scheduling
RT
p;
"#
how
*ubuf,
value.
<linux/kernel.h>
Pointer
executing
These
jiffies;
start,
won't
up.
size);
cftype
allows
thus
Red
waiters
drop
regs);
Get
readers
writer
hotplug
lock_class
addr;
diag
*data;
changes
hierarchy
*tsk,
scheduled
interface
hlist_head
chip
bin_table
hope
false
debug
checks
matching
state)
Returns:
enough
itself
Try
.write
blocking
deadlock
sig,
table
log
destination
alarm
k_itimer
distributed
warranty
shared
defined
3
initial
action
arch
attached
trace_event
@cpu:
time,
credentials
pmu
*timer)
pwq
along
written
>>=
GFP_KERNEL,
swap
sets
away
got
...
for_each_possible_cpu(cpu)
output
removed
cache
smp_processor_id();
*mod,
parent_ip,
dependency
pool_workqueue
original
execution
Ensure
most
atomic
maximum
*task)
.flags
*task,
filter_pred
-1,
timer_list
irqaction
ftrace_hash
channel
useful,
lockdep
initialized
CPU,
prev
mask,
res;
snapshot
points
it,
taken
audit_buffer
Some
version.
WITHOUT
ANY
WARRANTY;
MERCHANTABILITY
FITNESS
FOR
PARTICULAR
not,
invoked
mark
necessary
below
"\n");
schedule();
handlers
I
rc;
generic
type)
->
*regs,
instruction
accounting
*desc,
Note:
PURPOSE.
special
otherwise.
*next;
offset;
leave
end;
times
*rsp,
trying
*q,
(or
.start
*desc
futex_q
<linux/interrupt.h>
early
*);
sizeof(unsigned
Allocate
sum
one.
groups
u8
actual
Otherwise
look
Hat,
*info,
simply
non
sched
addr)
latency
*table,
NUMA
rw_semaphore
seq
timespec64
takes
high
makes
printk(KERN_INFO
negative
happens
operations
softirq
head,
sequence
failure
hit
extra
i)
semaphore
holds
@buffer:
Foundation;
increment
offline
pid;
much
*curr,
them.
structure.
reason
migration
4
power
schedule
again.
send
(err
around
At
seq_lseek,
func,
*lenp,
capacity
uid
again;
*kobj,
(sample_type
(at
very
t;
Number
node,
n)
reserved
handling
CONFIG_HOTPLUG_CPU
whole
checking
Handle
*s
event.
item
seconds
-ENOENT;
hard
off,
bandwidth
runqueue
printk("
__read_mostly;
diag;
sprintf(buf,
fast
ready
pid_namespace
(and
case,
tail
graph
GFP_KERNEL))
*ctx)
kobj_attribute
event_trigger_ops
sched_rt_entity
*timer,
After
structures
period,
list,
(j
anything
cgroup_subsys
delay
sigset_t
gcc
Call
mm
messages
domains
.extra2
*fmt,
locked
mask;
received
IBM
section.
(if
sections
next;
page.
cnt
now.
place
rt_mutex_waiter
;
convert
regs,
dump_stack();
offset,
local_irq_enable();
busy
.stop
us.
name);
weight
affinity
CONT;
workers
mapping
SRCU
which_clock,
driver
option)
unless
3)
CPUs.
guarantee
having
Because
directory
position
ignore
lower
printk(KERN_CONT
%u
cases
header
failure.
execute
Thomas
resolution
group.
union
search
data);
op
Create
count)
give
task's
frequency
nsec
load_info
kimage
resource_size_t
invoke
2);
someone
<linux/syscalls.h>
link
walk
everything
limit
requested
*tmp;
moved
wants
low
rb_node
processing
info,
(p
entry,
event_filter
fail;
sk_buff
smp_wmb();
bpf_map
less
ret);
*tg,
expires
&zero,
protected
clockid_t
event_trigger_data
License,
grace-period
had
prior
HZ
callbacks.
list;
goes
save
(state
That
namespace
attach
current);
init
raw
torture
10
action,
per-cpu
DEFINE_PER_CPU(struct
amount
cpu.
slot
sched_dl_entity
deadline
insn
trace_kprobe
perf_cpu_context
rchan_buf
val)
(which
<linux/cpu.h>
future
disabled,
newly
info);
(c)
remaining
children
runs
enter
NULL))
-ESRCH;
able
BUG();
sizeof
nr
file);
stats
statistics
filp->private_data;
n);
ppos,
usage
text
@irq:
expiry
fmt,
@work
sysfs
r;
scan
account
well
lockdep_map
Also
beginning
*t,
waits
complete
Inc.
for_each_online_cpu(cpu)
%ld\n",
(1)
j;
error.
completed
detect
outside
val);
*head;
It's
.next
value,
put_online_cpus();
updating
An
sched_group
flush
bitmap
parameter
memory_bitmap
iteration
copied
Otherwise,
freezer
<linux/seq_file.h>
NMI
handled
preempt
come
things
faults
trace_seq_puts(s,
local_irq_disable();
safely
adding
ns
bio
Interrupt
devices
retry;
effective
CONFIG_FAIR_GROUP_SCHED
*cfs_rq,
again:
va_list
ctx
acquired
proc_dointvec_minmax,
tracepoint
r
"\t
include
starting
<linux/mm.h>
marked
related
saved
large
Clear
correct
ticks
Is
*name;
therefore
expected
fails
comment
-EAGAIN;
requeue
exists
continue
tmp,
sd
become
seq_operations
v;
overflow
"",
*sem)
policy
node);
cycle_t
program;
executed
idle,
Each
(argc
system.
resume
commit
'\n');
including
track
zero.
cmd
notify
not.
siginfo
simple
*name)
&iter->seq;
-ENOSYS;
prio
exception
.show
trace_uprobe
migrate
recursion
*key)
irq)
seq;
file->private_data;
srcu_struct
barrier();
notrace
states
Start
best
end)
start;
additional
PAGE_SIZE
nr_pages
reading
element
<linux/uaccess.h>
tasks.
Used
this.
ret,
users
we'll
dump
type;
protect
requires
(!event)
cpu_relax();
setup
balancing
(retval)
*se)
*tk
dyn_ftrace
<linux/mutex.h>
guarantees
*t;
updates
argc,
kzalloc(sizeof(struct
help
ERR_PTR(-ENOMEM);
buf);
migrated
determine
transition
*css,
short
architecture
*t)
5
dropped
becomes
too.
And
tell
vm_area_struct
*call
instance
out_free;
raw_spin_unlock_irqrestore(&rnp->lock,
'%s'
d_tracer,
moving
fill
events.
process.
ppos);
iter
entity
capability
*cfs_rq)
depth
runnable
*data
timekeeping
unbound
futex_hash_bucket
__lockfunc
<linux/spinlock.h>
idle.
2009
NOTIFY_OK;
main
0.
PAGE_SIZE)
(size
count);
Not
writing
*task;
below.
CONTEXT:
parent,
report
checked
helper
operation
*file;
.init
break
done;
unlock:
2,
*v,
relative
parameters
l
balance
itimerspec
CONFIG_RT_GROUP_SCHED
ip);
area
lines
lock_class_key
context,
whose
fully
previously
WARN_ON(1);
info;
Kernel
*page;
((unsigned
hibernation
containing
file,
responsible
didn't
equal
__set_current_state(TASK_RUNNING);
architectures
provide
equivalent
across
except
calculate
rec
various
kgdb_state
input
shift
ensures
might_sleep();
*ptr
adjust
cpu_rq(cpu);
nice
audit_krule
audit_names
(u32)
*iter
*rt_rq)
ftrace_probe_ops
<linux/delay.h>
<linux/percpu.h>
subsequent
useful
show
First
patch
addr);
pointers
made
making
highmem
ktime_get();
(count
leaf
on.
Allow
taking
returning
10);
false,
Foundation,
filter_parse_state
^
invalid
percpu
buf;
better
curr
enabled.
.type
func
temporary
next,
matches
bit,
file.
(!desc)
-EACCES;
writes
tick_sched
workqueue_attrs
gcov_node
mutex_unlock(&trace_types_lock);
tvec_base
cpu_buffer
desc->istate
<linux/kthread.h>
Should
exiting
linked
minimum
<<=
j++)
<linux/fs.h>
*head,
offset);
zero,
handler,
bother
Gleixner
s
entering
thread.
group,
(pid
ts;
method
set)
SMP
*field;
ring_buffer_event_data(event);
pos);
(!retval)
rather
watchdog
sysctl
ever
irq_to_desc(irq);
<linux/kallsyms.h>
threaded
*curr
lookup
device.
audit_tree
*prev,
*dev)
m->private;
ip)
kernel_param
padata_instance
fact
callers
require
Disable
(in
systems
seen
Mark
(addr
<linux/string.h>
gfp_t
node;
index;
@task
finished
2008
override
first.
immediately
processor
pi
current,
%true
case.
rely
stopped
read_unlock(&tasklist_lock);
@info:
args);
pc
(;
trace_handle_return(s);
*entry,
noinline
eBPF
verifier
preemptible
Does
earlier
perf_sample_data
lenp,
Generic
unlock
iter;
dl_rq
resched_curr(rq);
retry:
audit_entry
monotonic
printk(KERN_WARNING
enabled,
oneshot
entry)
pi_state
*cs)
for_each_subsys(ss,
request_queue
cleared
returned.
shouldn't
bad
.notifier_call
few
finish
debugging
2006
entire
types
zone
initialization
compute
*=
loaded
indicates
set_current_state(TASK_INTERRUPTIBLE);
str
private
m
signal_struct
*tsk
Note,
8
condition
ID
c
bpf_prog
*parent,
Skip
ref
torture_param(int,
(error
*filp)
sample
*rsp)
get_online_cpus();
cpu));
functions.
possibly
hardirq
cpumask_clear_cpu(cpu,
throttled
processes
IPI
CONFIG_SCHED_DEBUG
(!cpumask_test_cpu(cpu,
cfs_bandwidth
that's
stat
regs
DST
printk
gcov_iterator
*ppos
*seq,
virq,
mutex_lock(&trace_types_lock);
key;
perf_output_put(handle,
&tk_core.timekeeper;
.%-30s:
called.
*addr,
effect
callback.
kernel.
fmt
CONFIG_MODULES
**argv)
(diag)
<asm/uaccess.h>
contents
*buffer;
(!error)
data.
likely
appropriate
cgroups
irq);
upon
machine
sig
trap
By
this,
status
itself.
return.
-EFAULT
implemented
(see
mutex_unlock(&event_mutex);
tr,
str,
up,
Caller
*entry
%d,
particular
*rdp)
*rnp)
fetch
note
higher
this_cpu
page,
delta);
*rq;
stays
se);
iterate
environment
records
platform
BPF_X]
BPF_K]
*tr);
CONFIG_FUNCTION_GRAPH_TRACER
prevents
synchronize_sched();
schedule()
final
implements
Print
KDB_ARGCOUNT;
i--)
bit;
modified
(n
src
nr_pages,
mask)
assume
lock;
node.
otherwise,
__percpu
races
1]
sent
periodic
*s)
(!ret
(!p)
tsk
implement
debugger
getting
dynamic
mutex_lock(&event_mutex);
param
instructions
<linux/ftrace.h>
syscall_metadata
symbols
10,
idx)
printing
firsterr
printed
completely
yet.
inode->i_private;
@data:
separate
arguments
err:
nested
changing
smp_mb__after_unlock_lock();
round
_RET_IP_);
rules
jump_entry
IRQs
seq));
Function
highest
kuid_t
audit_chunk
*ctx,
trace_array_cpu
trace_buffer
CONFIG_TRACER_MAX_TRACE
*trace,
*domain,
gid
character
perf_output_handle
Elf_Shdr
buffer->buffers[cpu];
numbers
<linux/smp.h>
user-space
per-CPU
complete.
ordering
conditions
vector
provides
conflict
One
insert
restore
use.
management
__visible
*parent
away.
Once
CPU's
tracing.
CONFIG_RCU_BOOST
NULL.
no-CBs
leader
explicitly
who
siginfo_t
While
CONFIG_COMPAT
(!err)
pid_t,
(val
entry.
string.
typedef
idx;
WARN(1,
programs
Convert
synchronize
(trace_flags
pos,
preempt_count();
num;
Reset
for_each_possible_cpu(i)
out.
option
work);
(delta
nr_cpu_ids)
-EIO;
value;
extended
Keep
Calculate
cleanup
*iter;
type);
...)
prepare
@dev:
fire
audit_watch
64
(cnt
entry);
kernel_symbol
in.
device_attribute
echo
timeout,
(event->state
futex_key
module_attribute
printk_log
*cpu_buffer;
padata
interrupts.
indicate
function,
clean
0644);
good
implies
t,
t)
past
modules
contain
NOTE:
x
positive
to.
state;
dead
necessary.
Author:
protects
Any
Force
why
capabilities
provided
kick
PI
detection
Nothing
steal
siglock
we've
held,
average
problem
call.
non-zero
recorded
wrong
MA
skb
slow
<linux/err.h>
starts
unwind;
raw_smp_processor_id();
arg);
cpus,
looking
timestamp
reschedule
enable)
@p:
now)
-deadline
nanoseconds
expires;
*parent)
*ns
task);
ip
workqueues
timer.
refcount
attribute
prev,
.print
items
virq
length;
(ctx)
*ctx
*ops)
cpudl
subsystems
&one,
PPS
blk_trace
Linus
*desc;
consider
3;
similar
done.
frame
small
freeing
chain.
memory.
reduce
recursive
GP
1:
dynticks
successful
rest
code,
%NULL
lock->owner
subset
q
queue.
POSIX
read_lock(&tasklist_lock);
waking
larger
fixed
*ts)
timeout)
fix
*ops;
*op,
fn
Can
u16
BPF_ALU
4;
trace_seq_putc(s,
num
unregister
*key,
key,
attributes
again,
counters
single_release,
event;
delayed
rdp
advance
s,
for_each_cpu(cpu,
concurrent
common
new;
static_key
tsk);
mode)
Steven
rq,
*env,
*tg)
lb_env
parent;
id,
audit_log_end(ab);
*trace)
node)
event_command
tick_device
irq_chip
rem
*tp)
crash
cpuctx,
Paul
<linux/notifier.h>
well.
*head)
HZ)
further
detected
<linux/ctype.h>
*node;
pfn
creating
pr_debug("PM:
anyway.
Need
later.
releases
We're
lost
4)
perf
reboot
Register
registers
cmd,
ip;
current->comm,
mask);
remain
ptrace
attempt
kill
serialize
-1);
Always
tr);
BPF
*call)
*ignore,
HZ);
*hcpu)
duplicate
fd
key);
delta,
nohz
binary
cpus.
i++,
single_open(file,
rnp
OK
empty.
level,
"%s",
commands
repeat
current)
rq->lock
*mod;
*new;
now);
offsetof(struct
quota
__always_inline
adjustment
vma
@cpu
<linux/proc_fs.h>
serial
verifier_env
fmt);
[BPF_ALU
[BPF_ALU64
mutex_unlock(&audit_filter_mutex);
hash,
k_clock
CONFIG_DYNAMIC_FTRACE
s32
*tk,
kprobes
timeval
(long
*handle,
cgrp
*cpu_buffer,
though
Just
example,
head);
lists
synchronize_sched()
However,
Rights
command.
drivers
PAGE_SIZE);
apply
buffer;
interest
update.
hasn't
<mingo@redhat.com>
<linux/security.h>
%false
legacy
running,
sighand
I/O
sleep.
set);
(type
arg
pc;
resulting
used.
<linux/rcupdate.h>
names
HZ;
args
none
pair
*attr)
container_of(work,
i.e.
CONFIG_NO_HZ_COMMON
pull
observe
smp_rmb();
accounted
variables
interrupted
lockup
cpumask_set_cpu(cpu,
deadlock.
jump
(clone_flags
controller
desc);
switching
*ns)
*prev)
imbalance
busiest
);
right);
exist
4]
insn->dst_reg,
ab
trace_assign_type(field,
*iter);
irq_chip_generic
newline
expired
NSEC_PER_SEC
lock_usage_bit
rb
ftrace_page
NTP
lockdep_assert_held(&cgroup_mutex);
@buf:
+---+
*)0,
interrupt.
spurious
periods
CONFIG_DEBUG_LOCK_ALLOC
half
(but
Run
callbacks,
region
head;
Peter
PAGE_SIZE;
objects
smaller
greater
exact
image.
nr_pages;
<linux/freezer.h>
decrement
rcu_dynticks
Prevent
force)
kmem_cache
*n;
middle
soon
pid)
security
bound
filters
(type)
op,
&val);
b)
c;
coming
error,
install
action);
Prepare
WARN_ON_ONCE(1);
pick
*self,
%s",
2004
breakpoints
ok
e
seq_open(file,
success.
*)dest
Fetch
*link;
gone
(1UL
They
fall
hrtimer_restart
old,
PAGE_SIZE,
resources
synchronization
new);
*new)
spinlock
mode);
cpusets
sched_class
*se
*env)
GFP_KERNEL
*sd,
chance
raw_spin_unlock_irqrestore(&desc->lock,
task_cputime
spin_unlock(&hash_lock);
bucket
pc)
ubuf,
line,
cpupri
L:
Map
probes
(offset
*buf
@lock:
group_entry)
irqreturn_t
cgroup_mutex
*cgrp,
mems_allowed
hrtimer_clock_base
buffer->cpumask))
*image,
parallel_data
comes
asynchronous
Based
assign
current->lockdep_recursion
handler.
unknown
needed.
(*func)(struct
func;
others
Reserved.
address.
Inputs:
argv
possible.
BITS_PER_LONG
touch
pages,
Determine
pages.
mapped
performance
Copy
ERR_PTR(-EINVAL);
freezing
parent's
clearing
so,
off)
three
raw_spinlock_t
expedited
side
running.
tracking
hence
loop.
Currently
current->flags
sync
testing
matter
Test
Found
tasklist_lock
caused
automatically
notification
changed.
mode.
(for
mm_segment_t
Boston,
id;
pos;
is,
-ENOSPC;
fail:
f
disabling
rule
@flags:
(!entry)
name)
depending
open
double
module.
warning
offset)
*rnp
preempted
cpu))
uprobe_consumer
caller.
triggers
*event
errno
reprogram
wait);
nobody
ring_buffer_iter
trace_flags
kdb_printf("
min
nr;
task_struct,
absolute
stay
scaled
nodemask_t
preferred
parallel
careful
cpus_allowed
v,
converted
cnt);
IP_FMT
cputime;
fn,
?:
Lock
mutex_lock(&audit_filter_mutex);
*m)
mutex_unlock(&module_mutex);
unique
*ts,
overhead
sched_attr
next);
.free
read;
Copyright(C)
irq_base,
kretprobe
byte
*ctx;
values[n++]
trampoline
rec->flags
desc->status_use_accessors
cpuset.
cgroup_root
@cgrp
*ss;
mutex_unlock(&cgroup_mutex);
generated
Corporation,
<linux/types.h>
<linux/atomic.h>
*t
NOTIFY_DONE;
10;
tasks,
above.
kthread.
self
argc
(PAGE_SIZE
Memory
blocks
let's
radix
represents
attempting
work.
on,
frozen
*css)
@task:
descendant
invoking
perform
.seq_show
.read_u64
boosting
wait_queue_head_t
processed
atomic_long_t
Wake
Time
*rnp,
__maybe_unused
step
shutdown
(rc)
enqueue
hrtimer_sleeper
reached
(mask
ptracer
n,
signal.
current_cred();
domain.
@tsk:
sleeping
that.
successfully
accordingly.
(!access_ok(VERIFY_WRITE,
get_fs();
"trace.h"
integer
'\0')
compare
index,
free_page((unsigned
CONFIG_FUNCTION_TRACER
seccomp
(action)
phase
2.
reads
<linux/vmalloc.h>
rcu_torture
pr_cont("
1.
normally
%p
*ks)
error:
Avoid
push
issue
factor
(idx
(*pos)++;
seq_release,
c,
one,
point.
turn
OK,
Either
assigned
check.
Setup
CONFIG_NUMA_BALANCING
kgid_t
exclusive
XXX
*wq
*),
Rostedt
RING_BUFFER_ALL_CPUS)
subclass,
nsproxy
1000
prev);
*sd;
shares
fit
for_each_sched_entity(se)
/=
cfs_rq_of(se);
TODO:
lead
"%d\n",
per-task
operand
l;
tr;
(*pos
mod->name,
gcov
timex
*glob,
entry;
printk("\n");
preempt_enable_notrace();
*ts
mutex_unlock(&wq->mutex);
simple_read_from_buffer(ubuf,
@domain:
*tk)
pg
hb
mutex_unlock(&ftrace_lock);
*buffer)
pr_warn("%s:
taskstats
*kp)
module_kobject
@timer:
(RB_WARN_ON(cpu_buffer,
current->audit_context;
user-namespace
logic
16
results
exclusion
iff
occur
enables
location
assumed
pid);
<linux/list.h>
gfp_mask,
elements
counter.
memory,
that,
asmlinkage
PAGE_SHIFT;
migrating
seq_putc(m,
mask.
placed
8)
structure's
for_each_rcu_flavor(rsp)
counts
*nb)
nb);
nothing.
sig);
user_struct
space.
proper
*cred
t);
suitable
utime,
cancel
null
set_fs(KERNEL_DS);
replace
59
Temple
Suite
330,
len))
basic
*root,
Then
event_subsystem
ftrace_subsystem_dir
ops,
CONFIG_PERF_EVENTS
h
seccomp_filter
pointer.
*task
avoids
optional
Here
boost
sched_param
(firsterr)
primary
lock:
!
panic
(i.e.
dont
path.
active,
cycle
load;
usually
raw_spin_lock_irqsave(&rnp->lock,
Special
snprintf(buf,
alloc_percpu(struct
far
userspace.
Enable
completed.
description
first,
nr_cpu_ids
true)
virtual
which,
*a,
*b)
enable);
(!new)
details
for_each_tracing_cpu(cpu)
overruns
100
min,
seq_puts(s,
pinned
dl_bw
leftmost
root_domain
kept
sibling
nid,
fault
seq,
representation
onto
raw_spin_lock_irqsave(&desc->lock,
uid_t
check_reg_arg(regs,
@fn
cputime
conversion
ASCII
(!(trace_flags
klp_object
disables
@size:
tracers
tracer_flags
*next)
*mm,
fail.
rt_bandwidth
expires,
__func__,
Parameters:
seq_printf(seq,
max_active
name;
irq_hw_number_t
depth;
*flags);
kmsg
*op;
data)
@cgrp:
ssid)
(mod->state
(int),
hrtimer_cpu_base
*base
compressed
(result
local_t
relay
*cpu_buffer)
f->val);
*context
rwbs[i++]
triggered
destroy
B
down.
(t
removing
pr_fmt(fmt)
start)
end,
(p)
<linux/device.h>
page;
saveable
(!page)
snapshot_handle
idea
(!buf)
2007
Foundation.
do.
*pos;
freeze
*cft)
Internal
structure,
*parent;
->completed
invoked.
array.
certain
rt
<linux/ptrace.h>
sig)
wakes
consumed
dequeue
parent.
characters
overrun
locked.
ancestor
miss
(action
supported
*pred,
*(u32
WARN_ON(ret);
"%s\n",
*call,
32-bit
Clean
iter->ent;
trace_buffer_lock_reserve(buffer,
((struct
pr_cont("\n");
spinning
spin
.priority
Take
software
sector_t
arg;
dev_t
<linux/file.h>
insn->imm
however
.owner
online.
trace_probe
<linux/time.h>
Record
sort
configured
there.
FIXME:
(entry
mod
group_info
filled
@q:
detach
seq_printf(s,
constraints
*bp)
task_rq_unlock(rq,
(unlikely(ret
se
Look
20
*cfs_rq
Put
selected
forward
(rc
args;
(opcode
err_free;
delta)
__field(
__field_desc(
serialized
'\n')
(__force
nsecs
event))
from_kuid(&init_user_ns,
tracefs
masks
(write)
irq_work
hits
mutex_lock(&wq->mutex);
storage
irq_chip_type
rem;
*hash,
raw_spin_unlock(&lock->wait_lock);
dependencies
*class)
uval,
ftrace_enabled
mutex_lock(&ftrace_lock);
(unlikely(ftrace_disabled))
(!ab)
alarm_base
optimized_kprobe
*ent)
qos
handler_errors++;
false))
known
mutual
block.
rcu
stall
*this,
*g,
barriers
sees
License.
Remarks:
reserve
random
(as
tree.
dst
pages;
frames
alloc
more.
ones
*buf;
thing
verify
according
execution.
towards
*rsp;
progress.
tries
leaving
Read
consume
,
userland
paths
spinlock_t
count.
preserve
PENDING
entered
progress
restart_block
PID
CONFIG_KGDB_KDB
*data);
file->event_call;
parse_error(ps,
ftrace_event_field
optimize
several
rlimit
Common
<linux/errno.h>
maps
includes
&flags,
Verify
remote
User
pairs
missed
*m
THIS_MODULE,
rdp->cpu,
properly
above,
i))
sections.
worry
j,
rnp,
CPUs,
handles
enqueued
elapsed
soft
big
label
@new:
*wq,
.private
desc
<srostedt@redhat.com>
trylock
bp
branch
slice
CONFIG_SCHEDSTATS
Store
rate
*mm
(end
constant
value)
resource,
cgroup.
autogroup
kernel_cap_t
dev);
opcode
trailing
*owner;
addresses
owner,
mutex_lock(&module_mutex);
klp_patch
@iter:
audit_parent
ftrace_graph_ent
*cmd,
mm,
request,
concurrency
timer,
pools
I:
pwqs
*pool)
hwirq
waiter);
(class->usage_mask
desc,
sizeof(u64);
SINGLE_DEPTH_NESTING);
executable
TID
mod,
*cp,
cgrp_cset_link
mutex_lock(&cgroup_mutex);
out_finish;
cpuset_mutex
int),
swap_map_handle
subbuf_size
cp
time_status
debug_alloc_header
stuck
kernel,
Rusty
invocation
CONFIG_PROVE_LOCKING
assumes
*head
queue,
reach
cond_resched();
accesses
*mod
None.
diagnostic
kdb_printf("\n");
%ld
creation
allocating
room
arbitrary
New
number.
CONFIG_HIGHMEM
present
saving
Compute
desired
Let
done,
size.
%lu
fails,
restored
belongs
lazy
dying
periods.
active.
*rdp,
mean
<linux/reboot.h>
off.
cmd);
limited
sufficient
'%s'\n",
sigqueue
<linux/capability.h>
Tell
*mask,
passing
permissions
Unlike
information.
probably
application
instead.
unlocked
vs
stores
Please
@pid:
Send
(const
locks.
match;
@len:
*rec;
idx,
&tr->events,
filter.
failing
lowest
smp_mb__before_atomic();
out_free:
(
(id
warn
idx);
queued,
causes
KGDB
kgdb
locks,
*ptr;
bpf_insn
nr_running
stuff
scale
*unused)
*rnp;
*dest)
vaddr,
filename
ctx,
*mm)
profile
smp_mb__after_atomic();
tests
words,
rdp);
raced
Advance
Stop
took
direct
cpu_online_mask);
retry
compiler
spent
*old;
new,
%p\n",
waitqueue
paired
bit.
*cp;
live
ERR_PTR(err);
tsk,
-EOPNOTSUPP;
RUNTIME_INF)
select
found,
raw_spin_unlock(&rq->lock);
diff
NSEC_PER_USEC;
freq
sg_lb_stats
*action)
),
gid_t
*ab,
watch
reg_state
*const
verbose("invalid
fields\n");
overwrite
[BPF_JMP
spin_lock(&hash_lock);
*ab;
iter->ent);
allocated,
cmpxchg
tested
parts
int)
options
__initdata;
Adds
*info;
*tr;
id);
spaces
expire
CONFIG_NO_HZ_FULL
question.
line.
enabling
*child,
@child
preempt_disable_notrace();
&data,
read,
CONFIG_HIGH_RES_TIMERS
<linux/workqueue.h>
@wq:
owner.
*nh,
regs))
Write
'\n';
sched_clock()
curr->held_locks
hlock,
locks:
*cgrp)
futex_pi_state
(ops->flags
*pg;
hlist)
(!retval
MODULE_STATE_UNFORMED)
-ENOEXEC;
*cs,
&left,
timer's
firing
error_packet(remcom_out_buffer,
mutex_unlock(&kprobe_mutex);
read_seqcount_begin(&tk_core.seq);
(read_seqcount_retry(&tk_core.seq,
mult
bitcount;
prefix
depends
<linux/hardirq.h>
rcu;
statically
dynamically
consistent
offline,
out,
so.
*list;
empty,
by:
once.
potentially
*info
*start,
<linux/suspend.h>
(res
(this
nodes.
together
parameter.
Sets
kmalloc(sizeof(struct
Image
destroyed
Tasks
regardless
synchronized
E.
Process
Even
1];
partition
kthreads
unregistered
priority.
immediate
Timesys
API
ignored
sig;
killed
events,
*regs
sighand_struct
reported
Also,
run.
deal
sense
window
System
remember
upper
k
backwards
old;
6
op;
*rec)
down,
trace,
-EEXIST;
FILTER
b
d
""),
(c
share
-EINVAL
(data
tracer.
str);
tr->trace_buffer.buffer;
trace_event_functions
.trace
container_of(p,
CONFIG_PREEMPT
func);
torture_type,
started");
exit.
ts,
happen.
100;
fdput(f);
bpf_func_proto
async
With
smp
-EBUSY
polling
*arg,
probe_arg
<linux/kernel_stat.h>
dyntick-idle
*rdp;
operations.
true.
Adjust
*rdp
trace_rcu_grace_period(rsp->name,
work,
section,
64-bit
protection
stamp
policy,
CLOCK_MONOTONIC,
event->state
*nfb,
releasing
May
task:
delayed_work
modification
irq;
usecs
manually
little
update_rq_clock(rq);
*next
fastpath
raw_spin_lock(&rq->lock);
CFS
gives
segments
1024
rounding
sanity
traced
audit_state
umode_t
__u32
(-EINVAL)
*old,
optimization
(class
insn->off;
BPF_MEM
struct_name,
@name:
("
cpuacct
*entry)
hlist_node
per_cpu_ptr(tr->trace_buffer.data,
PM
@p
":
Module
loading
mod->name);
What
posix_clock
item)
*ppos,
ftrace_graph_ret
trace_parser
*class;
Can't
(tmp
circular
*d,
CONFIG_NUMA
Description:
*pt_regs)
*buffer
*rt_se,
Display
&file->flags);
*n)
*pool
pr_info("
written;
(attr
Size
virq;
tracepoint_func
pr_warn("error
(name)
itimerval
trace_array_put(tr);
events:
exit;
irqd_set(&desc->irq_data,
MSI
*cpuctx,
sec
(read_format
*uaddr,
optimized
*base,
signature
section_objs(info,
%11lu\n",
pid_type
(txc->modes
(gid_t)
(uid_t)
padding
*cp
*uprobe,
*pinst,
<linux/irq.h>
Read-Copy
McKenney
CREATE_TRACE_POINTS
RCU.
rcu_read_lock()
Will
(state)
Invoke
(event
stack,
<linux/compiler.h>
sizeof(void
form
(pfn
curr;
*dst,
pointed
Release
stop;
atomically
1),
descendants
initiate
destruction
fork
Task
*css
#elif
Current
CONFIG_RCU_NOCB_CPU
etc.
duration
Torvalds
0:
Default
system,
2004-2006
<linux/pid_namespace.h>
2:
++i)
HZ,
notified
(p->flags
PF_EXITING)
eligible
exactly
mechanism.
16;
*flags)
members
current))
realtime
switched
clears
yet,
return,
receive
stops
find_task_by_vpid(pid);
regex
*str,
*ps,
*filter)
DATA_REC(YES,
"%s
long),
<linux/completion.h>
rcu);
important
Corporation
refcnt
compatible
(val)
dest
this_rq();
@n:
cycles
128
errors
regular
"");
smp_hotplug_thread
smp_processor_id());
cpu),
follows
USA
Failed
fetch_type
string,
uprobe_cpu_buffer
dsize);
file;
*key
rcu_get_root(rsp);
usermode
haven't
partial
rdp,
Move
looks
us,
increase
Using
routines
call_single_data
appear
activate
comments
(iter
references
LSM
Invalid
mutex_waiter
nr_irqs);
period;
Account
*rt_rq
task;
worst
1000;
bitmask
diff;
*tsk;
read-only
*curr)
detached
aren't
ns;
char,
verifier_state
verbose("(%02x)
SRC_OP);
i++;
grab
directly.
.write_u64
*waiter)
remains
<linux/hrtimer.h>
u64)
stime
enforce
Needs
text,
fsnotify_mark
*map,
tracer_stat
(start
TASK_UNINTERRUPTIBLE);
(v
filesystem
tracer_opt
skip,
files.
CONFIG_CGROUP_SCHED
tracer_enabled
*rt_se)
state);
kdb_printf("%s:
(file->f_mode
');
*work,
@work:
*pwq;
spin_unlock_irq(&pool->lock);
prev;
rusage
hrtimer_mode
*cgrp;
cpuctx
tstamp
came
ops->flags
ftrace_profile
reprogramming
pr_err("%s:
*cgrp
cset
*val,
mod->state
&modules,
@cs:
*base)
auditd
Address
thr
subprocess_info
ww_mutex
ww_acquire_ctx
mutex_lock(&kprobe_mutex);
tk->tkr_mono.xtime_nsec
log_flags
event_probe_data
audit_field
oldlen,
CTL_STR,
newlen)
xol_area
uprobe_task
{{
mechanism
<linux/tick.h>
enabled)
c)
limits
till
terminated
ensuring
0444);
generate
console_loglevel
(strcmp(argv[0],
allocations
build
easy
*bm,
pfn,
freed.
copies
During
Helper
base);
%u\n",
*handle)
inherit
grabbing
nesting
CBs
jiffy
level;
rnp)
itself,
call_rcu()
jiffies.
Maximum
field.
defines
arg)
Corp.,
belong
globally
4:
Given
word
stop.
process,
signr
flag.
they're
latter
change.
spin_unlock_irq(&current->sighand->siglock);
XXX:
causing
failed,
relevant
close
compat_uptr_t
"Illegal
strings
filter;
*ps)
filter,
m,
alignment
BPF_K:
filter)
Validate
Drop
affect
increments
safe.
3,
""
end:
4,
rcutorture
reuse
(!torture_must_stop());
(the
sync,
-E2BIG;
done:
no_llseek,
registration
implementations
obtain
e)
catch
per_cpu_ptr(rsp->rda,
rsp,
poll
list_for_each_entry(event,
probed
used,
DEFINE_PER_CPU(int,
this_cpu_ptr(&rcu_dynticks);
permitted
maintain
rdp->mynode;
jiffies,
ends
working
Similar
removed,
deferred
Give
leak
returns.
synchronize_rcu();
parameters.
arch_spinlock_t
seconds.
is.
HRTIMER_RESTART;
Let's
HRTIMER_MODE_REL);
expects
@func:
spread
Zijlstra
mod);
mod;
successful,
prepare_creds();
Install
*new,
Change
current_user_ns();
nr,
&wait);
C
((flags
ns);
archs
*dev_id)
queueing
rq.
*timer
delta_exec);
is:
siblings
vruntime
MB
*ns,
difference
cross
interval;
nr_node_ids;
(queued)
(e.g.
wl
GFP_NOWAIT);
.get
pointing
accessed
va_start(args,
va_end(args);
insn->code,
pointer,
insn->off
walking
item,
})
ctx;
(DST
CONT_JMP;
__initdata
lets
exist,
*f,
f,
values.
versions
klp_func
sym
clocksource.
gcov_ctr_info
IF_ASSIGN(var,
pc,
trace_enum_map
time;
nearest
p->prio
clone_flags,
*param,
raw_smp_processor_id(),
0555,
sd;
*d)
display
tr->max_latency
compat_timespec
Check,
"Display
FMODE_READ)
command)
stack_trace
default_llseek,
already.
@worker:
@worker
@nh:
tracing_open_generic,
*ct
(data)
hwirq,
controllers
@lock
base,
kretprobe_instance
cnt)
seq->private;
backtrace
trace_enum_map_item
(tracing_disabled)
NOTICE
SEQ_PUT_FIELD(s,
ftrace_func_command
[L]
futexes
hlock
curr->lockdep_depth;
perf_event_header
driver.
event->ctx;
cgrp)
(is_cgroup_event(event))
swevent_htable
KDB
kernfs
ssid))
kernfs_node
cgroup_pidlist
*kp;
(cp
mutex_unlock(&cpuset_mutex);
nr_threads;
wait_opts
"..
netlink
fgraph_data
stat_session
00
console_unlock();
rchan
for_each_buffer_cpu(buffer,
QoS
pm_qos_request
list_for_each_entry(n,
write_unlock(&resource_lock);
res->end
*bt
@val:
in-kernel
performed
intended
and,
extend
Corp.
magic
<linux/kdb.h>
*addr)
touch_nmi_watchdog();
5;
inactive
argv,
N
List
once,
rtree_node
mem_zone_bm_rtree
*zone;
adds
*node,
How
levels
nor
cur
pfn)
bits,
*page)
chunk
free;
stop,
all.
swsusp_info
Save
snapshot.
Restore
cgroup,
css_task_iter
Snapshot
forcing
initialized.
(HZ
CONFIG_RCU_TRACE
7
exported
registered,
cases,
daemon
w
*mask)
current->pid,
RETURNS:
~mask;
*q)
Notify
allowed.
wasn't
found.
pretend
group)
Queue
unlock_task_sighand(p,
PIDTYPE_PGID,
stime;
passes
8;
Whether
e.g.
spin_lock_irq(&current->sighand->siglock);
enough.
places
bail
-EINTR;
@ts:
somewhere
USA.
<linux/perf_event.h>
"trace_output.h"
parse
*d
WALK_PRED_DEFAULT;
*filter;
list_for_each_entry(file,
a,
*root)
-errno
creates
inherited
BPF_X:
(mode
installed
order)
32bit
"%s:
irq_flags,
*file
<linux/moduleparam.h>
testing.
torture_random_state
.readlock
.read_delay
.readunlock
pr_alert("%s"
TORTURE_FLAG
%d",
prints
David
mode;
GFP_USER);
bigger
feature
*end;
"sched.h"
sampling
affected
@x:
definition
\Sum
<linux/debugfs.h>
m->private
smp_processor_id())
td
runqueue.
seconds,
Place,
02111-1307
event)
*value
way,
*rdtp
(not
2))
finally
in,
supposed
*next,
bit)
head)
called,
Fix
<linux/sysctl.h>
<linux/sched/rt.h>
guest
perf_event_attr
.size
traces
this_cpu;
contexts
(cpumask_test_cpu(cpu,
iter,
fd,
thread_info
@mode:
(*cp
contention
*sem,
namespace,
ns_common
couldn't
(!(flags
instances
HRTIMER_NORESTART;
delta_exec;
earliest
else.
*se,
cfs_rq->min_vruntime;
CONFIG_64BIT
(task)
numa
net
use,
work;
se,
%=
length,
fair
for_each_cpu(i,
Load
saves
collect
raw_spin_unlock_irqrestore(&rq->lock,
resources.
protection.
2005
(BPF_SRC(insn->code)
edge
loops
F_STRUCT(
(true)
nsec;
(u64)
local_save_flags(flags);
off;
({
<linux/mount.h>
*s;
s;
__buffer_unlock_commit(buffer,
ap);
acquiring
cpu_relax_lowlatency();
proceed
infrastructure
suspend_state_t
record.
print,
ent,
(retval
question
.child
printk(KERN_DEBUG
sched_domain_attr
NSEC_PER_SEC;
uts_namespace
":");
CONFIG_TRACER_SNAPSHOT
posix
pool->lock
WQ:
*pool;
color
@pool:
*worker;
rescuer
@wq
nr_to_call,
r);
@d:
nr_irqs,
*domain)
linux
@parent:
base;
(left
nokprobe_inline
*name
symbol_cache
kallsyms
pr_info("Testing
recording
fails.
clockid_t,
rounded
allocated.
msg
SEQ_PUT_HEX_FIELD(s,
generic_file_llseek,
tracing,
*ret
[P]
acquires
printk("[
order);
2003
timers.
u
(NSEC_PER_SEC
PERF_EVENT_STATE_INACTIVE)
perf_output_put(&handle,
kfree(buf);
q->lock_ptr
ftrace_func_entry
ftrace_iterator
(!(torture_random(trsp)
down_read(&css_set_rwsem);
up_read(&css_set_rwsem);
info->hdr->e_shnum;
&cs->flags);
cs
mutex_lock(&cpuset_mutex);
spin_lock_irq(&callback_lock);
spin_unlock_irq(&callback_lock);
secs
%11llu\n",
leap
print_name_offset(m,
*sp)
CONFIG_SUSPEND
audit_log_cap(ab,
*desc);
-EINVAL);
strcpy(remcom_out_buffer,
iter->private;
kp);
raw_spin_lock_irqsave(&timekeeper_lock,
raw_spin_unlock_irqrestore(&timekeeper_lock,
tasklet_struct
#x,
trace_rcu_nocb_wake(rdp->rsp->name,
dah_overhead
write_lock(&resource_lock);
exit_free;
blk_io_trace
"internals.h"
potential
(we
http://www.gnu.org/licenses/gpl-2.0.html.
CONFIG_PROVE_RCU
when:
activated
secs,
for.
Control
arrived
transitions
allowing
entries.
subject
<linux/nmi.h>
filtered
trace.
chain_allocator
represent
leaves
bitmaps
*list
*list)
order.
updated.
buffer);
keys
swsusp_free();
j);
@buf
eventually
it;
kernfs_open_file
nbytes,
worth
Last
drain
head.
end.
aware
reasons
exited
name.
switches
follow
typically
Calls
rtmutex_chainwalk
functions,
*task);
<linux/compat.h>
cleared.
sending
threads.
force);
p))
delivered
&flags))
*action;
&utime,
interested
went
explicit
stable
sized
compat_sigset_t
from,
plus
field,
(!error
TASK_RUNNING
root;
err,
(call->flags
8:
ops;
*count
&count);
failure,
reset,
manage
accessing
ERR_PTR(ret);
validate
syscalls
nr)
id)
facility
*p);
bug
1000);
mutex,
test.
licensed
reaches
key)
handle;
lock_system_sleep();
unlock_system_sleep();
bpf_attr
*map;
fixup
GPL
nr_active
:=
result.
ran
relies
flip
*this_rq,
cpu_possible_mask);
%lu\n",
free_out;
park
*td;
per_cpu
*(type
location.
src,
pr_info("Failed
(%d)\n",
<-
methods
incremented
debugfs
seeing
possibility
Now,
check,
lockless
array,
changed,
hierarchy,
So,
Thus
0));
softirqs
cookie
forced
_rcu_barrier_trace(rsp,
variable.
operation.
fired
physical
block_device
<linux/gfp.h>
@wait:
wait)
put_cpu();
finish.
sum;
end);
@start:
Written
abort_creds(new);
from.
"CRED:
(current->flags
p->state
standard
KDB_ENABLE_ALWAYS_SAFE);
<linux/utsname.h>
max)
routines.
*bp,
enable,
slots
Device
*dl_rq)
task_rq(p);
*dl_se,
possible,
point,
fits
rq->curr;
(next
topology
for_each_domain(cpu,
sd)
(sd->flags
CONFIG_CFS_BANDWIDTH
entities
Iterate
curr);
numa_group
actively
domain,
treated
callback_head
*vma;
(vma->vm_flags
up_read(&mm->mmap_sem);
contrib
expiration
*cfs_rq;
boundary
S
we'd
raw_spin_lock_irqsave(&rq->lock,
sg;
'struct
new_value);
seq_printf(p,
desc->action;
uid;
reg
insn->src_reg,
class;
valid.
FILTER_OTHER
underlying
Added
BPF_R0
__weak;
(!s)
reorder
(!call_filter_check_discard(call,
atomic_dec(&data->disabled);
.funcs
.reset
CONFIG_FTRACE_SELFTEST
writers
raw_spin_unlock_irqrestore(&sem->wait_lock,
*obj)
disabled;
*info);
links
ts);
.clock_getres
.clock_get
dev,
(index
swapped
*event);
flag,
Cannot
defined(CONFIG_SMP)
clocks
Valid
j)
deleted
waiters.
*tp
kdb_bp_t
__func__);
Zero
@ops:
event_trigger_init,
@type:
de
per_cpu(cpu_profile_hits,
tick_period);
WARN_ON_ONCE(!irqs_disabled());
KTIME_MAX;
wq
CONFIG_SYSFS
wq)
@pwq
spin_lock_irq(&pool->lock);
kstrtoul_from_user(ubuf,
hwirq);
*tp,
*ri,
(u8
entry->ip
warn++;
@s:
save_len;
args)
hex
compat_ulong_t
align
TASK_COMM_LEN);
"\t\t\t
raw_spin_unlock_irqrestore(&task->pi_lock,
curr->comm,
lock(");
printk(");\n");
print_kernel_ident();
printk("\nstack
backtrace:\n");
irqclass);
(!mark_lock(curr,
raw_local_irq_save(flags);
raw_local_irq_restore(flags);
*rb;
*rb)
irqd_clear(&desc->irq_data,
*msg
msi_domain_info
bsd_acct_struct
*cpuctx;
*css;
*pmu)
hw_perf_event
day
uaddr2
kthread_worker
@pos
csses
subsys
down_write(&css_set_rwsem);
up_write(&css_set_rwsem);
list_for_each_entry_rcu(mod,
**/
*negp
timer->base
*skb;
avail
audit_log_start(context,
LZO
thr++)
error);
&timer_debug_descr);
@platform_mode:
platform_mode)
log_first_seq;
log_first_idx;
raw_spin_unlock_irq(&logbuf_lock);
syslog,
subbuf_size;
PM_SUSPEND_FREEZE
event->type_len
pm_qos_object
async_cookie_t
temp_start
called\n");
CONFIG_RCU_NOCB_CPU_ALL
(!diag)
bytesperword
@targ
__blk_add_trace(bt,
IRQS_PENDING;
(desc->istate
detailed
cases.
member
elapsed.
finished.
debug_obj_state
Limit
waited
beyond
started.
to,
invokes
local_bh_disable();
structures.
Wind
River
Systems,
Outputs:
Locking:
proc
*)addr,
"power.h"
included
represented
linked_page
*))
bits.
tree,
Functions
PG_UNSAFE_CLEAR);
*bm)
extents
representing
corresponds
*addr;
well,
*src)
tables
Total
released.
Suspend
(nr_pages
happen,
decrease
failed\n");
End
(!buffer)
reason);
*page,
created.
freezer->state
cgroup_taskset
&it);
@state:
level.
Are
hierarchy.
Mask
boosted
Place
visit
batch
offline.
OOM
determined
Where
fields.
activity
*cmd)
Unregister
available.
1UL
*waiter,
&~
@mask
*q;
k_sigaction
m;
stopping
pending.
deadlocks.
*pid)
leader.
&stime);
cleaned
parent);
gone,
tasklist
behavior
consecutive
wakeup.
namespace.
tsk;
spin_unlock_irq(&tsk->sighand->siglock);
*param)
sigsetsize)
please
belonging
(!access_ok(VERIFY_READ,
SLAB_PANIC);
'\0'
move_type
pred
*filter,
considered
root,
ch;
'"')
left,
committed
2012
<linux/audit.h>
BPF_JMP
*child)
strict
snprintf(buf
sizeof(*entry),
rctx;
sizeof(u32),
sizeof(u64));
"Number
ts
occasionally
think
cond_resched_rcu_qs();
rcu_torture_writer_state
(!kthread_should_stop())
smp_mb()
tracing_off();
oops
module,
IO
Unlock;
filp);
notifiers
values,
dev;
map;
macro
attaching
mess
(long)
load,
keeping
contribution
tick.
balance.
*rsp
CPU-hotplug
outgoing
attempts
ent;
type)(struct
u32,
*tu,
event_file_link
(file)
event:
<linux/random.h>
r3,
r4,
preemption.
RCU-sched
oldval,
queues
Linux
ULONG_MAX
Kick
Attempt
future,
progress,
requests
recent
required.
Report
Walk
disallow
%d:
counting
softirq_action
invocation.
specify
anyway
ahead
last,
ordered
accept
CPU_DEAD:
systems.
ULONG_MAX)
2010
detector
twice
rw,
cpu's
nodemask
@func
ids
moves
<pzijlstr@redhat.com>
*)(unsigned
overlap
CONFIG_DEBUG_CREDENTIALS
query
subjective
behalf
wait_queue_t
place.
*wait,
2005-2006,
subclass)
*ns;
1000,
<linux/kprobes.h>
*res,
occurs
identity
*rq);
runtime,
container_of(timer,
throttle
We'll
cpumask_test_cpu(cpu,
rq->curr
(!task)
math
<linux/profile.h>
const_debug
Enqueue
rss
distance
around.
keeps
*work
*tg;
tg
occurred
accumulated
consuming
owned
rq's
*cfs_b
raw_spin_unlock(&cfs_b->lock);
expires)
RUNTIME_INF;
Per
subtract
schedstat_inc(p,
serialization
~CPU_TASKS_FROZEN)
Ignore
reap
nr_irqs;
uid,
audit_tree_refs
(BPF_MODE(insn->code)
regno,
(off
discovered
explored
identifier
%lx
Russell
cpu_stop_work
*work;
repeat;
stop_machine()
wakelock
active;
utime
clock;
ptr;
2002
address,
*inode
key_size);
hash);
modules.
X
front
waiter;
freezable
@node:
*func;
object.
rtc
gcov_fn_info
checksum
anything.
(!iter)
iter->pos
posix_clock_desc
tstruct,
entries;
individual
with.
old_flags,
There's
Thus,
Removes
*child;
sleep,
sched_clock();
Callers
depth,
first;
tracing_reset_online_cpus(&tr->trace_buffer);
(!pid)
*rt_rq,
tick_do_timer_cpu
__ref
incr
this_cpu_ptr(&tick_cpu_sched);
days
HRTIMER_MODE_ABS);
manager
flushing
for_each_pwq(pwq,
*wq)
mutex_unlock(&wq_pool_mutex);
match.
comma
unmodified
max,
sizeof(buf),
flow
*chip
nr_irqs)
nests
commit_creds(new);
__trace_seq_init(s);
(s->full)
s->full
out_err;
timezone
tracing_start();
tracing_stop();
pr_warning("**
**\n");
-EBADF;
clock.
**data)
put_task_struct(task);
<0
lock_class_stats
(!name)
]\n");
printk("%s/%d
lockdep_print_held_locks(curr);
dir
*timer;
clock_id,
*msg)
*ops
exp
perf_cgroup
timer);
raw_spin_unlock_irq(&ctx->lock);
value);
&event->hw;
current->mm;
p->flags
&ftrace_list_end;
ftrace_profile_stat
hash.
lock_torture_ops
(cxt.nrealwriters_stress
GOT_YOU_MORON
css_set.
@ss
ssid;
ns)
*mattr,
*mk,
*size,
num)
cp);
cs->flags
*cs
atomic_add_return(1,
%TRUE
kbuf
write_unlock_irq(&tasklist_lock);
rcu_ctrlblk
infop)
rcu_batch
trace_test_buffer(&tr->trace_buffer,
auditing
listeners
out_clean;
spin_unlock_irqrestore(&base->lock,
_RET_IP_,
"OK");
optimized_kprobe,
readout
tk_read_base
xtime_nsec
write_seqcount_begin(&tk_core.seq);
write_seqcount_end(&tk_core.seq);
bpage
prctl_mm_map,
syslog
msg->flags;
kmsg_dumper
*newval,
kdb_printf("%s",
!trace_seq_has_overflowed(s);
pm_qos_constraints
(result)
(!(time_status
pr_warn("crashkernel:
cap)
defcmd_set
q->blk_trace;
begin
portion
desc)
raw_spin_unlock_irq(&desc->lock);
happened
masked
<linux/bitops.h>
act
illegal
boot.
tracked
stack.
Finally,
in-flight
default.
Pick
for_each_process_thread(g,
bool,
inform
Debugger
bt
mask))
&nextarg,
&addr,
&offset,
functionality
Pavel
J.
GPLv2.
<linux/console.h>
5)
image,
gfp_mask)
__free_page(page);
initializes
extent
More
*addr
nodes,
(bit
Basic
themselves
*src,
terminate
nr_meta_pages
@mask:
them,
Normal
ones.
page_address(page);
(buffer)
Assumes
completion.
addition
<linux/cpumask.h>
CONFIG_NO_HZ_FULL_SYSIDLE
GP.
CONFIG_RCU_CPU_STALL_INFO
scheduling-clock
statistics.
Next
acquisition
DECLARE_PER_CPU(struct
*rnp);
width
tools
<linux/syscore_ops.h>
impossible
shutting
immediately.
Execute
obvious
trust
Instead
(cmd)
arg,
interrupt,
NOT
*lock);
*lock;
helpers
1992
POSIX.1b
SIG_IGN
signals.
sigpending
synchronous
*sig
Our
notice
(sig
P
from_kuid_munged(current_user_ns(),
away,
*pid,
lose
info.si_code
@tsk
calls.
&info);
Schedule
Every
Other
expect
spin_lock_irq(&tsk->sighand->siglock);
sizeof(sigset_t))
(!err
*vma)
filtering
cmp
dereference
*r,
^=
OR
*root;
(pos
TRACE_EVENT_FL_USE_CALL_FILTER)
(op
fold
*call;
CONFIG_FTRACE_STARTUP_TEST
DATA_REC(NO,
"Failed
6,
length.
skip;
reverse
leading
64bit
TRACE_TYPE_UNHANDLED;
long);
tp
sizeof(*entry)
trace:
dependent
NSEC_PER_USEC);
rcu_torture_ops
2000
*rhp)
much,
set_user_nice(current,
(long)hcpu;
CPU_ONLINE:
CPU_DOWN_PREPARE:
Duplicate
%lx\n",
Interrupts
Enter
old_fs
map,
verification
uattr,
expensive
late
shift;
*this_rq)
flipping
load.
NOHZ
10))
ticks)
table,
Therefore
wakeups
rsp->n_barrier_done);
rnp);
cpu_to_node(cpu));
died
timed
problem.
quite
*group,
tail,
Increment
Parse
FMODE_WRITE)
dsize;
Event
*d_tracer;
d_tracer
(IS_ERR(d_tracer))
r1,
r5)
.ret_type
export
responsibility
only.
future.
accounting.
this_cpu_ptr(rsp->rda);
Exit
exit:
example
units
rnp->gpnum
s);
trace_rcu_future_gp(rnp,
useless
raw_spin_unlock_irq(&rnp->lock);
rdp->qlen_last_fqs_check
loop,
contention.
fine
undo
(!strncmp(str,
task_pid_nr(current));
unable
(rw
correctness
supplied
balancing.
percpu_rw_semaphore
run-time
entries,
mod))
exits
session
*old)
controls
old_uid_t,
old_gid_t,
kgid;
GROUP_AT(group_info,
CONFIG_DEBUG_MUTEXES
lock->wait_lock
Nadia
Yvette
spin_lock_irqsave(&q->lock,
spin_unlock_irqrestore(&q->lock,
@key:
waiter.
//
interruptible
bit);
&wait,
hashed
cpu)->disabled);
avg
*context,
RB_ROOT;
(p->nr_cpus_allowed
unlikely
task_rq_lock(p,
SCHED_DEADLINE
overloaded
rq_clock_task(rq);
finds
<tglx@linutronix.de>
load_weight
(s
intermediate
dist)
balanced
(cur
swapping
100)
down_read(&mm->mmap_sem);
32;
(running)
*tg
se->vruntime
raw_spin_lock(&cfs_b->lock);
cfs_rq->runtime_remaining
next:
slack
racing
&p->se;
schedstat_inc(sd,
*tmp,
sd_lb_stats
environment.
class.
rq;
all,
Search
second.
case:
elem
key.
*state
(arg_type
regs[insn->dst_reg].type
inserted
macros
happens.
container,
")\n\t=>
(*fn)(void
sources
(!len)
preempt_count
cputime,
*skb,
*e;
*tree)
audit_log_untrustedstring(ab,
map);
ftrace_branch_data
.selftest
----
discarded
raw_spin_lock_irqsave(&sem->wait_lock,
%current
*sym;
obj->funcs;
unload
RTC
*value)
store_gcov_u32(buffer,
kfree(iter);
*clk
tr
CONFIG_STACKTRACE
parsing
CONFIG_PROC_FS
<linux/debug_locks.h>
buf[64];
SCHED_FIFO
"BUG:
preempt_count());
rcu_read_unlock_sched();
proc_doulongvec_minmax,
rd
Build
cpumasks
cpusets.
set_tracer_flag(tr,
.set_flag
.close
rt_rq);
jiffies)
freq);
KDB_MAXBPT;
instruction.
enable_trigger_data
out_put;
van
<linux/posix-timers.h>
"tick-internal.h"
alive
*offset)
*offset
&work_debug_descr);
@pool
worklist
*worker
attrs
mutex_lock(&wq_pool_mutex);
@fmt:
turned
dev_to_wq(dev);
pretty
*gc
irq_data_get_irq_chip_data(d);
removed.
cell
supports
@domain
oldfs;
oldfs
set_fs(oldfs);
rmtp
options,
read.
comm);
trace_buffer_struct
=>
(l
"%u\n",
trace_option_dentry
PAGE_MASK;
ops);
@timeout:
TASK_INTERRUPTIBLE,
<linux/jiffies.h>
suspended
classes
CONFIG_LOCK_STAT
CONFIG_DEBUG_LOCKDEP
LOCKDEP_STATE(__STATE)
"lockdep_states.h"
LOCKDEP_STATE
circular_queue
data))
*hlock,
(unlikely(!debug_locks))
prev_hlock
(!debug_locks_off())
(!rb)
*base;
(value
ACCT_VERSION
raw_spin_lock(&ctx->lock);
__get_cpu_context(ctx);
*pmu
ctxn,
*hwc
*vma,
err_context;
child,
FUTEX_WAITERS
hb2);
do_for_each_ftrace_op(op,
multiplier
@desc:
irq_put_desc_unlock(desc,
unmask
*cur
tick_broadcast_device.evtdev;
bc
kstat_incr_irqs_this_cpu(irq,
@cp:
@ss:
ss);
init_css_set
&cgrp_dfl_root)
ssid);
@root
subtree
->css_online()
non-empty
length);
Elf_Sym
*size
symbol.
mmput(mm);
*cs;
%14s
(!data)
td->event
&rttest_event);
CONFIG_PRINTK
*lenp
@ppos:
from/to
put_user(0,
write_lock_irq(&tasklist_lock);
pgrp
ptracee
*sp,
trace->reset(tr);
..");
uncompressed
compression
(thr
header,
@alarm:
vec
jprobe
(!file)
__OLD_UTS_LEN);
arg5)
arg2);
kdb_machreg_fmt
kdb_symtab_t
kexec
log_next_seq)
msg_print_text(msg,
mutex_lock(&clocksource_mutex);
mutex_unlock(&clocksource_mutex);
mapkey
iter->cpu_buffer;
pr_warn("Could
&ctx->names_list,
context->type
time_state
calibration
ftbl);
f->fn
kexec_segment
*image)
mstart
mchunk;
#F,
%Ld.%06ld\n",
*word
*oldval,
(res->start
kdb_printf("due
map_info
uid_gid_map
projid
1992,
Torvalds,
probing
raw_spin_lock_irq(&desc->lock);
chips
msleep(100);
Scan
synchronize_rcu()
Undo
positives
bottom
Simple
RCU-tasks
WARN_ON(signal_pending(current));
path,
synchronize_srcu()
local_bh_enable();
flavor
*r)
definitions.
reporting
Silicon
Graphics,
(KDB_FLAG(CMD_INTERRUPT))
nextarg
Rafael
<linux/ktime.h>
smallest
unsafe
"safe"
Data
recently
linear
Link
list_for_each_entry(node,
ranges
gfp_mask);
@bm
touch_softlockup_watchdog();
(to
PAGE_SHIFT)
needed,
approximately
approach
From
Hibernation
(handle->cur
handle->cur
entirely
<linux/cgroup.h>
0),
css;
*tset)
propagate
css_task_iter_end(&it);
behind
interface.
level)
resched
Name
*isidle,
isidle,
among
combined
callback,
tracepoints
system_state
@nb:
handler;
available,
logical
cpumask_of(cpu));
meaning
way.
1991,
<linux/binfmts.h>
<asm/unistd.h>
thread,
locking.
STOP
pending,
Flush
signal,
priv
signr;
need_resched
handling.
SIGKILL
PIDTYPE_PID);
SEND_SIG_PRIV,
back.
Both
exit_code
unconditionally
want.
accordingly
(sigsetsize
changes.
probe.
compat_pid_t,
action;
__force
act,
granularity
risk
',
quick
(ch
infinite
times.
root);
filter);
(tmp)
synchronize_sched().
((1
*sd)
Takes
aligned
(k
failed;
current's
reference.
insns
processes.
trace_reg
LEN_OR_ZERO,
trace;
FILTER_OTHER);
*)call->data)->syscall_nr;
(num
.reg
sizeof(u32);
MODULE_LICENSE("GPL");
tickless
9
SCHED_FIFO,
Yes,
Go
increasing
ATOMIC_INIT(0);
%s:
Already
*rcu)
missing
say
<linux/clocksource.h>
kgdb_connected
connected
deep
failed:
ks->linux_regs);
Locking
*x)
io
thaw_processes();
(int
set_fs(old_fs);
*map
attr);
NO_HZ
algorithm
write.
calculated
8,
inode->i_private);
*td
-EAGAIN
CPU_DEAD_FROZEN:
*inode;
*tu;
*regs);
*offset,
delete
'-')
is_return,
interfaces
trace_define_field(event_call,
mm);
tracing_init_dentry();
r2,
__used
capable
near
WARN_ONCE(1,
Idle
outermost
neither
boot,
(in_nmi())
snap
comparisons
rnp->lock
concurrently
sublist
Apply
everyone
Someone
states.
significant
10)
queued.
smp_call_function_single(cpu,
CPU_UP_PREPARE:
nr_cpu_ids;
d;
content
CONFIG_HARDLOCKUP_DETECTOR
watchdog_enabled
LOCKDEP_STILL_OK);
prio)
&param);
PERF_EVENT_STATE_OFF)
%llu\n",
IPIs
Insert
smp_call_func_t
func(info);
Like
next_cpu
happens,
service
false)
overload
<linux/rwsem.h>
stable.
a;
jump_label_unlock();
*end)
iter++)
addresses.
commits
inode,
years
old_gid_t
<linux/hash.h>
wait->flags
Same
smp_wmb()
*word,
64)
region.
desc->lock
suspend/resume
(!cnt)
CAP_SYS_ADMIN))
clone
bp_type_idx
weight;
(enable)
@attr:
@dev_id:
runtime;
set_task_cpu(p,
Therefore,
seems
delta_exec
curr,
figure
raw_spin_lock(&rt_rq->rt_runtime_lock);
raw_spin_unlock(&rt_rq->rt_runtime_lock);
rq->cpu,
rb_node);
spare
*this_rq);
CONFIG_SCHED_HRTICK
put_prev_task(rq,
tsk_cpus_allowed(p)))
runqueues
Might
<trace/events/sched.h>
are:
se;
ms
PAGE_SHIFT);
gid;
placement
nid
dist);
fraction
capacity;
get_task_struct(p);
ourselves
env
spans
16)
sharing
start);
mappings
*cfs_rq);
achieve
correction
refer
reflect
left;
note:
clock,
span
overrun;
nest
distribution
CPU:
hierarchies
failed.
env);
p->on_rq
Structure
equals
(p->prio
cgroup's
(!parent)
(task
init.
external
"%d",
TRACE_SYSTEM
chars
*path,
1st
2nd
*insn)
insn->off,
R
calls,
packet
v
insn_cnt
seq_printf(sf,
cpu_stop_done
@arg:
rb_root
CONFIG_IRQ_TIME_ACCOUNTING
@cputime:
utime;
local_clock();
design
SRC;
IMM;
&tmp);
audit_log_start(NULL,
vfsmount
put_tree(tree);
bpf_htab
htab_elem
container_of(map,
*/,
extract
'/')
trace_seq_printf(&iter->seq,
.print_header
MODULE_STATE_COMING)
list_for_each_entry(p,
ap;
sem->count
rwsem_waiter
grant
sem;
KBUILD_MODNAME
klp_ops
*patch)
rtc_device
(status
*dev;
compilation
@v:
@seq:
.timer_create
console_cmdline
*array
ref;
opened
selftest
file)
pipe_inode_info
*pipe,
ftrace_func_t
below,
split
command,
MAX_RT_PRIO;
die
yield
locked,
CONFIG_PROC_SYSCTL
!capable(CAP_SYS_ADMIN))
increased
sign
new_mask);
put_task_struct(p);
'0'
rcu_read_lock_sched();
"ERROR:
what,
sched_domain_topology_level
custom
normalize
manner.
Couldn't
.print_line
*pi
uaddr,
*dev);
freq)
KDB_REPEAT_NO_ARGS);
kfree(data);
@file:
tracing_lseek,
Usually
out_reg;
seq_write_gcov_u32(seq,
0600,
Protected
(delta.tv64
unstable
stale
Updates
*dev
workqueue.
*wq;
ODEBUG_STATE_ACTIVE:
spin_lock_irq(pool->lock).
worker->pool;
traversal
*pwq)
wq;
flushed
requeued
scnprintf(buf,
(sscanf(buf,
DMA
immediately,
enable/disable
irq_gc_lock(gc);
irq_reg_writel(gc,
irq_gc_unlock(gc);
device_node
mutex_unlock(&irq_domain_mutex);
domain;
@
*node)
Step
(old)
loaded.
tk
Hat
dir,
-ERESTART_RESTARTBLOCK)
nsec)
*rec,
copy.
(tr->flags
(trace_array_get(tr)
"\t\t
kfree(s);
deletion
strip
consumer
sizes
USEC_PER_SEC);
raw_spin_lock(&lock->wait_lock);
clock_read_data
(seq
*hlock)
*stats;
graph_unlock();
*data),
lock:\n",
printk("\nother
it:
event_id
*rb
(B)
header;
(event->parent)
(action->flags
(!res)
(group_dead)
raw_spin_unlock(&ctx->lock);
ctx->lock
*pmu;
&pmus,
mutex_unlock(&ctx->mutex);
event_entry)
VM
((mask
uval
mm;
FLAGS_SHARED,
CONFIG_MMU
kthread_work
ftrace_trace_function
*hash;
while_for_each_ftrace_op(op);
do_for_each_ftrace_rec(pg,
rec)
filter_hash,
inc
(rec->flags
iter->pg
(len)
(buf)
irq_get_desc_lock(irq,
chip_bus_sync_unlock(desc);
*c
data->parent_data;
this_cpu_ptr(&tick_cpu_device);
&per_cpu(tick_cpu_device,
CLOCK_EVT_STATE_ONESHOT);
jitter
raw_spin_unlock(&desc->lock);
pidlist
css's
css_sets
*cset;
S_IRUGO
cftypes
@root:
length)
module_use
kernel_param_ops
.set
*sechdrs,
CONFIG_KALLSYMS
symtab
'.'
cs,
integers
css_cs(css);
cpuset_read_u64,
cpuset_write_u64,
@table:
@write:
@lenp:
31,
*wo,
upid
DYN_FTRACE_TEST_NAME();
warn_failed_init_tracer(trace,
!count)
REC_FAILED_NUM;
feature;
tp);
iter->cpu;
CONFIG_KRETPROBES
max_cycles
tk->ntp_error
timekeeping_update(tk,
clocksources
(opt
(arg2
Command
noop,
VT
log_next_seq;
raw_spin_lock_irq(&logbuf_lock);
log_first_seq)
raw_spin_unlock_irqrestore(&logbuf_lock,
timer->it.cpu.expires
*sample
mutex_unlock(&relay_channels_mutex);
read_subbuf
read_pos
lastchar
kdb_nextline
cp++;
buffer_data_page
raw_spin_lock_irqsave(&cpu_buffer->reader_lock,
raw_spin_unlock_irqrestore(&cpu_buffer->reader_lock,
rb_inc_page(cpu_buffer,
RB_WARN_ON(cpu_buffer,
*cpu_buffer
fetch_param
++result;
clc
sched_clock_data
"System
system_ram,
trace_seq_printf(p,
rdp;
(punc
word;
0x%lx
copied;
res.end
pd
@pinst:
mutex_lock(&pinst->lock);
mutex_unlock(&pinst->lock);
appears
Authors:
MODULE_PARAM_PREFIX
sequence.
non-zero,
Callback
container_of(head,
CONFIG_DEBUG_OBJECTS_RCU_HEAD
&rcuhead_debug_descr);
internally
debug_obj_descr
*ptr)
variant
execution,
words
heavy
Furthermore,
complain
pre-existing
forces
avoiding
(list)
list->next;
maintains
Uses
Unused.
Architecture
processes,
*)addr);
(argc)
kdbgetaddrarg(argc,
Wysocki
<linux/highmem.h>
bytes,
"original"
(res)
allocator
designed
area.
(PAGE_SHIFT
populated
safe_needed,
(!node)
*aux;
for_each_populated_zone(zone)
page_to_pfn(page));
max_zone_pfn;
*page
nr_copy_pages
nr_pages)
preallocated
slab
Count
nr_highmem
Except
(it
2),
@css:
forked
css_task_iter_next(&it)))
usual
synchronizes
*of,
NR_CPUS
non-idle
non-lazy
RCU's
completed;
rcu_for_each_leaf_node(rsp,
quiescent-state
Core
leader,
256
Protect
started,
implementations.
r1;
<linux/kmod.h>
machine.
Or
shutdown.
<tglx@timesys.com>
independent
lock);
detection.
*to,
<linux/tty.h>
<linux/signal.h>
<linux/nsproxy.h>
<linux/user_namespace.h>
x;
cleared,
participate
Ok,
SIGCONT
@t:
tracee
stopped.
GFP_ATOMIC
abort
tty
specifies
backward
notifications
change,
decide
other.
resuming
current->signal;
Setting
however,
Did
remove,
asked
anymore.
tgid
old_sigset_t
"Invalid
pred_stack
'!')
move,
*err,
AND
checks.
rec);
*filter
__free_filter(filter);
*dir,
op)
field;
max;
correctly
.count
*root
call,
@prev
*prog;
loads
legal
BPF_LD
f;
scenarios
sd);
silently
(trace_seq_has_overflowed(s))
syscall_nr;
syscall_nr
mutex_lock(&syscall_trace_lock);
mutex_unlock(&syscall_trace_lock);
Could
RCU_TORTURE_PIPE_LEN
trace_clock_local();
pool.
*rp)
rcu_bh
purposes
priority,
incrementing
kthread,
numbers.
Jason
<linux/kgdb.h>
Flag
instruction_pointer(regs);
KGDB_MAX_BREAKPOINTS;
tracing_on();
especially
CONFIG_MAGIC_SYSRQ
<linux/swap.h>
filp->private_data
accessible.
res);
res,
<linux/bpf.h>
rbtree
ptr,
(insn->imm
calculation
*pos
*pos);
rdp->rcu_qs_ctr_snap
hierarchical
exit,
put_task_struct(tsk);
omit
tu
path;
Probes
(file->f_flags
esize
ucb,
ftrace_event_name(call));
WARN_ON_ONCE(!rcu_read_lock_held());
.gpl_only
DEFINE_PER_CPU_SHARED_ALIGNED(struct
odd
trace_rcu_utilization(TPS("Start
trace_rcu_utilization(TPS("End
stats.
state:
(oldval
rdp->gpnum,
handlers,
(rnp->qsmask
obtained
preceding
Remember
ACCESS_ONCE(rsp->gp_activity)
ACCESS_ONCE(rsp->gp_flags)
order,
pair.
ACCESS_ONCE(rsp->gpnum),
raw_spin_unlock(&rnp->lock);
needing
rcu_barrier()
remainder
invoke_rcu_core();
hotplug.
wrap
strictly
RCU,
advanced
caller's
lglock
Most
implementation.
DEFINE_PER_CPU(unsigned
generally
..
.sched_priority
shut
(no
proc_dointvec_minmax(table,
<linux/pagemap.h>
<linux/irq_work.h>
removal
get_cpu();
Be
dummy
hinting
HAVE_JUMP_LABEL
b;
jump_label_lock();
Howells
container_of(rcu,
CLONE_THREAD)
record,
groups,
@inode:
uid)
*wait)
targets
waker
knows
UP
timeout;
anyone
avg;
Frederic
facility,
constraint
weight);
@handler:
performs
dev_id);
*dl_se)
*dl_se
*dl_rq
fallback
applying
(absolute)
rt_rq->rt_time
!(flags
queued)
sched_domain_span(sd)))
next_task
persistent
timeslice
min_vruntime
(entity_is_task(se))
weighted
nid)
dist
group_faults(p,
dst_nid)
nid;
max_faults
goal
searching
complexity
offsets
tracks
endif
se)
update_curr(cfs_rq);
*se;
segment
denote
cfs_b->runtime
(!se)
(cfs_rq_throttled(cfs_rq))
wl;
task_cpu(p);
load_idx
sg
wake_flags)
owning
TIF_NEED_RESCHED
cpu_idle_type
env->imbalance
lockdep_assert_held(&rq->lock);
rare
constraints.
false.
Hence
tmp);
out_balanced;
domains.
sd->flags
kzalloc_node(sizeof(struct
tg;
kref
(level
set_current_state(TASK_UNINTERRUPTIBLE);
(write
escape
read/write
free_cpumask_var(mask);
nr_irqs
(int)
Development
cmdline
audit_aux_data
ok,
verifier_state_list
insn->imm);
accessible
BPF_REG_0
fall-through
safety
readable
valid,
unloaded
added.
(1ULL
stopper
@fn:
-ENOENT
cpu_stop_fn_t
*str
&cputime);
Normally
aux
Fill
protecting
hash;
audit_filter_mutex
__cacheline_aligned_in_smp
AUDIT_CONFIG_CHANGE);
rule->tree
'/'
kmalloc
*l;
(can
%llu
headers
instead,
formats
TRACE_ITER_PRINTK))
va_start(ap,
va_end(ap);
sem
*owner)
operation,
(func
mutex_unlock(&klp_mutex);
KERN_ERR
",");
dev
merge
int);
(!info)
prev->next
counters.
magic.
added,
poll_table
.poll
cd;
put_clock_desc(&cd);
trace_type
*ppos);
nsecs);
*parent);
func)
commands.
allocates
neg
cnt))
buf[cnt]
enqueue_task(rq,
dequeue_task(rq,
p->prio;
succeed
either.
run-queue
p->sched_class
Doing
raw_smp_processor_id());
(!mm)
running;
dead.
attr,
find_process_by_pid(pid);
param);
retlen
breaks
%5d
(p->state
Requeue
sd_data
(request
k;
append
same.
**);
rt_rq->rt_runtime
reaching
tr->current_trace;
(is_graph())
tracepoint"
(!futex_cmpxchg_enabled)
uaddr2,
(cmd
*work);
rt_prio_array
for_each_sched_rt_entity(rt_se)
rq_of_rt_rq(rt_rq);
reclaim
prev_prio)
next)
"";
*c)
&file->triggers,
seq_release(inode,
strsep(&param,
trigger_data,
(!param)
(data->count
event_trigger_free,
7;
accuracy
ktime_sub(now,
Switch
device,
protected.
wq_pool_mutex
issued
cursor
@pwq:
*worker,
worker->flags
wq,
bogus
long.
for_each_node(node)
@node
rebind
Value
(r
irq_data_get_chip_type(d);
d->mask;
clr,
irq_get_irq_data(virq);
-ENOTSUPP;
*domain;
irq_data->domain
@virq:
CONFIG_IRQ_DOMAIN_HIERARCHY
optimistic_spin_node
(next)
*)addr
pr_warning("Could
(WARN_ON_ONCE(ret))
save_len
s->seq.len
symbolic
node_lock
rqtp,
x)
size))
console.
'='
list_for_each_entry(tr,
&ftrace_trace_arrays,
distinguish
print_entry
tracing_start_cmdline_record();
iter->cpu);
iter->cpu,
&l);
iter->iter_flags
0/1
*d_tracer)
tried
*ptr,
Userspace
buffer_ref
(!tr)
mount
lock->wait_lock.
raw_spin_lock_irqsave(&task->pi_lock,
get_task_struct(task);
top_waiter
TASK_UNINTERRUPTIBLE,
debug_locks_off();
(class->name_version
task_pid_nr(curr));
(DEBUG_LOCKS_WARN_ON(!irqs_disabled()))
context:
(a
contended
do_each_thread(g,
while_each_thread(g,
perf_event__output_id_sample(event,
*rb,
watermark,
AUX
configuration
CLOCK_REALTIME
~SIGEV_THREAD_ID)
*kc
Another
*tmp
event_type_t
cgrp;
PERF_EVENT_STATE_INACTIVE;
PERF_EVENT_STATE_ACTIVE)
cpuctx->task_ctx
ctx->task;
raw_spin_lock_irq(&ctx->lock);
perf_event_ctx_unlock(event,
ctx))
for_each_task_context_nr(ctxn)
sec;
mutex_unlock(&event->mmap_mutex);
aux_unlock;
header->size
perf_output_end(&handle);
sample;
&sample,
mmap_event->event_id.header.size
logging
swevent_hlist
(event->attr.type
hlist
*swhash
lists.
newval
requeue_pi
pi_state->owner
*hb;
spin_unlock(&hb->lock);
expire.
&q,
semaphore.
timeout);
irq_trace())
*pid;
ftrace_profile_page
pg->next;
update,
ftrace_func_probe
ftrace_ops_list)
filter_hash
ftrace_ops_hash
ftrace_pages_start;
ops->func_hash->filter_hash;
rec->ip);
while_for_each_ftrace_rec();
modifying
mutex.
(iter->flags
ftrace_graph_data
ftrace_ret_stack
(!desc
&action->thread_flags))
chip_bus_lock(desc);
(new->flags
vs.
(tick_broadcast_device.mode
tick_broadcast_mask);
TICKDEV_MODE_PERIODIC)
(dev->state
*trsp)
raw_spin_lock(&desc->lock);
sorted
*ss,
ssid,
(key
rwlock
list_for_each_entry(link,
preloaded
@from:
cgroup_subsys_state,
s->private;
CONFIG_MODULE_UNLOAD
mod->module_core
mod->module_init
"bad
pos_css,
callback_lock
trial
rebuild
csn;
chip->irq_eoi(&desc->irq_data);
60;
*lenp;
Reads/writes
*lvalp
hang
((clone_flags
unshare
(unshare_flags
NSEC_PER_SEC)
ts->tv_sec
*vec
bases
&hrtimer_debug_descr);
CONFIG_TIMER_STATS
@expires:
pip,
tracer_init(trace,
%d\n%s:
avail,
spin_lock_irqsave(&base->lock,
*alarm,
(TVR_BITS
tvec
@j:
timer_jiffies
(platform_mode
DBG_MAX_REG_NUM
kgdb_hex2long(&ptr,
packets
ftrace_graph_ent_entry
kprobe_insn_cache
text_mutex
TK_MIRROR
shifted
TK_CLOCK_WAS_SET);
tick_length
(event->type_len)
event->array[0]
who,
*p_event)
p_event);
consoles
*text,
raw_spin_lock_irqsave(&logbuf_lock,
n))
console_lock();
dumper
*x,
signaled
(scancode
ENTER
(cs->flags
+------+
(event->type_len
@event:
RING_BUFFER_ALL_CPUS
tail_page,
event->time_delta;
(dolock)
__u64
*dir
down_write(&trace_event_sem);
up_write(&trace_event_sem);
ftype),
pm_qos_class
cpu_maps_update_begin();
cpu_maps_update_done();
f->uid);
(f->op
AUDIT_SUBJ_USER:
AUDIT_SUBJ_ROLE:
AUDIT_SUBJ_TYPE:
AUDIT_SUBJ_SEN:
AUDIT_SUBJ_CLR:
(n->type
n->type
MQ
data->orig.fn))
genl_info
CONFIG_KEXEC_FILE
IORESOURCE_MEM
nr_segments,
purgatory_info
*cmdline,
(*cur
kallsym_iter
(rand1
mcs_spinlock
trace_seq_putc(p,
(verbose)
preemptible-RCU
rnp->gp_tasks
defined(CONFIG_NO_HZ_FULL)
system-idle
__field(int,
@ns:
oldlen
out_kfree;
nlen,
@cap:
superior
ns,
*conflict;
res->start
kdbtab_t
numeric
*c++
printable_char(*cp++);
kdb_current_regs);
*uprobe;
vaddr)
uprobe;
blk_log_generic
pinst->flags
mutex_unlock(&sparse_irq_lock);
32)
<paulmck@us.ibm.com>
explanation
Inform
CONFIG_PREEMPT_RCU
covers
c);
switch,
such,
Global
Track
needwake;
needwake
(needwake)
situations
B,
over.
incorrectly
gcc-internal
referenced
Perform
unloaded.
*nb,
located
Machek
<pavel@ucw.cz>
suspend.
unnecessary
correspond
Tree
end_pfn
marking
PG_ANY);
allocation.
highmem,
(2)
divided
err_out;
only).
nr_highmem)
sizeof(long),
(ie.
overwritten
@buffer
returned,
Instead,
FREEZING
whatever
css)
anytime
post
Consider
thaw
traversal.
strstrip(buf);
nbytes;
Of
CONFIG_RCU_FAST_NO_HZ
gpnum;
->blkd_tasks
(only
boost:
Possible
temporarily
ended
nocb
start.
initialization.
Forward
*rdp);
definitions
guard
text.
reboot_mode
(i.e.,
cpumask_first(cpu_online_mask);
call:
task_active_pid_ns(current);
namespaces
CONFIG_KEXEC
2006,
rtmutex
prio;
*waiter);
Modified
SLAB
*m;
ptracer.
recalc_sigpending();
info->si_code
purpose
Bad
finding
fatal
chosen
real-time
group);
%d.\n",
*sighand;
success;
dead,
*cred,
compatibility
deliver
info.si_signo
task_cputime(tsk,
0x80)
care.
do_exit
reports
respect
lot
&current->restart_block;
add,
nset,
size_t,
new_set;
preclude
sigset_t's.
too,
contained
-EINTR
tgid,
groups.
(unlikely(ret))
shall
stack_t
older
alter
Signal
02111-1307,
Tom
opstack_op
postfix_elt
pattern
root)
*err
*src;
dest);
OP_NONE)
indeed
children;
Optimize
(file->flags
*filter_str,
&filter);
dir->subsystem;
';
separated
.ops
event_id,
rec;
CONFIG_SECCOMP_FILTER
<linux/filter.h>
@prev:
*prev;
instructions.
BPF_ADD
BPF_JEQ
*f
parent)
free_prog;
branches
registers.
hooks
syscall.
repeatedly
flags.
prepared
(op)
compat
irq_flags;
local_save_flags(irq_flags);
(!rec)
rctx,
arch-specific
weak
descriptors
2005,
bursts
torture_param(bool,
5,
verbose
RCU_TORTURE_PIPE_LEN)
.completed
.call
*rcu))
readers,
inversion
flight,
Repeatedly
DEFINE_TORTURE_RANDOM(rand);
MAX_NICE);
giving
ULONG_MAX;
CPU_DOWN_FAILED:
test:
memory");
Wessel
GDB
bootup
at:
regs;
hw
secondary
tasklet
pm_restore_gfp_mask();
-ENOTTY;
(offset)
PLUMgrid,
http://plumgrid.com
(IS_ERR(map))
*map)
hashtable
f)
(!f.file)
*value,
map->key_size)
free_key;
fact,
*key;
*insn
silly
space,
turns
machines
assuming
accumulate
synchronizing
@offset:
a0
pushing
(time_before(jiffies,
raise
{0,
(!n)
[1]
periodically
says
decay
wrt
observed
skew
However
mcount
classic
'?';
rnp->grplo,
(rnp
0x1)
rspdir,
*cur;
(tsk
cpu)))
<linux/namei.h>
DATAOF_TRACE_ENTRY(entry,
FETCH_FUNC_NAME(memory,
u64,
tu;
is_return
(argv[0][0]
'p',
dsize
.arg1_type
.arg2_type
RET_INTEGER,
<linux/wait.h>
->lock
switch.
batches
*flags,
period?
user)
oldval;
reject
Detect
comparing
rnp->completed
rnp->gpnum,
rcu_gp_kthread_wake(rsp);
raw_spin_lock_irq(&rnp->lock);
wakeup,
rnp->qsmask
complete,
barrier.
rq->lock.
often
Still
^^^
readers.
ignoring
indicating
Currently,
(now
carry
covered
rounds
CPU_UP_CANCELED:
Before
arithmetic
though,
heavily
hrtimer,
threshold
nanosecond
this_cpu);
(regs)
pin
asynchronously
.store
threads,
old);
!write)
submit
request.
**bio_chain)
cpumask;
assignment
WARN_ON(!irqs_disabled());
*info),
processors
cpus;
HAVE_RT_PUSH_IPI
policy.
exclude
selection
mod)
<linux/cred.h>
<linux/init_task.h>
%p,
Destroy
creds
copy,
modifies
CONFIG_KEYS
fork()
*inode)
do_exit()
user,
rgid,
ruid,
euid,
*user_ns
kgid
Molnar:
2004,
*l
try_to_wake_up()
removes
pointers,
schedule_timeout(timeout);
wait_bit_key
*action,
manipulated
KDB_BADINT;
*user_ns,
fs_struct
user_ns,
2014
"%d
<linux/kdebug.h>
(nr
stub
check:
*context)
*this
ascii
irq_handler_t
&p->dl;
runtime)
Fail
double_unlock_balance(rq,
whenever
rq_clock(rq)
reason,
former
p->nr_cpus_allowed
pulled
slip
cached
task)
racy,
cpu_to_node(i));
oldprio)
prio,
tasks:
msec
wake-up
gain
small.
bounce
overall
dst_cpu
imp
Compare
(sd)
recheck
faster
1));
decays
group's
join
accurate
outstanding
choose
y
-->
unit
1024)
aggregate
update_entity_load_avg(se,
sleeps
(curr
*cfs_b)
dest_cpu
(rq->curr
rq->lock,
defer
expires);
total;
somewhat
this_cpu,
Select
*sd
new_cpu
tmp)
hot
class,
current.
correct.
imbalance.
expand
---
property
do,
fbq_type
0x01
We've
kernels
SCHED_CAPACITY_SCALE;
runqueues.
@env:
(rq->nr_running
ourself
~0UL;
presence
rid
to:
____cacheline_aligned;
isolated
(need_resched())
Upon
mostly
pids
CONFIG_CHECKPOINT_RESTORE
*table;
"%*pb\n",
v);
cpu_online_mask))
bump
CONFIG_AUDITSYSCALL
insn,
R0
matched
(like
insn->src_reg
(insn->src_reg
%x\n",
BPF_K
12
e;
src_reg
access.
compile
prev_state
*cft,
cpu_stopper
.fn
multi_stop_data
repeat:
support.
index)
opportunity
stime,
scaling
depend
socket
bpf
[BPF_LD
do_div(tmp,
16:
@addr:
result,
chunk;
(size)
grabbed
buckets
GFP_ATOMIC);
insertion
trace_handle_return(&iter->seq);
((void
*)p
trace_bprintk_fmt
format.
format,
debug_check_no_locks_freed((void
returning.
name))
*func)
obj);
free:
RAM
&now);
PM_SUSPEND_MAX)
info->next;
32);
header.
get_posix_clock(fp);
on)
get_clock_desc(id,
&cd);
.timer_set
.timer_del
.timer_get
dev)
records.
*)data;
special,
comm[TASK_COMM_LEN];
(in_interrupt())
*trace);
output.
utility
trace_event_triggers.c).
ignored.
event_trigger_type
*file);
*sig)
replacing
wheel
*from,
(parent)
CONFIG_LOCKDEP
Cross
dest_cpu;
changes,
number,
cpuset,
owns
wake_flags);
schedstat_inc(rq,
raw_spin_unlock_irqrestore(&p->pi_lock,
link)
(mm)
reload
CONFIG_DEBUG_PREEMPT
semaphore,
paths.
ask
interrupted.
wont
current_cred(),
param,
&attr);
min_t(size_t,
overlapping
describes
b,
table.
doms
replaced
printk(KERN_EMERG
runtime);
PF_EXITING))
(arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;
.val
.opts
trace_buffer_unlock_commit(buffer,
&tracer_flags,
.allow_instances
robust_list
val2,
rt_se
!CONFIG_SMP
RLIM_INFINITY)
*bc)
kdb_breakpoints;
bp++)
"\n",
*c,
non-NULL
written,
ERR_PTR(-ENODEV);
combination
logged
3:
enable;
(WARN_ON(ret
fn)
((char
@count:
Arjan
Ven
profile_hit
read++;
S_IRUGO,
Started
(tick_do_timer_cpu
(!strcmp(str,
KTIME_MAX)
softirq.
*attrs;
sched-RCU
workers.
match,
worker;
*pwq
*worker)
wq_barrier
routine.
times,
RO
*attr;
*nr_calls)
max_stack_trace.nr_entries
sizeof(buf))
register.
gc->chip_types;
irq_domain_ops
mutex_lock(&irq_domain_mutex);
(hwirq
(virq
virq);
old)
node->locked
right)
type));
field->ip,
trace_seq_init(s);
bytes)
/proc
*tv,
restart->fn
&r);
&ts);
&global_trace;
RING_BUFFER_ALL_CPUS);
resize
frees
memcpy(buf,
buffers,
TRACE_ARRAY_FL_GLOBAL)
ftrace_entry
internals
ent
iter->cpu
\n"
(iter->cpu_file
tracing_get_cpu(inode);
tracing_open_generic_tr,
re-enable
*pid
filp,
ftrace_buffer_info
do_div(t,
d_cpu,
directory.
layer
owner);
[4]
out_unlock_pi;
lock's
timeout.
arches
wrapping
<trace/events/power.h>
hashes
already,
(!debug_locks_off_graph_unlock())
depth)
(debug_locks_silent)
(depth
chain_key
conflicting
(unlikely(current->lockdep_recursion))
check_flags(flags);
BUG:
printk("...
code:
handle->size
aux_head
PMU
order;
"Trying
timer;
.nsleep
clockid_to_kclock(which_clock);
shot
msi_desc
msi_msg
msg);
vma->vm_end
*cpuctx
rcu_head);
ctx)
put_ctx(ctx);
run_end
alone
PERF_FORMAT_ID)
update_context_time(ctx);
perf_event_ctx_lock(event);
ctxn;
hwc->sample_period
&ctx->event_list,
list_for_each_entry_safe(event,
Dynamic
perf_event_pid(event,
perf_event_tid(event,
perf_output_begin(&handle,
&handle,
&sample);
output,
cpy_name;
PATH_MAX
vma,
hrtimers
group_leader
event->clock
newval;
(uval
key2
faulted
@hb:
FUTEX_KEY_INIT;
atomically.
&key2,
out_put_keys;
put_futex_key(&key2);
q.key
notes
kthread_create().
CONFIG_PREEMPT_TRACER
CONFIG_IRQSOFF_TRACER
FTRACE_OPS_FL_RECURSION_SAFE
ftrace_stub;
&ftrace_list_end)
*hhd;
hhd
calltime
kfree(entry);
ftrace_ops_init(ops);
checked.
ftrace_rec_iter
ignores
slen
subsystem.
irq_get_desc_buslock(irq,
irq_put_desc_busunlock(desc,
*action);
IRQS_WAITING);
raw_spin_lock_irqsave(&tick_broadcast_lock,
raw_spin_unlock_irqrestore(&tick_broadcast_lock,
td->evtdev;
clockevents_set_state(dev,
Out
range[j].start
massive
.writelock
.write_delay
.writeunlock
max-heap
printk()
Patrick
css_set_rwsem
percpu_ref
css,
dec
ssid)))
~(1
ss->name);
(!strcmp(token,
token
@cfts
*cfts)
->css_offline()
(length
CONFIG_UNUSED_SYMBOLS
symsearch
sprintf(buffer,
add_taint_module(mod,
relocations
SHT_NOBITS)
info->len
discard
bytes;
time_t
&top_cpuset)
@cs
(cs
*mm;
CONFIG_TRACE_IRQFLAGS
%11u\n",
lock_stat_data
namelen
test_thread_data
tid
*negp,
*lvalp,
*valp,
(*ppos)
proc_put_char(&buffer,
table->maxlen/sizeof(unsigned
lval
files_struct
ts->tv_nsec
USER_HZ);
new_base
expires_next
HRTIMER_MAX_CLOCK_BASES;
timerqueue_node
*cpu_base
this_cpu_ptr(&hrtimer_bases);
Sleep
@sp:
CONFIG_PM_SLEEP
CONFIG_PM_DEBUG
audit_pid
nlmsghdr
*skb)
@ab:
escaped
from_kgid(&init_user_ns,
swap_map_page
&alarm_bases[alarm->type];
alarmtimer_type
TVN_BITS))
usermodehelper_disabled
hibernation_mode
hibernate
hibernation_ops)
mutex_set_owner(lock);
__mutex_lock_common(lock,
remcom_out_buffer,
*bpt_type
ftrace_graph_ret_entry
&(per_cpu_ptr(data->cpu_data,
per_cpu_ptr(data->cpu_data,
kprobe_mutex
kprobe_opcode_t
hlist_for_each_entry_rcu(p,
list_for_each_entry_rcu(kp,
mutex_lock(&text_mutex);
mutex_unlock(&text_mutex);
KPROBE_TABLE_SIZE;
vaddr
tk->tkr_mono.shift;
tk->xtime_sec;
base[0]
o
CONFIG_NTP_PPS
uint,
KILL_TEST();
(!user)
gid_t,
make_kgid(ns,
uid_t,
make_kuid(ns,
errno;
RLIMIT_CPU
exit_err;
arg4
sysinfo
message.
text_len;
log_next_idx;
text_len
&endp,
log_from_idx(idx);
log_next(idx);
seq++;
@dumper:
firing,
buf->subbufs_produced
buf->subbufs_consumed
buf->chan->n_subbufs;
buf->chan->subbuf_size;
read_start
scancode
keychar
&clocksource_list,
grep
kdb_grepping_flag
suspend_grep
|-->|
RB_EVNT_HDR_SIZE;
*bpage)
BUF_PAGE_SIZE
tail_page
HEAD
buffer_page,
*bpage,
(success)
per_cpu(tasklet_vec,
cpu).head;
async_domain
AUDIT_OBJ_USER:
AUDIT_OBJ_ROLE:
AUDIT_OBJ_TYPE:
AUDIT_OBJ_LEV_LOW:
AUDIT_OBJ_LEV_HIGH:
to_send;
ktime_to_ns(ktime_get());
f->data
nr_segments;
&image->purgatory_info;
kimage_entry_t
(image->file_mode)
*crash_size,
expected\n");
(sechdrs[i].sh_type
"%-45s:%21Ld\n",
->expmask
h->size
perf_callchain_entry
bin_convert_t
bin_net_decnet_conf_vars
PF_SUPERPRIV
KB_MASK;
GFP_KDB);
quoted
bytesperword;
mdcount
return_instance
vaddr);
put_uprobe(uprobe);
user_namespace.
uid_gid_extent
MASK_TC_BIT(rw,
blk_rq_bytes(rq),
blk_add_trace_bio(q,
bio,
(tc
*pd)
*pd;
padata_priv
pcpumask,
doing,
mutex_lock(&sparse_irq_lock);
obviously
Russell,
Andrea
Arcangeli
friends
rcu_scheduler_active
Checks
@head:
fixup.
*rhp,
rhp,
rates
delimited
returns,
Although
pr_err("INFO:
detects
Without
scanning
exiting.
carried
parameters,
pr_info("Running
Oberparleiter
<oberpar@linux.vnet.ibm.com>
((addr
Independent
"COPYING"
archive
"kdb_private.h"
optionally
pointers.
reasonable
argcount,
nextarg;
&addr);
NOTREACHED
<rjw@sisk.pl>
<linux/pm.h>
<asm/io.h>
exceed
restoring
storing
*res;
(page
*ca,
*ret;
zone.
stored.
wrapper
rtree
nodes;
end_pfn;
*zone,
block_nr
*dst
clear_nosave_free);
walks
pfn;
pfn);
non-highmem
devices.
term
(count)
(s390
small,
largest
(all
nr_highmem;
works.
appear.
time)
zeroed
computed
handle->buffer
highmem_pbe
cnt++;
Reserve
buffer)
*p1,
container_of(css,
*parent_css)
*freezer
mutex_lock(&freezer_mutex);
mutex_unlock(&freezer_mutex);
freezer_attach()
conform
cgroup_taskset_for_each(task,
tset)
((task
.css_alloc
.css_free
<paulmck@linux.vnet.ibm.com>
Define
course,
DIV_ROUND_UP(NR_CPUS,
rcu_node's
effect,
State
Refused
why,
____cacheline_internodealigned_in_smp;
orphaned
_rcu_barrier()
7)
Values
fqs_state
lazy,
<linux/kexec.h>
preparing
trouble
effort
Registers
%-ENOENT
Restart
registered.
rebooting
orderly
guess
*str;
',');
debugging.
w;
PI-futex
<linux/cn_proc.h>
<asm/cacheflush.h>
"audit.h"
signals,
signal_wake_up(t,
(x
task->jobctl
q;
(first)
info.
caller,
siglock.
behaviour
uid_eq(cred->uid,
sid
trapped
for_each_thread(p,
why;
SIGCHLD
*pending;
current_uid());
blocked,
interesting
handling,
autoreap
info.si_errno
info.si_pid
info.si_uid
0x7f;
semantics
&info,
pointless
info))
hook
scheduled,
gonna
afterwards.
sleeping.
spin_unlock_irq(&sighand->siglock);
PF_EXITING
tsk->flags
compat_size_t,
set;
*from)
codes
uinfo)
@sig:
-ESRCH
CLONE_THREAD
compat_siginfo
cloned
preserves
oact)
Order
predicate
tail;
Filter
arrays
@str:
(including
above).
*buff,
(pred->op
(pred->left
OP_OR)
&data);
kfree(filter->filter_string);
ops.
call->flags
field->filter_type
*elt;
right.
set_str,
**
cut
<linux/tracepoint.h>
"(e
(g
(b
(d
err);
Google,
seccomp_data
BPF_LDX
Explicitly
BPF_SUB
BPF_MUL
BPF_DIV
BPF_AND
BPF_OR
BPF_XOR
BPF_LSH
BPF_RSH
Synchronize
unprivileged
children.
(via
computing
Show
actions
*sym,
str[KSYM_SYMBOL_LEN];
*ent
syscall_trace_enter
syscall_trace_exit
#type,
file->tr;
(hlist_empty(head))
&rctx);
TRACE_REG_REGISTER:
TRACE_REG_UNREGISTER:
TRACE_REG_PERF_REGISTER:
TRACE_REG_PERF_UNREGISTER:
TRACE_REG_PERF_OPEN:
TRACE_REG_PERF_CLOSE:
<linux/stat.h>
"Use
fake
verbose,
__rcu
selecting
Definitions
longdelay_ms
longdelay_ms)))
20000)))
preempt_schedule();
RCU_TORTURE_PIPE_LEN;
.ttype
rcu_sync_torture_init,
.started
.deferred_free
.sync
.exp_sync
.cb_barrier
.stats
messages.
delays
schedule_timeout_uninterruptible(1);
cur_ops->cb_barrier();
schedule_timeout_interruptible(1);
1)))
Leave
but...
created,
"End
scenario
\"%s\"\n",
torture_init_end();
kthreads.
Debug
emit
addr))
debugger.
client
resets
exceptions
exception.
snapshot_data
data->swap
Unlock:
filesystems
*)arg);
problems
&offset);
*tl;
PTR_ERR(map);
err_put;
of,
*)uattr
calculating
exp;
described
window,
slight
LOAD_FREQ;
function:
(n)
adjusted
calc_load_update
messing
idle)
32,
16,
drift
calculation.
rnp->grphi,
rsp);
structs
(!tsk)
*per_cpu_ptr(ht->store,
(tsk)
get_task_struct(tsk);
checking.
processing.
oldstate
uprobe_trace_entry_head
trace_uprobe,
maxlen
*tu)
*group)
'r'
':')
fail_address_parse;
*parg
name:
conflicts
file.\n");
listing
((file->f_mode
O_TRUNC))
*ucb,
dsize,
tu->tp.size
call->event.type,
dsize)
entry))
(tu->tp.flags
kfree(link);
DEFINE_FIELD(unsigned
(!done)
R1
<linux/stop_machine.h>
<linux/ftrace_event.h>
rcu_num_lvls
boot-time
contexts.
ATOMIC_INIT(1),
scheduler,
delay.
flavors
rsp
rdp->nxttail[i]
occur.
irq_enter()
(We
CPU-bound
four
(ULONG_CMP_GE(jiffies,
scheduler.
smp_processor_id(),
warnings
preventing
another.
buffered
online,
deadlocks
encountered
raw_spin_lock(&rnp->lock);
(user)
rcuo
disappear
vice
__call_rcu(head,
nested.
hand,
8))
free_cpumask_var(cm);
counter,
CPU?
-not-
CPU_UP_PREPARE_FROZEN:
(nr_cpu_ids
scheduler's
booting
softlockup
thanks
<asm/irq_regs.h>
differently
NMI_WATCHDOG_ENABLED
DEFINE_PER_CPU(bool,
couple
dumping
duration,
timestamp.
permanently
(event)
writing.
*bio;
bio_chain);
put_page(page);
<linux/rculist.h>
arrive
latencies
non-blocking.
serialization.
redundant
applicable
Prefer
increases
priorities
*brw)
T
Ensures
3.
<linux/sort.h>
iter_start;
end))
verifies
(!mod)
<linux/key.h>
GLOBAL_ROOT_UID,
cred,
*cred;
date
*old
old->uid)
old->gid)
@old:
new->fsgid
egid,
*group_info)
group_info->ngroups;
nr_exclusive,
identify
TASK_RUNNING;
wait_bit_queue
word,
instance,
produce
1998-2006
2007-2008
cpu_file
rwsem_set_owner(sem);
<linux/proc_ns.h>
ipc
1000)
max);
count))
S_IRUSR,
Weisbecker
Thanks
<linux/irqflags.h>
cpumask)
cover
triggered,
irq_devres
Scheduling
dl_task_of(dl_se);
rb_entry(parent,
&parent->rb_left;
&parent->rb_right;
link);
later_rq
specifying
dl_rq_of_se(dl_se);
rq_of_dl_rq(dl_rq);
dl_se->runtime
are,
overflowing
anyway,
replenishment
rq->clock
here:
(!task_on_rq_queued(p))
schedulable
solution
&rq->rt;
(relative)
(!task_current(rq,
sd_flag,
*curr;
prefer
p->se.exec_start
hrtick
best_cpu
(this_cpu
preempts
src_rq
*new_mask)
new_mask))
(weight
p's
.put_prev_task
Mike
<linux/mempolicy.h>
units:
lw->inv_weight
8);
cfs_rq)
refcount;
total_faults;
buffers.
(sched_numa_topology_type
faults;
task_faults(p,
task_numa_env
src_cpu,
taskimp,
slightly
movement
p->numa_preferred_nid)
workload
p->numa_scan_period
orders
p->se.sum_exec_runtime;
recursively
b);
priv);
impact
kzalloc(size,
no_join;
WARN_ON_ONCE(p
rq_of(cfs_rq);
ifdef
(se->on_rq)
multiplication
sa->avg_period
se->avg.decay_count
se->avg.load_avg_contrib
cfs_rq->runnable_load_avg
contribute
SCHED_LOAD_SCALE
approximate
position.
update_curr()
reschedule.
sched_clock
truly
performing
deactivated
repeated
RUNTIME_INF
(expires
hrtimer_forward(timer,
prev_cpu
this_load
possible)
sd->groups;
min_load
cpuidle_state
target.
(!(sd->flags
gran
idle;
next.
(env->idle
CPU_NEWLY_IDLE)
TASK_ON_RQ_QUEUED;
check_preempt_curr(rq,
sched_group_capacity
issues
tasks'
*sgs)
(sgs->sum_nr_running
numbered
SMT
balancing,
options.
512
although
_before_
target_cpu
Originally
(time_after_eq(jiffies,
(curr)
CONFIG_SCHED_AUTOGROUP
*tg);
<linux/acct.h>
*kref)
container_of(kref,
kref);
nr);
*pid_ns,
Access
proc_dir_entry
unusable
PDE_DATA(inode));
desc->dir);
Hewlett-Packard
audit_cap_data
collected
mqdes;
*n,
audit_net
*krule,
CONFIG_AUDIT_TREE
size_t);
wrote
BPF_X)
*(%s
bpf_ldst_string[BPF_SIZE(insn->code)
3],
insn->code);
registers,
mem
CONST_IMM;
BPF_LD_IMM64
((u64)
IMM
endianness
BPF_ABS
pseudo
env->prog->len;
peek_stack;
insn_cnt;
(one
TRACE_FN,
%llx
trace_printk
TRACE_PRINT,
30
DEFINE_PER_CPU(u64,
*cpuusage
reset)
"%llu
%lld\n",
cputime)
waiter,
enabled;
*stopper
&per_cpu(cpu_stopper,
completes.
list_for_each_entry(work,
buf)
<linux/tsacct_kern.h>
*cpustat
kcpustat_this_cpu->cpustat;
cputime);
Accumulate
tsk->signal;
Tick
rq->idle)
rtime
Atomically
Depending
write_seqlock(&tsk->vtime_seqlock);
write_sequnlock(&tsk->vtime_seqlock);
*fp)
Base
operating
[BPF_STX
BPF_W]
(skb->data
<asm/sections.h>
exception_table_entry
RW
*tree;
(tree)
*chunk
audit_chunk,
mark);
n++)
spin_unlock(&entry->lock);
owner->root
get_tree(tree);
fsnotify_put_mark(old_entry);
(unlikely(!ab))
*rule,
audit_entry,
audit_tree,
mutex_lock(&audit_cmd_mutex);
mutex_unlock(&audit_cmd_mutex);
fsnotify
select_bucket(htab,
limit,
ok.
%lld
CONFIG_BRANCH_TRACER
events\n");
----\n");
shows
fmt;
t_show,
t_stop,
reinitializing
rwsem_waiter,
writer.
wake_up_process(waiter->task);
left.
my
waiter.task
list_add_tail(&waiter.list,
__rwsem_do_wake(sem,
slowpath
Block
decremented
pr_debug("%s
current->comm);
TASK_INTERRUPTIBLE);
replacement
patched
KLP_DISABLED;
func++)
KLP_ENABLED)
mutex_lock(&klp_mutex);
limit.
MODULE_STATE_GOING
suspend,
GCOV_COUNTERS;
v)
*fp,
(!clk)
put_posix_clock(clk);
lists,
*path)
bpf_array
FTRACE_ENTRY(name,
kprobe_trace_entry_head
kretprobe_trace_entry_head
occurs.
on/off
var
trace_options
Your
(in_irq())
*buf);
*s);
instance.
identifies
retrieve
CONFIG_EVENT_TRACING
Allocates
Work
Gleixner,
<linux/blkdev.h>
<linux/delayacct.h>
<linux/unistd.h>
smp_send_reschedule(cpu);
*rt_se
from)
p->static_prio
(task_cpu(p)
&arg);
introduce
task_on_rq_queued(p);
ktime_set(0,
cpu_to_node(cpu);
-1.
task_pid_nr(p),
p->comm,
raw_spin_lock_irqsave(&p->pi_lock,
stat;
*rd
attr->sched_period
externally
for_each_online_cpu(i)
nanoseconds.
*class
rq->nr_running
userspace,
things.
(policy
attr->sched_priority
UID
cpumask_t
@param:
err_size;
(!alloc_cpumask_var(&mask,
broken
interval)
Migrate
exec
validation
*table
Flags
rd;
*rd;
built
*sdd
*per_cpu_ptr(sdd->sd,
(!first)
*cpu_map)
cpu_map)
&tmp,
compares
%pS\n",
etc
proc_dointvec(table,
cgroup_exit()
timings
Empty
trace_function(tr,
register_ftrace_function(tr->ops);
graph)
TRACE_GRAPH_PRINT_DURATION)
ctx_switch_entry
wakeup_task
reset.
save_flags
.print_max
.flag_changed
.use_max_tr
err_unlock;
*rt_b,
*rt_b
&rt_rq->active;
rt_rq->highest_prio.curr
rt_rq->rt_throttled
group_rt_rq(rt_se);
closest
dev->event_handler
CONFIG_TICK_ONESHOT
(*handler)(struct
broadcasting
down_read(&uts_sem);
up_read(&uts_sem);
task_lock(task);
task_unlock(task);
Breakpoint
instruction_pointer(regs));
Handles
'*'
@rec:
list_for_each_entry_rcu(data,
Trigger-specific
pointer)
(!data->count)
(data->count)--;
.trigger_type
.unreg
.get_trigger_ops
.set_filter
set_trigger_filter,
data->private_data;
out_put:
<linux/stacktrace.h>
Assume
prof_on
intervals
*ppos;
2005-2007,
Slow
duty
CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
unlike
time_delta
jiffie
(ktime_t)
dev->next_event.tv64
highres
Andrew
wq->mutex
mutexes
flusher
wq_device
idr
assertion
pool,
pool)
lockdep_assert_held(&wq_pool_mutex);
anywhere
worker,
spin_lock_irq(pool->lock)
outer
(!(wq->flags
WQ_UNBOUND))
@dwork
pool;
restart;
reliably
work_color
task_pid_nr(current),
@target
armed
incoming
WARN_ON_ONCE(wq->flush_color
freed,
alloc_workqueue_attrs(GFP_KERNEL);
Grab
&workqueues,
short,
brought
peek
nl
notifier_chain_register(&nh->head,
notifier_chain_unregister(&nh->head,
notifier_call_chain.
nr_calls);
(*p
FTRACE_OPS_FL_RECURSION_SAFE,
Library
irq_flow_handler_t
@irq_base:
*gc;
IRQ_*
*chip;
*irq_data
hwirq;
domain->revmap_size)
hwirq)
(domain
%i
*irq_data)
bindings
IRQ_TYPE_SENSE_MASK;
@nr_irqs:
%d)\n",
MCS
@node->next
(N
(void)
mid;
pagefault_disable();
pagefault_enable();
*tk;
tk->tp.flags
mutex_unlock(&probe_lock);
tp->nr_args;
entry\n");
probe.\n");
PM_SUSPEND_ON)
Inc,
tool
printf
s->seq.len;
(unlikely(seq_buf_has_overflowed(&s->seq)))
converting
@path:
@cnt:
*dentry;
out_err:
pr_warn("could
gcov_info_filename(info));
Intel
latency_record
(q
Stephen
tv,
fail,
rmtp;
rlim)
sigevent
timer_t,
timer_id,
*pages;
comm
CONFIG_TRACE_ENUM_MAP_FILE
internal_trace_puts("***
***\n");
@tr
parser
tr->current_trace
"Could
tracing_stop_cmdline_record();
trace_buffer_iter(iter,
sym_flags);
TRACE_TYPE_PARTIAL_LINE;
entry->pid,
TRACE_TYPE_HANDLED;
(trace_empty(iter))
iter);
tracing_release_generic_tr,
set_ftrace_filter
Filters
ptr++;
(%s)\n",
cpu)->entries
splice_pipe_desc
timestamps.
iter->cpu_file);
(m)
filling
(*ppos
usec_rem
funcs
file_system_type
printk(KERN_TRACE
(rt_mutex_has_waiters(lock))
rt_mutex_owner(lock);
argument.
(owner)
[7]
chwalk,
-EDEADLK;
versus
next_lock
rt_mutex_top_waiter(lock);
Replace
acquired.
*timeout,
(signal_pending(current))
unconditionally.
clocks.
NSEC_PER_SEC,
Fall
lock_classes
(hlock->read)
printk("%s",
class->name_version);
Hash
list:
target_entry,
INFO:
*uninitialized_var(target_entry);
&target_entry);
Prove
defined(CONFIG_PROVE_LOCKING)
this:\n");
curr->lockdep_depth
lock_chain
[%p]
hlock->irq_context)
depth:
another,
print_ip_sym(ip);
2008-2011
handle->rb;
(A)
D
event->parent;
rcu_dereference(event->rb);
rb;
__output_copy(handle,
rb->aux_nr_pages
pg;
(pg
(pgoff
system-wide
*timr,
.nsleep_restart
*timr;
timr
ktime_add(now,
current->group_leader;
INT_MAX)
TIMER_RETRY;
*rmtp)
rmtp);
(that
severe
flag)
(ops
perf_clock();
*cpuctx)
list_for_each_entry_rcu(pmu,
cpuctx->task_ctx);
fdget(fd);
perf_event_time(event);
event->ctx
raw_spin_unlock_irqrestore(&ctx->lock,
PERF_FORMAT_TOTAL_TIME_RUNNING)
sample_type
(event->attach_state
PERF_EVENT_STATE_OFF;
perf_event_update_userpage(event);
next_event
context;
local64_set(&hwc->period_left,
*pmu,
pmu;
(has_branch_stack(event))
mutex_lock(&event->mmap_mutex);
deprecated
preempted.
rctx
stack_size,
.header
.misc
(attr_mmap2
only)
__u16
futex,
*hb)
@uaddr:
pi_state;
(pid)
Lookup
uaddr
vpid
*hb1,
double_unlock_hb(hb1,
put_futex_key(&key1);
-EWOULDBLOCK;
-ERESTART_RESTARTBLOCK;
@sem:
raw_spin_lock_irqsave(&sem->lock,
raw_spin_unlock_irqrestore(&sem->lock,
-ETIME;
(!preempt_trace()
X.509
db_result
(!(ops->flags
**p;
stddev
*stat;
hhd,
calltime;
*hash)
(ftrace_hash_empty(hash))
new_hash
IPMODIFY
rec->ip;
@ip
start_pg;
iter->flags
put_pid(pid);
ret_stack
(on
IRQ_GET_DESC_CHECK_GLOBAL);
IRQF_SHARED)
(irq
new->flags
*act)
CLOCK_EVT_FEAT_ONESHOT))
(!(dev->features
*bc,
nr_range;
range[j].end
lock_stress_stats
mdelay(longdelay_ms
cxt.nrealwriters_stress
variables.
calling.
##
(%NULL
said
cgroup_parent(cgrp);
cgroup_css(cgrp,
cgroup_for_each_live_child(child,
hierarchies.
csets
cset_link)
*res
seq_puts(seq,
cgroup_sb_opts
forking
reusing
&preloaded_csets);
invisible
iteration,
last)
cgroup_filetype
*dentry)
S_IWUSR,
'\n'
ARCH_SHF_SMALL
Waiting
sym,
&info->sechdrs[i];
crc
Init
*use;
LOCKDEP_NOW_UNRELIABLE);
(mod->taints
'F';
sched_annotate_sleep();
attribute_group
long)base
mod->core_size);
SHF_ALLOC))
info->secstrings
~0UL
mod->core_size
mod->init_size
mod->num_symtab;
num,
~(unsigned
KSYM_NAME_LEN);
fmeter
mems_allowed.
*pos_css;
ndoms
Have
top_cpuset.mems_allowed
dependencies:
*seq
~(IRQS_REPLAY
td->mutexes[id]
<linux/oom.h>
&one_hundred,
whitespace
*valp
(copy_from_user(kbuf,
free_page(page);
<linux/fs_struct.h>
mm.
CMOS
time_constant
firsttime
timestamps
ts.tv_nsec
USER_HZ
comm,
idx++)
cpupri_vec
reaper
maxrss
clockid
new_base;
debug_object_init(timer,
debug_object_free(timer,
clock_was_set();
tim,
srcu_readers_active_idx()
1/10
REC_FAILED_NUM
(ab)
res=%d",
payload
audit_log_format(*ab,
(s.mask
audit_unpack_string(&bufp,
security_release_secctx(ctx,
signed
pathname
swap_map_page_list
"boot"
swsusp_header,
error)
err2;
hib_wait_on_bio_chain(&bio);
atomic_set(&d->stop,
wake_up(&d->done);
decompression
*alarm)
early.
TVN_MASK;
sub_info->retval
trace_suspend_resume(TPS("machine_suspend"),
pm_restore_console();
&remcom_in_buffer[1];
lock->owner_cpu
magic");
TRACE_GRAPH_PRINT_CPU)
TRACE_GRAPH_PRINT_PROC)
TRACE_GRAPH_PRINT_ABS_TIME)
holder
*ap,
&p->list,
list_del_init(&op->list);
unoptimizing
ap
&kprobe_table[i];
kfree(p);
long)p->addr,
ri
*session
tk_fast
printk_deferred("
clock->shift;
tk->ntp_error_shift;
listener
timekeeping_get_ns(&tk->tkr_mono);
nsecs;
mult_adj
xtime_nsec_1
xtime_nsec_2
*bpage;
RINGBUF_TYPE_PADDING:
RINGBUF_TYPE_TIME_EXTEND:
(niceval
gid_eq(cred->gid,
arg3
cnts
tp_event->class->reg(tp_event,
console_lock
fragment
clear_seq;
clear_idx;
*msg;
text_len);
devkmsg_user
user->seq
user->idx
user->buf[len++]
printed_len
sizeof(buf)
newcon->flags
CON_BOOT)
thread_group_cputimer
*cputimer
@chan:
buf->offset
mutex_lock(&relay_channels_mutex);
n_subbufs
wait_for_common(x,
KP
list_for_each_entry(cs,
escape_delay
kdb_printf(kdb_prompt_str);
prompt
CMD_BUFLEN);
(buf1[0]
event->time_delta
rb_event_length(event);
rb_irq_work
+-----+
cpu_buffer->nr_pages_to_update
iter->read_stamp
(commit
rb_test_data
t->next
per_cpu(tasklet_hi_vec,
#item,
is_signed_type(type),
ftrace_event_enable_disable(file,
__ftrace_set_clr_event(tr,
top_trace_array();
offsetof(typeof(field),
fetch_func_t
(!req)
CONFIG_NR_CPUS)
ctx->trees;
f->gid);
AUDIT_FILTERKEY:
&context->names_list,
*context;
unroll_tree_refs(context,
to_send
time_adjust
pps_normtime
deref_fetch_param
(CHECK_FETCH_FUNCS(deref,
(CHECK_FETCH_FUNCS(symbol,
arg->fetch.fn))
maddr
mutex_unlock(&kexec_mutex);
crash_size,
VMCOREINFO_OFFSET(page,
VMCOREINFO_OFFSET(pglist_data,
purgatory
symbol_end
address)
preh_val
target(rand1);
pr_err("kprobe
num_tests++;
errors++;
field->buf);
trace_seq_buffer_ptr(p);
seq_print_ip_sym(s,
.raw
PN
P(x)
pr_info("\tRCU
rnp->boost_tasks
KDB_BADWIDTH;
h_offset;
h_used
nsecs\n",
%u.%06lu
usec_rem,
oldlen)
sizeof(*vec);
cap);
read_lock(&resource_lock);
read_unlock(&resource_lock);
orig_end;
res.start
IORESOURCE_BUSY;
first)
callchain_cpus_entries
KDB_NOTIMP;
KDB_ENABLE_MEM_READ
KDB_ENABLE_INSPECT);
AUDIT_BITMASK_SIZE;
audit_rule_data
data->values[i]
uprobe,
*uprobe)
*uc)
&mm->flags);
*area;
current->utask;
&siginfo);
extents;
@targ:
*targ,
lower_ns
seq_user_ns(seq);
setgroups
*bio,
padata_parallel_queue
*pinst)
cbcpumask);
rmmod
"torture_onoff
mutex_unlock(&fullstop_mutex);
mk
gfp,
<linux/async.h>
progress:
for_each_irq_desc(i,
(!(desc->istate
minus
candidate
2001
primitives
(t->rcu_read_lock_nesting
nonzero
rcu_read_unlock()
(!rcu_is_watching())
situation.
awaken
(might
object)
ODEBUG_STATE_NOTAVAILABLE:
on-stack
suppress
CONFIG_TASKS_RCU
60
Post
elapsed,
synchronize_rcu_tasks()
memory-ordering
addition,
Complain
wait,
job.
accesses.
cleaning
Spawn
Early
compiled
serve
profiling.
*counters,
n_counters)
MODULE_STATE_GOING)
none.
argv[1]
Recursive
<linux/bootmem.h>
<asm/mmu_context.h>
pbe
page)
safe_needed)
PG_SAFE);
*list,
lp;
gfp_mask;
lp
pfns
inner
bitmap.
operate
*ca)
bm->cur.node_pfn
mem_extent
*cur,
zone->zone_start_pfn;
zone_end_pfn(zone);
&bit);
Starting
code)
PAGE_SHIFT,
forbidden_pages_map
free_pages_map
pfn++)
pfn_to_page(pfn);
copying
for(;;)
duplicated
resume.
to_free_normal
(3)
anonymous
max_size
excessive
pressure
min_t(unsigned
indicated
pages_highmem
swsusp_show_speed(start,
needed:
memory_bm_next_pfn(bm);
@handle
(!handle->cur)
.
Free;
css_freezer(css);
(parent
@css
(!(freezer->state
skipped
pre-order
CFTYPE_NOT_ON_ROOT,
.legacy_cftypes
NUM_RCU_LVL_1
(NR_CPUS)
NUM_RCU_LVL_2
NUM_RCU_LVL_3
posted
boosted.
boosting.
though.
partition.
Shared
Grace
6)
wakeups.
dyntick
jiffies_till_first_fqs
forcing.
third
guarded
0x1
0x2
rcu_data,
*rsp);
->dynticks_nesting
interrupts,
case).
<linux/kmsg_dump.h>
reboot.
(!cpu_online(cpu))
halt
pid_namespace,
do_exit(0);
kernel_power_off();
*dummy)
keyboard
choice
pr_warn("Failed
str++;
Gleixner:
Various
adjustments
*l,
Jim
sig))
ready;
;)
TIF_SIGPENDING);
exiting,
@task->sighand->siglock
dying.
(task->flags
completes
*user;
SIG_IGN)
ptraced,
SI_USER;
itimers
dropped.
queues.
putting
Probably
thread_group_empty(p))
soon.
slower
SIGKILL);
(info
struct.
overflow,
lost.
unexpected
i)))
insn);
count++;
(unlikely(sighand
->siglock
ways
BSD
*lock
child.
tied
rcu_read_lock(),
info.si_status
reaped
kind
%true,
TASK_TRACED.
current->exit_code
TRACED
INTERRUPT
separately
do_notify_parent_cldstop(current,
why);
inactive.
freezable_schedule();
examine
cancelled
stopped,
wanted
try_to_freeze();
relock;
Trace
tasklist_lock.
maybe
dump.
(failed)
how,
@sigsetsize:
current->blocked;
sigsetsize);
None
awakened
uinfo,
only,
(act)
spin_unlock_irq(&p->sighand->siglock);
@set:
new_set);
@act:
do_sigaction(sig,
compat_old_sigset_t
compatibility.
handler)
__set_current_state(TASK_INTERRUPTIBLE);
-ERESTARTNOHAND;
subsystem's
*)(event
pred->offset);
(*addr
characters.
type.
'!'
(!i)
&search,
&not);
strlen(search);
search,
not;
*preds,
pred;
FILTER_PRED_INVALID)
checks:
treat
as:
rec,
(!filter)
n_preds
pos)
event_mutex
(filter
*system,
n_preds;
dest->index
~FILTER_PRED_FOLD;
file->flags
*type)
OP_EQ
OP_NE)
fn;
filter_opstack_pop(ps);
top_op
op);
(top_op
(elt->op
MOVE_DOWN)
applied
ps,
*ps
filter_str);
(filter)
*system
system->filter
g
(f
facility.
evaluate
architecture.
BPF_W
codes.
smp_read_barrier_depends();
(!sd)
assert_spin_locked(&current->sighand->siglock);
child;
Expects
synchronization.
thread)
(thread
0)))
dropping
(This
enters
sock_fprog
__GFP_NOWARN);
bounded
syscall,
skip:
Archs
*start;
*trace;
0x%lx\n",
call)
kfree(call->print_fmt);
NR_syscalls)
(ftrace_trigger_soft_disabled(ftrace_file))
TRACE_REG_PERF_ADD:
TRACE_REG_PERF_DEL:
<linux/linkage.h>
<linux/trace_clock.h>
MODULE_AUTHOR("Paul
fqs
"Time
tests.
(nrealreaders
rcu_torture_cb);
rcu_read_delay,
.fqs
.irq_capable
0x1;
pr_alert("%s%s
Runs
registering
invoked,
sp;
interval.
(torture_must_stop())
relatively
probability
rhp
disabled:
expediting
torture_type);
started;
statistics,
task");
get_seconds()
warning.
barriers.
drive
<jason.wessel@windriver.com>
2003-2004
listed
above:
George
<linux/sysrq.h>
master
CPU#
:-)
Weak
BREAK_INSTR_SIZE);
(kgdb_break[i].state
recover
dbg_remove_all_break();
breakpoint.
us:
.index
kgdb_breakpoint();
%s,
atomic_inc(&snapshot_device_available);
free_basic_memory_bitmaps();
~PAGE_MASK;
-ENODATA;
freeze_processes();
2011-2014
fdget(ufd);
bpf_map_get(f);
kmalloc(map->key_size,
(!ptr)
kfree(key);
err_put:
accessors
(insn->code
prog
attr
lifted
load-average
minimize
serious
Variables
calc_load_update;
exp,
forward,
scale,
ticks,
mult/shift
sane
curr_jiffies
ACCESS_ONCE(jiffies);
raw_spin_unlock(&this_rq->lock);
*)m->private;
rdp->qlen;
rsp->gpnum;
__exit
<linux/smpboot.h>
(kthread_should_stop())
mutex_lock(&smpboot_threads_lock);
mutex_unlock(&smpboot_threads_lock);
properly,
die.
seconds)
quickly.
(sizeof(struct
is_return)
trace_uprobe_filter
*filename;
MUST
pr_warning("Failed
mutex_unlock(&uprobe_lock);
(!group)
':');
MAX_EVENT_NAME_LEN,
is_return);
seq_list_next(v,
(is_ret_probe(tu))
TP_FLAG_PROFILE)
(!link)
tu->tp.flags
TP_FLAG_TRACE)
(enabled)
*event_call)
mm)
need.
R2
r2;
Increase
kthread_prio
Delay
not)
*rdtp;
raw_cpu_ptr(rsp->rda);
rdtp
ulong
jiffies_till_next_fqs
20;
initiated
RCU_NEXT_TAIL;
rdtp->dynticks_nesting);
pid:
atomic_inc()
oldval
DYNTICK_TASK_NEST_MASK)
DYNTICK_TASK_NEST_VALUE;
upcalls
messed
rdtp->dynticks_nmi_nesting
thinks
Similarly,
marks
implicit
*maxj)
NO_HZ_FULL
Dump
rdp->nxtlist
initializing
rnp->level,
rnp->lock,
equal,
incremented.
requested.
accelerate
(last
Assign
invoke,
saw
->qsmaskinit
rnp->qsmaskinit
rdp)
reduces
over,
rnp->grpmask;
rnp;
rnp->parent;
rdp->qlen_lazy
rdp->qlen
adopt
cost
ordering.
masks.
*tail
(user
primitive
!lock_is_held(&rcu_lock_map)
!lock_is_held(&rcu_sched_lock_map),
section");
synchronize_sched_expedited();
recommended
cpu_online_mask,
num_online_cpus());
barrier,
->n_barrier_done
transition.
proceed.
view
CPU_UP_CANCELED_FROZEN:
range.
Match
lockups
chunks
sample_period;
waste
hardlockup
sample_period
5);
Preemption
get_irq_regs();
generating
MAX_RT_PRIO
PTR_ERR(event);
disabled\n");
CONFIG_SYSCTL
new)
*bdev,
rw
notifier_from_errno(-ENOMEM);
csd
*csd)
-ENXIO;
spinlocks.
protocol
Loop
llist_head
this_cpu)
controlling
IPI.
buddy
<linux/lockdep.h>
bugs
executes
<linux/static_key.h>
(((unsigned
executing.
Architectures
*iter_start
*iter_stop
iter_stop;
static_key_mod
*entries;
(mod)
mod->num_jump_entries;
MODULE_STATE_COMING:
MODULE_STATE_GOING:
GLOBAL_ROOT_GID,
objective
CONFIG_SECURITY
validate_creds(new);
keyring
old->euid)
old->egid)
derived
*new
alternative
new->fsuid
inode);
24
gid)
container_of(lock,
Chambers,
_after_
set_current_state(state);
(ie
imply
wait_bit_action_f
TASK_NORMAL,
accessor
*val
retrigger
&cp,
RING_BUFFER_ALL_CPUS;
R/W
Derived
LOCK_CONTENDED(sem,
dependency.
(ns
min)
usecs;
<fweisbec@gmail.com>
<linux/hw_breakpoint.h>
old_idx
before.
.task_ctx_nr
perf_sw_context,
.event_init
.add
.del
err_alloc;
managed
@handler
claiming
@dev
irqflags,
Deadline
dl_bandwidth
dl);
*dl_b,
dl_b->bw
global_rt_runtime());
dl_b->total_bw
dl_rq->earliest_dl.next
(!rq->online)
pushable
deadline.
**link
(*link)
(leftmost)
rq);
later_rq);
that:
dl_se->deadline
pi_se->dl_runtime;
runtime.
budget
residual
if:
thinking
soft,
deadline,
rq)
sched_dl_entity,
like:
*rt_rq);
deadline)
cpudl_set(&rq->rd->cpudl,
dl_rq);
p->dl.dl_boosted
update_curr_dl(rq);
updated,
(sd_flag
target;
migrated,
rq->curr,
pushed
0-lag
best_cpu;
((cpu
rq->cpu
p->dl.dl_bw);
-1))
unlocking
Again,
.dequeue_task
.check_preempt_curr
.pick_next_task
.select_task_rq
.set_curr_task
.task_tick
.prio_changed
.switched_to
.update_curr
improvements
nanoseconds)
notion
precise
to)
workloads
(either
relationship
(because
update_cfs_rq_blocked_load(cfs_rq,
for_each_leaf_cfs_rq(rq,
task_of(se);
&rq->cfs;
methods:
sched_entity,
run_node);
Nodes
Maintain
(!next)
se->load.weight);
(se
cfs_rq->curr)
rq_clock(rq_of(cfs_rq))
(p->numa_preferred_nid
1)];
backplane
faults,
p->total_numa_faults;
src_load
dst_load
Moving
direction
about.
satisfy
domains,
env.dst_nid);
group_weight(p,
(faults
numa_group->active_nodes))
ratio
NUMA_PERIOD_THRESHOLD
(where
areas
quickly
Scale
sched_max_numa_distance;
locate
grp
(time_after(jiffies,
find_vma(mm,
VMAs
NSEC_PER_MSEC;
shares;
cfs_rq->tg;
bounds
historical
load_avg
sched_avg
delta_w
1024;
SCHED_CAPACITY_SHIFT;
Figure
contrib;
w/
se->avg.load_avg_contrib;
cfs_rq->blocked_load_avg
wakeup)
Indicate
(unlikely(delta
normalized
update_cfs_shares(cfs_rq);
excess
order:
curr)
left)
cfs_rqs
(cfs_b->quota
expiring
*cfs_b,
(runtime
Requires
cfs_rq->runtime_enabled
encountering
calculates
root_task_group
@wl
avg_load;
(local_group)
SCHED_CAPACITY_SCALE)
shallowest_idle_cpu
cpu_rq(i);
(idle
whereas
sd->child;
with,
measure
]
of:
0x02
lockdep_assert_held(&env->src_rq->lock);
interleave
iteration.
list_for_each_entry_safe(p,
Right
enqueue.
*busiest;
Local
*sds)
0UL,
total,
situation
@sds:
modes
(busiest->avg_load
(so
etc).
rebalancing
balance_cpu
sd,
cpu_active_mask);
ld_moved
iterations
sd->last_balance
Attempts
one)
initiates
raw_spin_unlock_irq(&rq->lock);
(sd
(with
class:
TASK_RUNNING)
nr_cpu_ids,
buflen)
Many
set_bit(0,
injected
"kernel",
CONFIG_GENERIC_PENDING_IRQ
"%*pbl\n",
desc->dir,
*)(long)irq);
"%10u
__array(
supporting
Company,
L.P.
ino;
mq_attr
*dentry,
portid,
portid;
audit_pid)
PTR_TO_STACK
PTR_TO_MAP_VALUE
bpf_reg_type
imm
insn_idx
verifier_stack_elem
log_len
log_size
3]
r%d
insn->off);
0x%x
verbose("R%d
value_regno);
(read
write)
check_mem_access(env,
BPF_SIZE(insn->code),
validity
check_func_arg(env,
&map);
(map
regs[insn->dst_reg].imm
ALU
BPF_MOD
BPF_REG_0)
11
15
insn_state[t]
env->prog->insnsi;
push_insn(t,
err_free:
slot,
insn_idx);
inserted.
UINT_MAX
exceeded
__field_struct(
shown
__field
TRACE_GRAPH_ENT,
TRACE_GRAPH_RET,
(%d)
phys
ca
*ca
charge
cycle_now
cycle_now;
@fn(@arg)
partially
stop_machine
executions
leaked
(NULL
(node
ksoftirqd
@rq:
cputime_one_jiffy;
*ut,
*st)
thread_group_cputime(p,
stolen
stime);
31)
followed
(new
(old
__vtime_account_system(tsk);
(ptr
__GFP_ZERO
@ctx:
BPF_B]
BPF_H]
BPF_DW]
BPF_IND
32:
64:
SRC)
IMM)
*(u8
Probe
built-in
hash_lock
fsnotify_group
tree;
*chunk)
container_of(entry,
chunk->dead
list_del_init(&p->list);
audit_tree_group);
p++)
audit_log_string(ab,
audit_free_rule_rcu);
killing
*node
attributes.
*htab
bpf_htab,
trace_printk()
(fmt[i]
Generate
constants
expect)
p++;
|\n");
percent
correct,
*a
*b
*fmt;
**start,
ap)
hand
t_show(struct
t_stop(struct
t_start,
t_next,
sem->owner
*waiter;
(waiter->type
stole
sem);
style
&sem->wait_list);
set_task_state(tsk,
safe,
owner)
cpu_relax()
hack
(except
obj->mod
func->old_name;
*obj;
(obj
patch->objs;
obj++)
*kobj)
kobj_type
*patch,
successful.
control.
*dest,
gcov_link
dir;
pm_states[state]);
-ENODEV)
(prev)
fi_idx;
ct_idx;
(fi_idx
fi_idx
info->n_functions;
fi_idx++)
(ct_idx
ct_idx
num_counter_active(info);
kfree(info);
@pos:
675
Mass
Ave,
Cambridge,
02139,
on);
|\
audit_put_watch(watch);
Translate
audit_panic("error
applies
*watch
<linux/ring_buffer.h>
<linux/trace_seq.h>
__field(type,
FTRACE_ENTRY
PARAMS(tstruct),
PARAMS(print),
"trace_entries.h"
continues
taken,
*dir;
events;
etype,
opt
trace_pipe
ftrace_init_array_ops(tr,
TRACE_ITER_PRINTK
traceoff
decision
(used
out_fail;
(printk_ratelimit())
(tg
MAX_NICE)
Dave
semaphores
Implemented
<linux/timer.h>
<linux/times.h>
SCHED_FEAT(name,
"features.h"
SCHED_FEAT
(copy_from_user(&buf,
file_inode(filp);
delay)
involve
asm(""
tg_visitor
SCHED_IDLE
priority:
for_each_class(class)
new_cpu);
migration_arg
wrong,
allowed,
you're
p->policy
&fair_sched_class;
to_ratio(period,
because:
finish_task_switch
Reading
re-check
ctx_state
it).
prio);
task_current(rq,
Actually
Fixup
@policy:
sizeof(*attr);
security_task_getscheduler(p);
new_mask;
out_put_task;
cpuset's
yield()
policy)
overflow;
proc_dostring,
root-domain
cpu_isolated_map);
span)
cpumask_size(),
(last)
last->next
sd_flags
"wrong
properties
other,
assumption
cpumasks.
intersect
doms_new
cpu_possible_mask,
100%
MMU
ONLY
IS
serviced
serializes
css);
rt_period;
rt_runtime;
own.
rt_runtime
quota,
latency_tracer,
Chambers
tracing_dl
continue,
graph,
unregister_ftrace_function(tr->ops);
(set)
(likely(disabled
ftrace_now(cpu);
this:
pr_info("wakeup
"failed
compat_long_t
*uaddr
earlier,
(pending)
val3);
rt_se;
*rt_rq;
rt_task_of(rt_se);
looked
rt_rq->rt_runtime;
for_each_rt_rq(rt_rq,
sched_rt_bandwidth(rt_rq);
iter->rt_runtime
settings
(prio
lowest_rq
(soft
management.
CONFIG_GENERIC_CLOCKEVENTS
*curdev,
*newdev,
&len);
(KDB_DEBUG(BP))
kdb_printf("%s
Space
"*")
kdb_bfd_vma_fmt
"[<vaddr>]",
*console)
rec))
list_entry(v,
(copy_from_user(buf,
long)buf);
*number;
(glob[0]
kfree(trigger_data);
(!strlen(number))
kstrtoul(number,
out_reg:
data->filter_str);
tracing_snapshot();
*enable_data
*system;
easily
iter->record
par;
',')
%ld)\n",
cpu)[1]
virt_to_page(per_cpu(cpu_profile_hits,
count--;
(copy_to_user(buf,
<trace/events/timer.h>
&per_cpu(tick_cpu_sched,
timeouts
write_sequnlock(&jiffies_lock);
tick_do_update_jiffies64(now);
long)hcpu;
Full
iowait
delta_jiffies
(expires.tv64
High
%WORKER_UNBOUND
effect.
PL:
____cacheline_aligned_in_smp;
put_pwq()
for_each_cpu_worker_pool(pool,
pi)
for_each_pool_worker(worker,
*pwq,
set_work_data(work,
Unless
@pool.
wake_up_process(worker->task);
NOT_RUNNING
nr_running.
@work.
get_work_pwq(work);
flush_color
del_timer()
pool->id);
pwq->pool;
timer->expires
@dwork:
try_to_grab_pending()
Sanity
mutex_lock(&pool->attach_mutex);
mutex_unlock(&pool->attach_mutex);
allocations.
pool);
mayday
rescuers
restart:
grabs
items.
requeueing
Finish
pwq)
chained
underneath
.list
full.
wq->first_flusher
overflowed
list_for_each_entry(tmp,
wq->flags
*attrs)
@gfp_mask:
(!attrs)
attrs);
pwq_adjust_max_active(pwq);
@attrs
wq->name);
apply_workqueue_attrs(wq,
put.
@...:
","
associations
list_for_each_entry(wq,
";
bus_type
long));
((*nl)
@nr_to_call:
@nr_calls:
Entry
halted
blocking_notifier_head
chain,
srcu_notifier_head
heads
this_size
parent_ip;
per_cpu(trace_active,
SEQ_START_TOKEN)
<linux/irqdomain.h>
(!(mask
num_ct,
syscore_ops
.suspend
.resume
Controller
first_irq,
(domain)
h;
found;
easier
irq_data->hwirq
irq_data);
irq_default_domain;
@hwirq:
specified,
virq)
%10u
%c
irqchip
deactivate
xchg()
tp_module
debug_print_probes(*funcs);
@tp:
gone.
module's
performed.
crash.
stride
mid
here;
Created
*kp,
tk->tp.nr_args;
(trace_kprobe_is_return(tk))
~KPROBE_FLAG_DISABLED;
KPROBE_FLAG_DISABLED;
mutex_lock(&probe_lock);
symbol,
create_trace_kprobe);
&tk->tp.call;
true))
mutex_unlock(&autosleep_lock);
simplify
*mem,
written.
@dentry:
fs
'.');
((node
err_remove;
one:
trace.max_entries
trace.entries
&trace);
*txc)
(COMPAT_USE_64BIT_TIME)
*restart)
compat_itimerval
tms
&s);
RLIM_INFINITY;
stat_addr,
ru)
compat_itimerspec
tp)
tv;
compat_ulong_t,
<linux/tracefs.h>
<linux/poll.h>
early,
trace_buf_size
tracing_on
global_trace.trace_buffer.buffer;
ftrace_trace_stack(buffer,
(!tr->allocated_snapshot)
tr->allocated_snapshot
&tr->trace_buffer;
tr->max_latency;
tsk->comm,
-EBUSY)
type->name);
ring_buffer_record_disable(buffer);
ring_buffer_record_enable(buffer);
saved_cmdlines_buffer
arch_spin_unlock(&trace_cmdline_lock);
strcpy(comm,
NMIs
recursion,
recording.
tracing_update_buffers();
iterating
iter->ent
(iter->snapshot
&l))
trace_access_unlock(iter->cpu_file);
m);
trace_print_seq(m,
TRACE_ITER_CONTEXT_INFO)
(iter->trace
FUNCTION
Clears
buffer\n"
CONFIG_RING_BUFFER_ALLOW_SWAP
iter->trace_buffer
Open
reenable
TRACE_ITER_LATENCY_FMT)
(t)
"\n
sub-buffers
sub-buffer
Format:
set_ftrace_filter\n"
seq_open(filp,
kmalloc(sizeof(*s),
trace_seq_init(&iter->seq);
POLLIN
sret
(rem
.pages
"%lu\n",
(!m)
30,
entry\n",
out_free_tr;
cpu_all_mask);
0200,
newer
*owner
task->pi_lock
config
comparison
[R]
[2]
[3]
[5]
[6]
[8]
[9]
[10]
carefully
walk.
(waiter
@waiter
transient
rt_mutex_deadlock_account_lock(lock,
(timeout
(likely(!ret))
-ETIMEDOUT
lock->magic
Complete
Shift
convenient
non-boot
recurse
arch_spin_unlock(&lockdep_lock);
hlock->class_idx
unique.
Debugging
class_filter(class);
print_lockdep_off("BUG:
low!");
*class,
'.';
(!depth)
list_for_each_entry_rcu(class,
&all_lock_classes,
class->name);
distance,
this;
**target_entry)
CPU0
printk("\n
(!debug_locks_off_graph_unlock()
debug_locks_silent)
defined(CONFIG_TRACE_IRQFLAGS)
*irqclass)
print_bfs_bug(ret);
new_bit)
connect
hardirq-unsafe
nest_lock,
*hlock;
reliable
chain_key)
(prev_hlock
hlock;
OFF
ON
transition:
(curr->softirqs_enabled)
general
*prev_hlock;
depth-1;
sizeof(*lock));
trylock,
(hlock->instance
(match_held_lock(hlock,
lock))
nested,
ip))
locked;
bytes\n",
handle,
capture
aux_head;
hardware,
rb->nr_pages
(i--;
IRQS_SUSPENDED
mask_irq(desc);
SIGEV_NONE
a)
permission
hrtimer_get_res,
unlock_timer(timr,
SIGEV_NONE)
(!kc)
expired,
ktime_sub(hrtimer_get_expires(timer),
HRTIMER_MODE_ABS
major
crashed
integrity
msg;
msi_domain_ops
"irq
kfree(acct);
rnd
event->owner
>0
Minimum
event_type,
event->cpu);
perf_ctx_lock(cpuctx,
perf_ctx_unlock(cpuctx,
*sub;
list_for_each_entry(sub,
ctx->mutex
those.
ctxn)
(ctx->is_active)
event->tstamp_stopped;
event->attach_state
(event->group_leader
(event->attr.read_format
PERF_FORMAT_TOTAL_TIME_ENABLED)
get_ctx(ctx);
tstamp;
(task_ctx)
installing
leaders
(!event_filter_match(event))
perf_event,
divisor
list_for_each_entry_rcu(event,
hwc->interrupts
(!ctx)
errout;
PERF_ATTACH_TASK)
(event->attr.freq)
ring_buffer_attach(event,
mutex_lock(&ctx->mutex);
&enabled,
&running);
(rb)
requirements
*user
offset.
vma.
TASK_SIZE
stack_size
*header,
dump,
tid;
.pid
perf_mmap_event
.id
hwc->last_period
Which
&per_cpu(swevent_htable,
stuff.
Single
(attr->sample_type
child);
parent_ctx,
Eric
bucket,
*hb,
spin_lock(&hb->lock);
OWNER_DIED
hb->lock
hb2
VERIFY_READ);
key1
hash_bucket
futexes.
nr_wake
-EINTR.
slowpath:
gracefully
per-thread
fills
kthread_create_info
*k)
kthread_stop()
irqsoff_trace;
ks->pass_exception
gdbstub
ftrace_disabled
ftrace_ops_list;
*rec
*)((unsigned
Sample
(from
rec->ip
FTRACE_OPS_FL_INITIALIZED,
global_ops
NOP
ops->func_hash->notrace_hash;
regs.
FTRACE_FL_REGS
records,
pr_info("ftrace
*)rec->ip,
iterator.
(ftrace_graph_active)
t_hash_start(m,
glob,
@ops
@reset
strsep(&buf,
Shut
ftrace_pages
PIDTYPE_PID,
ftrace_graph_entry
IRQ_NONE;
desc->threads_oneshot
(action_ret
thread_mask
w/o
agree
action.
IRQD_IRQ_INPROGRESS);
desc->depth
desc->irqs_unhandled
licenced
x86
*newdev)
CLOCK_EVT_FEAT_C3STOP))
CLOCK_EVT_FEAT_ONESHOT)
raw_spin_lock(&tick_broadcast_lock);
raw_spin_unlock(&tick_broadcast_lock);
(!tick_device_is_functional(dev))
dev->next_event
clockevents_lock
*bc;
CLOCK_EVT_STATE_ONESHOT)
starts.
*range,
nr_range,
GTOD
.lock
ticking
*cp)
architectures.
cpu:
*rq1,
BUG_ON(!irqs_disabled());
originally
Mochel
pidlists
*ss)
enclosing
lockdep_assert_held(&css_set_rwsem);
*cset,
cset,
cset;
(cgroup_on_dfl(cgrp))
child_subsys_mask
KERNFS_DIR)
cft,
(root
use_task_css_set_links
dentry;
(may
(!cgrp)
(tsk->flags
PATH_MAX);
DEFINE_WAIT(wait);
ss,
@cfts:
*cft;
(cft
cfts;
cft->name[0]
cft++)
*ss
*pos,
sibling)
*it)
css->ss;
@from
populate
cgroupstats
&css->cgroup->flags);
confirmed
argv[2]
boundaries
Elf_Ehdr
find_sec(info,
CONFIG_MODVERSIONS
*owner,
align);
list_for_each_entry(use,
'a'
buf[l++]
crc,
versindex,
symname);
(!sym)
bin_attribute
(info->sechdrs[i].sh_type
info->sechdrs,
mod->strtab
(bytes
directory,
section_addr(info,
gdb
modname,
nextval
*type,
top_cpuset
parent_cs(cs);
cs;
meter
second,
cpuset_filetype_t
cpus_updated,
[max:
%lu]\n",
&rem);
seq_line(m,
'-',
action_ret);
(!irq_may_run(desc))
handle_irq_event(desc);
lockindex
BKL
td->opcode
CONFIG_COREDUMP
**buf,
TMPBUFLEN
character.
'-'
converts
*valp;
long)-val;
long)val;
milliseconds
input.
'\n',
action_ret)
enabled.\n");
<linux/personality.h>
nr_threads
*ti)
fail_nomem;
mm's
exe_file
mode))
Thread
PIDTYPE_MAX;
newsp,
parent_tidptr,
unsharing
MSEC_PER_SEC
USEC_PER_SEC
time64_t
year
@nsec:
tv_nsec
(nsec
necessarily
slot.
*rcp)
restrictions
cleanup;
EXIT_ZOMBIE)
EXIT_DEAD;
status)
reaped.
->notask_error
wo->notask_error
(which)
(upid
xtime
*new_base;
&per_cpu(hrtimer_bases,
KTIME_MAX
timer->start_site
timer->state
mode:
@which_clock:
*expires,
clock)
@timeout
Request
console->flags
_name##_attr
->seq[]
%.*s\n",
save_ftrace_enabled
ftrace_enabled;
"*"
(trace_selftest_test_probe1_cnt
save_ftrace_enabled;
optimizing
pr_cont("*callback
ftrace_graph_stop();
trace_test_buffer(&tr->max_buffer,
CONFIG_DEBUG_FS
'wakeup_count'
audit_log_task_context(ab);
(audit_enabled
nlh
*aunet
audit_net_id);
*net
*ab
af.features
old_feature,
new_feature,
old_lock,
res)
sizeof(s));
audit_log_common_recv_msg(&ab,
&ctx,
AUDIT_DISABLED;
*ab)
skb_tailroom(skb);
(!avail)
quote
*ptr++
sessionid
secid
&len))
atomic_set(&lock->count,
d->state_use_accessors
image;
CRC32
required;
kfree(tmp);
release_swap_reader(handle);
Loading
timerqueue
alarm);
base->gettime());
alarmtimer
(approximately)
mod_timer()
base->timer_jiffies;
base->next_timer
cascade
migrate_timer_list(new_base,
*cp);
CAP_FULL_SET;
hibernation.
Platform_finish;
disable_nonboot_cpus();
syscore_suspend();
syscore_resume();
enable_nonboot_cpus();
Close;
suspend_console();
resume_console();
pm_prepare_console();
Resume
noresume
spin_unlock_mutex(&lock->wait_lock,
'$')
dbg_get_reg(i,
dbg_set_reg(i,
(remcom_in_buffer[0]
put_packet(remcom_out_buffer);
remcom_in_buffer[0]
fgraph_cpu_data
\n",
spaces);
iter->private
kprobe_insn_page
pre_handler
optimizer
*ri;
!kprobe_gone(p))
kprobes_all_disarmed
(!dir)
stat_node
*session)
*f)
TK_CLEAR_NTP
tk->xtime_sec
base[1]
seconds;
ktime_add_ns(base,
timekeeping_forward_now(tk);
tk_set_wall_to_mono(tk,
printk_deferred(KERN_WARNING
xtime_interval
adj_1)
tick_error
bpage;
(kill_test)
niceval,
niceval
(who)
gid.
CAP_SETGID))
uid.
CAP_SETUID))
kuid;
old_fsuid;
old_fsgid;
new_utsname
__put_user(0,
*u;
rlimit64
(who
(arg3
kdb_putarea(addr,
kdb_printf
*tp_event
p_event->tp_event;
&ftrace_events,
console_sem
console_drivers
27
/dev/kmsg
syslog_seq;
log_buf_len
(endp
cont
LOG_CONT)
SYSTEM_BOOTING)
syslog_partial
textlen;
*con;
con
console,
console_locked
up_console_sem();
prefix,
(cont.len
dictlen,
lflags
*c;
bootconsoles
dumper->cur_idx
CPUCLOCK_SCHED)
CPUCLOCK_PROF:
CPUCLOCK_VIRT:
&tsk->signal->cputimer;
timer->it.cpu.task;
lock_task_sighand(p,
sample_to_timespec(timer->it_clock,
it->expires
spin_unlock_irq(&timer.it_lock);
proc_do_uts_string,
buf->bytes_consumed
(!chan)
*chan,
failed\n",
n_subbufs)
buf->chan->subbuf_size
@filp:
read_pos,
padding,
(read_subbuf
read_start,
this_len,
this_len
this_len;
interrupted,
Keyboard
freeze_ops
suspend_freeze_state
y)
clocksource_select();
cs->maxadj
0x%llx
lastchar)
(diag
next_avail
size_avail
RINGBUF_TYPE_TIME_STAMP:
RINGBUF_TYPE_DATA:
RINGBUF_TYPE_TIME_EXTEND)
skip_time_extend(event);
lines,
cpu_buffer->reader_page
cpu_buffer->commit_page;
cpu_buffer->head_page;
old_flag,
*tail_page,
rb_set_head_page(cpu_buffer);
list_for_each_entry_safe(bpage,
cache_line_size()),
free_buffer_page(bpage);
&cpu_buffer->entries_bytes);
cpu_buffer->write_stamp
cpu_buffer->read_stamp
next_page,
(tail
rb_advance_reader(cpu_buffer);
RINGBUF_TYPE_PADDING)
ring_buffer_read_prepare
bytes:
tasklet_hrtimer
cpu).tail
*system)
trace_get_fields(call);
filter_type);
item),
SOFT_DISABLED
file->dir,
cval
(value)
DECLARE_FETCH_FUNC(method,
is_kprobe,
*req,
hcpu,
v2
audit_uid_comparator(cred->uid,
AUDIT_UID:
AUDIT_GID:
(!result)
f->val))
AUDIT_WATCH:
AUDIT_DIR:
AUDIT_LOGINUID:
f->type,
f->lsm_rule,
list_for_each_entry_rcu(e,
(!context)
len_left
loginuid
latch
-ETIME
dev->min_delta_ns
&clockevent_devices,
cpu_pm_exit
TIME_OK;
time_maxerror
NTP_PHASE_LIMIT;
wander
(time_status
pps_intcnt
STA_NANO))
time_freq
cpu_clock(i)
scd->tick_gtod
'@')
(FETCH_FUNC_NAME(method,
bitfield_fetch_param
'+');
sizeof(*stats));
stats);
nlmsg_free(rep_skb);
IORESOURCE_BUSY
segments,
image->nr_segments;
mstart,
mend;
image->segment[i].mem;
mend
*image;
kernel_fd,
initrd_fd,
cmdline_len,
destination)
uchunk);
*crash_base)
kexec_buf
temp_end
digest
pi->ehdr->e_shnum;
buf_sz
elapsed_msecs
(preh_val
pr_err("incorrect
krph_val
(krph_val
node))
trace_seq_puts(p,
*)ptr);
SPLIT_NS((long
0LL,
%Ld\n",
Phase
(rnp->exp_tasks
*statusp
CB
(rdp
sysidle
(!tick_nohz_full_enabled())
rdtp->dynticks_idle_nesting
(KDB_DEBUG(AR))
w1;
w2;
w4;
w8;
RUNNING
*)(debug_alloc_pool
*((char
timer_list_iter
-EBADMSG;
header_iter
(newval
`%s'
resource_constraint
res->flags
(p->start
free_resource(res);
region_devres
kdb_cmd_enabled,
kdb_printf("kdb:
KDB_NOTFOUND;
KDB_DEBUG_STATE("kdb_local
sprintf(fmtstr,
reg64;
rname
tm->tm_sec
kdb_bt,
(f->val
f->val;
singlestep.
uprobe_opcode_t
spin_lock(&uprobes_treelock);
spin_unlock(&uprobes_treelock);
uprobe->consumers;
uc
uc);
area->vaddr
slot_addr
*utask
bp_vaddr);
TIF_UPROBE
(sem->count
mutex_unlock(&userns_state_mutex);
projid_t
*extent
skip_spaces(pos);
*bt,
*bt;
xchg(&q->blk_trace,
(likely(!bt))
blk_add_trace_rq(q,
(bt)
blk_io_trace_remap
t_sector(ent),
*pinst;
pinst
per_cpu_ptr(pd->pqueue,
(!pd)
pinst->cpumask.cbcpu);
padata_sysfs_entry
shuffle_task
shuffle_idle_cpu
s16
*mk;
module_version_attribute
autodetect
async_synchronize_full();
considering
(!desc->action
trigger.
startup
irq_shutdown(desc);
Manfred
Spraul
Andi
"rcu.h"
rcu_expedited
rcu_expedite_gp()
Cancel
rcu_unexpedite_gp();
rcu_read_lock().
Decrement
rcu_read_unlock_special()
prove
rcu_synchronize
fixup_activate
tracker.
debugobjects
!CONFIG_DEBUG_OBJECTS_RCU_HEAD
.fixup_activate
CONFIG_RCU_STALL_COMMON
rcu_cpu_stall_suppress
warnings.
till_stall_check
atomic_notifier_chain_register(&panic_notifier_list,
involved
required,
manipulation
guarantees.
preceded
extends
CPU).
preempt_disable()
Author(s):
"gcov.h"
Printing
gcc's
Turn
*prev
Stack
1999-2004
console_loglevel;
kdb_trap_printk++;
(addr)
*)p,
CONFIG_X86
show_stack(p,
kdb_trap_printk--;
alternate
<address-expression>
kdb_do_each_thread(g,
kdb_while_each_thread(g,
kdbgetularg((char
&init_pid_ns);
swsusp.
swsusp
image_size
frames.
swsusp_free()
chains
ca->used_space
objects.
objects,
bm_position
Additionally,
BITS_PER_BYTE)
Radix
long))
chain_alloc(ca,
*dst;
builds
bm->cur.node
bm->cur.node_bit
hook)
PFNs
ext->end
*ext;
memory_bm_position_reset(bm);
zone->start_pfn)
memory_bm_find_bit(bm,
bits)
Registered
(usually
zone_page_state(zone,
NR_FREE_PAGES);
(swsusp_page_is_forbidden(page)
max_zone_pfn
memcpy
usable
src);
BM_END_OF_MAP))
alloc_highmem;
purpose.
restore_pblist
allocate.
(PageHighMem(page))
avail_normal
alloc_normal;
x,
total)
total);
save;
acceptable
proportional
(5)
Preallocate
PAGES_FOR_IO
continued
pages)
err_out:
to_alloc
memory\n");
Kill
disk.
CONFIG_ARCH_HIBERNATION_HEADER
Then,
get_image_page(GFP_ATOMIC,
kaddr
kmap_atomic(page);
Extract
GFP_ATOMIC,
@buffer.
safe_pages_list
snapshot_write_next()
50%
"before
IOW,
CGROUP_FREEZING)
committing
CGROUP_FREEZING))
FROZEN
revert
noop
*child
@state
migration,
.attach
files,
version)
RCU_NUM_LVLS
NUM_RCU_LVL_0
NUM_RCU_LVL_4
NR_CPUS)
excludes
conditions.
requests.
breadth-first
Entries
completed,
QS
cbs
kicked
invoke.
follower
rcu_node.
boost.
stalls.
flavors.
force-quiescent-state
Sequence
DECLARE_PER_CPU(int,
rcu_preempt_has_tasks(struct
2011
API.
public
yes
reboot_type
Unregisters
selects
watchdog.
I'm
PF_NO_SETAFFINITY;
set_cpus_allowed_ptr(current,
Shutdown
migrate_to_reboot_cpu();
arguments.
kernel_restart(NULL);
CONFIG_HIBERNATION
envp,
poweroff
Trigger
@force:
'c':
's':
Mutexes:
tester
complex
rt_mutex_waiter,
tracking:
RT_MUTEX_HAS_WAITERS
RT_MUTEX_MIN_CHAINWALK,
<linux/tracehook.h>
<linux/uprobes.h>
ignored,
(_NSIG_WORDS)
set_tsk_thread_flag(t,
dequeued
jobctl
Group
do_signal_stop()
credentials.
__sigqueue_free(q);
SIG_DFL;
__ARCH_HAS_SA_RESTORER
priv;
Collect
info->si_signo
(sig)
(!signr)
current->jobctl
NOTE!
tcred->suid)
tcred->uid)
schedules
continuing
*signal
(signal->flags
flush_sigqueue_mask(&flush,
Such
t))
info)
ret:
result);
de_thread()
retval,
source.
priv)
applications
Timers
stopped/continued
SIGCHLD;
task_pid_nr_ns(tsk,
tsk->exit_code
becoming
zombie.
-ECHILD.
situation,
schedule().
exit_code,
block,
TRAPPING
regrabbed
trap,
parents
interact
JOBCTL_STOP_PENDING
signr,
task_pid_vnr(current);
scheduled.
resumed
intervening
races.
CLD_STOPPED;
(signr
user-mode
wait(2)
job
delivered.
@which
handle.
threadgroup.
sigmask(SIGSTOP));
*set,
newset;
old_set
(nset)
sizeof(sigset_t)))
(oset)
__BIG_ENDIAN
sizeof(compat_sigset_t)))
raised
__put_user(from->si_pid,
&to->si_pid);
__put_user(from->si_uid,
&to->si_uid);
tick,
&mask,
uthese,
uts,
existence
receiving
{};
flush_sigqueue_mask(&mask,
spin_lock_irq(&p->sighand->siglock);
discarded,
compat_stack_t
NULL),
current->blocked.sig[0];
sigaction
oact,
oact
(!signal_pending(current))
unewset,
Allows
"The
t->pid);
Zanussi
displayed
filter_op
">",
FILT_ERR_TOO_MANY_PREDS,
pred->not;
cmp,
(strncmp(str,
r->pattern,
regex_type
MATCH_FULL;
len--;
buff;
*search;
r->match
fine.
series
*pred;
filter->filter_string
*filter_string)
*)__get_free_page(GFP_TEMPORARY);
event_filter(file);
stack->index
*stack)
FILTER_PRED_INVALID))
folding
kfree(filter);
filter_disable(file);
(file->system
dir)
*field,
filter_pred_fn_t
(field_is_signed)
'\0',
OP_NONE;
WALK_PRED_ABORT;
walk_pred_tree(filter->preds,
logic.
(!dry_run)
append_filter_err(ps,
succeeded
succeeded,
**filterp)
everything,
ps
@filter_str:
Creates
','
re
kfree(str);
1)"
"all
unwanted
<asm/syscall.h>
<linux/pid.h>
@filter:
enforces
BPF_JGE
BPF_JGT
BPF_JSET
evaluates
synced
current->seccomp.mode
-ve
restriction
*fp;
new_len;
GFP_KERNEL|__GFP_NOWARN);
place,
decrements
normally.
(mode)
Explicit
uargs);
call->data;
wrappers
')');
LEN_OR_ZERO
print_fmt
trace_define_field(call,
*sys_data;
trace_get_syscall_nr(current,
(syscall_nr
sys_data
syscall_nr_to_meta(syscall_nr);
(!sys_data)
(WARN_ON_ONCE(num
NR_syscalls))
ftrace_event_class
__refdata
meta
redirected
primitives");
threads");
"Enable
hotplugs
onoff_interval,
(s),
0=disable");
shuffle_interval,
shutdown_secs,
stat_interval,
60,
printk()s");
stutter,
test");
"Start
boost_starttime
shortdelay_us
sometimes
udelay(shortdelay_us);
pieces.
rcu's
buggy
rcu_no_completed,
precede
VERBOSE_TOROUT_STRING("rcu_torture_boost
sp.sched_priority
(jiffies
mutex_unlock(&boost_mutex);
(!torture_must_stop())
rcu_torture_current,
synctype[nsynctypes++]
pr_alert("rcu_torture_writer:
primitives.\n");
updating.
Cycle
(cur_ops->cb_barrier
broken.
(cur_ops->started)
cur_ops->completed();
cur_ops->readunlock(idx);
pipe_count
(pipe_count
dereferences
relying
loaded,
TORTURE_FLAG);
holdoff");
stats_task);
fire.
rcu_torture_print_module_parms(cur_ops,
leads
happening
pr_alert("rcutorture:
ARRAY_SIZE(torture_ops);
type:
num_online_cpus()
(num_online_cpus()
Corporation.
2002-2004
Technologies
MontaVista
"as
is"
kind,
express
implied.
host
us?
kgdb_bkpt
[0
collection
CPUs)
interfere
exception,
flushes
(current->mm)
SW
kgdb_break[i].state
breakpoints.
attach.
(!dbg_kdb_mode)
*ks,
num_online_cpus();
hardirq_count()
driver's
loops_per_jiffy
kgdb_contthread
panic_timeout
panic.
ks->signo
.handler
*)data);
connection
core.
spin_unlock(&kgdb_registration_lock);
Arm
(!hibernation_available())
((filp->f_flags
nonseekable_open(inode,
snapshot_handle));
swsusp_resume_device
create_basic_memory_bitmaps();
data->ready
*offp
freezer_test_done
old_fs;
snapshot_ioctl(file,
bpf_map_type_list
'union
bpf_attr'
*ukey
u64_to_ptr(attr->key);
ufd
attr->map_fd;
(!key)
(copy_from_user(key,
ukey,
map->value_size)
free_key:
*value;
*prog)
bpf_prog_aux
*prog
prog;
license
handed
extensions
addr++)
calculations
\Sum_i
deltas
cacheline
yields
@shift:
shift)
a1
active)
exp);
idle-delta
starts,
5s
+10
effectively
intervals.
*this_rq
(delta)
atomic_long_add(delta,
woke
this_rq->calc_load_update
O(log
natural
multiplied
n_i
n),
course
active);
boundary.
exponential
Flip
column
0},
(load
Round
ql,
qll
ql
gpnum
&rsp->node[0];
ACCESS_ONCE(rsp->gpnum);
gpnum)
debugfs_remove_recursive(rcudir);
per_cpu(idle_threads,
status;
td->status
list_for_each_entry(cur,
gotten
permissible
processing,
(oldstate
Outgoing
normally,
newstate;
newstate
Srikar
Dronamraju
tp;
(offsetof(struct
uprobe_dispatch_data
*con,
u8,
u16,
tu->tp.nr_args;
mutex_lock(&uprobe_lock);
Argument
syntax:
is_delete
pr_info("Probe
specified\n");
pr_info("Event
parg->name
kstrdup(buf,
argument[%d]
*tu
probes_seq_start,
probes_seq_next,
probes_seq_stop,
alloc_pages_node(cpu_to_node(cpu),
esize;
SIZEOF_TRACE_ENTRY(is_ret_probe(tu));
ftrace_file,
entry->vaddr[0]
ucb->buf,
list_for_each_entry_rcu(link,
link->file);
*)iter->ent;
parg->name,
parg->offset,
uprobe_filter_ctx
tu->offset,
perf_trace_buf_submit(entry,
PTR_TO_MAP_VALUE_OR_NULL
ARG_CONST_MAP_PTR,
ARG_PTR_TO_MAP_KEY,
CONFIG_TRACING
rcu_fanout_leaf
rcu_num_nodes
rcu_scheduler_fully_active
people
passed,
Pretend
further.
rdp->nxttail[RCU_DONE_TAIL]
RCU_WAIT_TAIL;
!is_idle_task(current))
entry:
comm:
crit
sects
atomic_inc(&rdtp->dynticks);
0x1);
rdtp->dynticks_nesting;
rdtp->dynticks_nesting
CONFIG_RCU_USER_QS
0x1));
irq_exit()
->dynticks_nmi_nesting
RCU-idle
cpu_online_mask.
non-atomic
notifiers.
nested)
snap;
incorrect
ACCESS_ONCE(rsp->jiffies_stall)
starvation
stacks
rnp->grphi
rnp->grplo;
totqlen
fault.
non-root
leave.
rnp->completed;
(current
assigned.
indexed
awakened.
rcu_accelerate_cbs(rsp,
CPU-local
rdp->gpnum
rdp->qs_pending
flags:
layout
both.
isidle
RCU_GP_FLAG_FQS)
Propagate
nasty
rdp))
detecting
advancing
involves
__releases(rnp->lock)
bad.
*rdp->nxttail[RCU_DONE_TAIL]
ready-to-invoke
rdp->nxtlist;
rdp->nxttail[RCU_NEXT_TAIL]
flags))
flight
RCU_TRACE(unsigned
structures,
acquisition.
Enforce
**tail;
rdp->qlen_lazy,
need_resched(),
LONG_MAX
(rdp->qlen
rdp->n_force_qs_snap
(cpu_has_callbacks_ready_to_invoke(rdp))
rcu_bh_qs();
softirq,
suppressed
section:
head->func
head->next
offline;
correctly.
synchronize_sched
rcu_read_lock_sched()
same,
rcu_lockdep_assert(!lock_is_held(&rcu_bh_lock_map)
wait_rcu_gp(call_rcu_sched);
account.
consumes
sync_sched_expedited_started
trycount
danger
cm);
-EAGAIN)
kfree
Recheck
Has
indication
container_of(rhp,
expression
bitmask.
online/offline
CPU_DYING:
kthread_prio;
fanout
arrays.
MAX_RCU_LVLS;
*lg)
per_cpu_ptr(lg->lock,
borrowed
detectors
controlled
'watchdog_enabled'
boolean,
watchdog_thresh
"1",
divide
10^9
.config
sched_clock_tick();
__touch_watchdog();
__this_cpu_write(soft_watchdog_warn,
smp_processor_id()
watchdog_nmi_disable(cpu);
failures
Reduce
cpu0
per_cpu(watchdog_ev,
.thread_should_run
.thread_fn
.thread_comm
.setup
pr_err("Failed
watchdog_running
fetching
READ
wait.
sector,
page_off,
csd_lock_wait(csd);
structure:
data:
csd_unlock(csd);
coherency
Invoked
(wait)
(ie:
embedded
serialized.
unfortunately
call;
(atomically)
what's
get_option(&str,
(*func)
hot,
lowering
succeeds
reordered
update_fast_ctr().
<linux/memory.h>
coming/going
STATIC_KEY_CHECK_USE();
jump_label_update(key,
static_key_deferred
/***
@mod:
mod->jump_entries;
iter_start
@start
@end
Code
*stop
(entry)
(dhowells@redhat.com)
Licence
Licence,
&init_user_ns,
GFP_KERNEL)
process's
validate_creds(old);
new))
put_cred(old);
cred->security
POISON_FREE
label,
from_kuid_munged(&init_user_ns,
from_kgid_munged(&init_user_ns,
line);
<linux/mman.h>
<linux/highuid.h>
low2highgid(group));
old_uid_t
euid
group;
grouplist+i))
(!gid_valid(kgid))
gidsetsize,
grouplist)
(gidsetsize
(!group_info)
put_group_info(group_info);
wake_flags,
waitqueue.
__wake_up_common(q,
wake_flags
-ERESTARTSYS;
PF_KTHREAD)
kthread_should_stop();
container_of(wait,
&q->wait,
!ret);
(BITS_PER_LONG
*word)
(signal_pending_state(current->state,
Level
IRQS_PENDING)
old_userobj;
trace_flags;
TRACE_FILE_LAT_FMT;
iter.cpu_file
cpu_file);
CLONE_NEWIPC
unshare_flags,
new_fs
Specifying
usecs,
iters;
time_passed;
(usecs
mutex_lock(&udelay_test_lock);
mutex_unlock(&udelay_test_lock);
bp_cpuinfo
bp_busy_slots
Serialize
get_bp_info(cpu,
(iter->cpu
bp,
flexible
pinned,
weight)
old_idx,
(old_idx
(!enable)
unregistration
counter:
max(per_cpu(info->tsk_pinned,
HBP_NUM
perf_overflow_handler_t
context);
wide
PERF_HES_STOPPED;
*res)
@devname:
argument,
separately,
*devname,
dev_id;
devres_add(dev,
&rq->dl;
dl_rq->earliest_dl.curr
cpumask_set_cpu(rq->cpu,
cpumask_clear_cpu(rq->cpu,
next_node
next_node;
set_post_schedule(struct
rq->post_schedule
admission
deactivate_task(rq,
ENQUEUE_REPLENISH);
(try
wall
pi_se->dl_deadline;
rules:
breaking
microseconds
runqueue,
reading.
(!dl_task(p)
(excluding
rq_clock_task(rq)
(unlikely((s64)delta_exec
rq->cpu);
*dl_rq,
shorter
any),
ENQUEUE_WAKEUP)
dequeue_pushable_dl_task(rq,
rq_clock_skip_update(rq,
SD_BALANCE_WAKE)
dealing
reschedule,
(rq->curr->nr_cpus_allowed
selection.
Running
TASK_DEAD
bw
(!task_running(rq,
task_running(rq,
it'll
ready.
put_task_struct(next_task);
next_task,
*src_rq;
src_rq);
any).
check_resched
blindly
rescheduling
.enqueue_task
.yield_task
enhancements
<linux/task_work.h>
(default:
monitor
Options
granularity.
latencies.
exceeds
inc)
Con
NICE_0_LOAD
weight,
32))
shift);
*grp)
enqueued.
cfs_rq's
*pse
cfs_rq;
max_vruntime
min_vruntime;
*last
nr_running;
weight.
slice;
*se);
rq_clock_task(rq_of(cfs_rq));
enqueueing
task?
/**************************************************
ms.
NR_NUMA_HINT_FAULT_STATS
p->numa_group
counters,
score
glueless
nid);
conjunction
compared
Source
huge
dist;
src_capacity
dst_capacity
slope
imbalance,
dst,
crossing
groupimp
moveimp
(cur)
task_weight(cur,
env))
task_cpu(p),
p->numa_preferred_nid
task_weight(p,
env.src_nid,
cases:
for_each_online_node(nid)
benefit
(p->numa_group)
16);
decisions
inverse
(diff
Sum
Decay
p->numa_faults[i];
grp);
grp)
user's
work->next
TICK_NSEC;
mm->mmap;
vma;
vma->vm_next)
confusion
work)
period)
tg_weight
tg->shares;
reasonably
history
scale_freq
measurement
delta_w;
sa->runnable_avg_sum
sa->running_avg_sum
se->avg.utilization_avg_contrib
force_update)
group_cfs_rq(se);
forms
understand
w_i
cfs_rq->curr
subtract_blocked_load_contrib(cfs_rq,
consideration
transitioning
Blocking
'current'
'current'.
TASK_UNINTERRUPTIBLE)
clear_buddies(cfs_rq,
resched_curr(rq_of(cfs_rq));
ideally
&tg->cfs_bandwidth;
cfs_b->idle
cfs_rq->runtime_expires
snapshots
tg_cfs_bandwidth(cfs_rq->tg);
cfs_rq,
src_cpu
load-balance
tg_nop,
sub_nr_running(rq,
add_nr_running(rq,
remaining;
unthrottling
throttling
distributing
refresh
cfs_b->quota
matter.
&fair_sched_class)
update_rq_runnable_avg(rq,
Bias
"nice"
tg->se[cpu];
trivial,
w,
effective_load(tg,
this_eff_load
load_idx);
idlest
sd->groups);
UINT_MAX;
for_each_cpu_and(i,
*sg;
affine
sg->next;
(sg
cap
usage,
SD_LOAD_BALANCE))
new_cpu;
p->pi_lock
'se'
'curr'
SCHED_IDLE))
SCHED_IDLE)
(prev
i.
W_i,0
solve
`-
pass,
steps.
bring
src_nid);
cpus_allowed,
env->flags
env->cpus)
Statistics
Obtain
SD_OVERLAP)
*sgc;
reduced
stabilize
busy.
all;
*sg
packed
capa_now
capa_move
load_above_capacity
wish
bumping
exists.
CPU_IDLE
Max
sd->parent;
env.flags
manipulate
affinity.
*group_imbalance
sd->balance_interval
next))
next_balance
pulled_task
next_balance;
setup.
rq->cpu;
rebalance
raw_spin_lock_irq(&rq->lock);
IRQs.
idle);
reachable
hitting
Priority
properly.
se->depth
adjustment.
.get_rr_interval
Pid
rcu));
(task_active_pid_ns(current)
&init_pid_ns)
SEND_SIG_FORCED,
EXIT_ZOMBIE
sys_wait4()
EXIT_DEAD
care,
drops
reaped,
zombie
Writing
means.
procfs
new_value;
MAX_NAMELEN
snprintf(name,
action->name);
&no_irq_chip)
j));
benchmark
TRACE_INCLUDE_FILE
TRACE_INCLUDE_PATH
anchored
decides
AUDIT_RECORD_CONTEXT
reasons.
anchor
gid,
attr;
fd;
Indicates
*dname,
multi,
sk_buff_head
sock
l,
BUG()
d)
bpf_check()
conditional
analysis
construct
program.
PTR_TO_STACK,
bpf_map_lookup_elem()
CONST_IMM
insn_idx;
verified
stack_size;
log_level
(log_level
BPF_REG_SIZE)
"(u32)
4],
insn->src_reg);
*)(r%d
BPF_OP(insn->code);
elem;
NOT_INIT;
UNKNOWN_VALUE;
(regno
regno);
(bpf_size
off=%d
value_regno
&env->cur_state;
env->cur_state.regs;
src1
src2
indirect
expected_type
map_ptr
misconfigured
regs[BPF_REG_0].type
DST_OP);
insn->imm;
BPF_EXIT)
imm)
(log_level)
caught
input:
13
e,
FALLTHROUGH,
unconditional
slot2=MISC)
equivalent.
sl
states,
insn_idx++;
insn++)
bpf_ld_imm64
map)
rejected
insn++;
item;
item[size];
TRACE_CTX,
FTRACE_ENTRY_DUP
TRACE_WAKE,
TRACE_STACK,
")\n"
"\t=>
TRACE_USER_STACK,
TRACE_BPRINT,
TRACE_BPUTS,
resource_size_t,
TRACE_BRANCH,
kernel_cpustat
for_each_present_cpu(i)
timecounter
undefined
cycle_now,
cycle_delta
2008,
errors.
@cpu.
.arg
cpu_stop_init_done(&done,
multi_stop_state
ack
is_active
*cpumask,
@cpumask:
@cpumask
identical
established
wakeup_source
*wl;
*end
scnprintf(str,
*wl)
NSEC_PER_MSEC
<linux/context_tracking.h>
hardirq/softirq
confuse
@cputime_scaled:
(task_nice(p)
p->stime
PF_VCPU)
index);
Lets
cast
ticks;
ticks);
CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
(!in_interrupt())
(stime
precision
rtime,
divide.
sched_clock_cpu(cpu);
Daniel
PAGE_KERNEL);
fp;
BPF_MOV
[BPF_ST
[BPF_LDX
OP
(unlikely(SRC
DST;
u16)
u32)
(((s64)
DST)
((s64)
imm32))
(such
RT-Mutexes:
*w)
*w,
tree->root
(!chunk)
&chunk->mark;
hash)
~(1U<<31);
fsnotify_put_mark(entry);
spin_unlock(&old_entry->lock);
*rule)
audit_log_key(ab,
rule);
list_add(&cursor,
&tree_list);
list_del(&cursor);
node->index
victim
seed;
Err;
mnt
auditctl
<linux/jhash.h>
elem_size;
free_htab;
GFP_USER
key_size;
key_size
map->key_size;
lookup_elem_raw(head,
(l)
map->refcnt
fmt[i]
<asm/local.h>
"[%s]
TIMESTAMP
"Warning:
File
-------
--------
*f;
%8lu
*p2)
showing
last.
__trace_printk_fmt
production
*(unsigned
rwsem_waiter_type
woken,
reader.
woken;
reader;
waiter->task
granted
waiter.type
TASK_RUNNING);
cmpxchg()
READ_ONCE(sem->count);
optimistic
(owner
owner))
wait_lock
list_del(&waiter.list);
(!list_empty(&sem->wait_list))
PF_KTHREAD))
Hmm,
&args);
pr_err("symbol
kmod
@patch:
(patch->state
KLP_DISABLED)
Performs
.sysfs_ops
kobjects
turning
GCOV_H
compatible.
time:
KERN_INFO
status);
pm_suspend(state);
PM_SUSPEND_FREEZE)
KERN_WARNING
PM_SUSPEND_MAX;
introduced
run-time.
@next:
@info
%NULL.
gcov_info_head;
info->next
gcov_info_head
ct_idx++)
gcc:
numbers,
(iter->pos
iter->size)
kit->it_clock;
lifetime
*path;
*watch)
krule->watch
new->parent
&audit_inode_hash[h];
bpf_array,
TRACE_BLK,
__field_desc
__field_desc(type,
__array
__array(type,
__array_desc
__array_desc(type,
__dynamic_array
__dynamic_array(type,
FTRACE_ENTRY_REG
consists
etc.)
continue.
update_max_tr
TRACE_ARRAY_FL_GLOBAL
format:
item.
@init:
describe
steps
feature,
*filp);
__tracer_data
Standard
formatting
ftrace_filter_param
defined,
parsed
*parser)
*parser,
*type;
15)
*name);
function).
per-trigger
hit,
traceon
names.
record;
logs
*tracer,
mainly
*ag
ag;
out_fail:
suggestions
Scheduler
interactivity
Rostedt,
period);
measured
1s
-rt
1000000;
sysctl_sched_rt_runtime
time);
CPU_DOWN_PREPARE_FROZEN:
trace_sched_wake_idle_without_ipi(cpu);
re-evaluate
assembly
down:
irq_delta
delta))
confused
&rt_sched_class;
p->rt_priority;
(oldprio
oldprio);
(task_on_rq_queued(p))
harm
[
sched_ttwu_pending();
simpler
CONFIG_PREEMPT_NOTIFIERS
CLONE_VM)
t.data
anymore
"sched
new_bw
puts
@notifier:
preempt_notifier
*notifier)
notifier.
past.
__releases(rq->lock)
thread's
statistics:
etc...
(preempt_count()
(unlikely(p
WARNING:
rq->clock_skip_update
unlocks
__schedule();
(need_resched());
preempt_schedule_common();
irq.
Catch
CONFIG_RT_MUTEXES
inheritance
base->lock
-dl
task_rlimit(p,
moment.
MIN_NICE,
truncate
oldpolicy
SCHED_NORMAL
SCHED_NORMAL,
set/change
*span
check)
check);
&uattr->size);
abi
param)
cpus_allowed);
cpumask_size())
user_mask_ptr)
((len
retlen;
&t,
PC
set_cpus_allowed_ptr()
*dl_b;
dl_b
raw_spin_lock_irqsave(&dl_b->lock,
raw_spin_unlock_irqrestore(&dl_b->lock,
(locked)
(mm
have.
entry->data
terminator
table;
buf[32];
SD_SHARE_PKG_RESOURCES
old_rd
Attach
Transfer
s_data
s_alloc
*sg)
sd->private;
*sibling;
*per_cpu_ptr(sdd->sgc,
using.
sched_domains_numa_levels;
for_each_sd_topology(tl)
(distance
intermediary
next_distance
k++)
sched_domains_numa_levels
child->parent
pr_err("BUG:
*per_cpu_ptr(d.sd,
doms_cur
ndoms;
ownership
partition_sched_domains()
mappings.
partition_sched_domains().
alloc_size
**)ptr;
CONFIG_IA64
task_group,
tg->rt_bandwidth.rt_runtime;
rt_period,
rt_period
global_rt_runtime();
css_tg(css);
*cftype,
(quota
quota);
cfs_schedulable_data
.exit
<linux/sched/deadline.h>
wakeup_prio
TRACE_DISPLAY_GRAPH)
data->disabled
'set'
(graph)
function_enabled
unregister_ftrace_graph();
is_graph());
graph);
__trace_graph_entry(tr,
__trace_graph_return(tr,
GRAPH_TRACER_FLAGS);
trace_default_header(s);
(tracing_thresh)
T1
wakeup_rt
wakeup_dl
tracer\n");
TRACE_ITER_OVERWRITE,
TRACE_ITER_LATENCY_FMT,
ftrace_reset_array_ops(tr);
<linux/futex.h>
Bit
head))
compat_size_t
PTRACE_MODE_READ))
err_unlock:
val2
tp,
rt_b->rt_runtime
raw_spin_lock(&rt_b->rt_runtime_lock);
raw_spin_unlock(&rt_b->rt_runtime_lock);
array->bitmap);
delimiter
rt_rq;
(!rt_se)
*rt_se;
cpu_online_mask;
inf
want;
indefinitely
ns.
rt_rq_of_se(rt_se);
cpupri_set(&rq->rd->cpupri,
queue);
&p->rt;
update_curr_rt(rq);
dequeue_pushable_task(rq,
logically
beginning.
lower,
"timekeeping.h"
tick_next_period;
!(dev->features
CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
Oneshot
versa.
kdb_breakpoints
bp->bp_type
**argv,
nextarg)
bp->bp_delay
breakpoint,
bp->bp_addr);
kdb,
printk().
*bp
KDB_MAXBPT
template
(bp->bp_free)
kdb_bfd_vma_fmt0
lowbp
highbp
KDB_CMD_SS
KDB_ENABLE_FLOW_CTRL
event_file_data(m->private);
'#');
mutex_lock(&trigger_cmd_mutex);
&trigger_commands,
mutex_unlock(&trigger_cmd_mutex);
\t");
__init,
(strcmp(cmd->name,
p->name)
@tr:
list_del_rcu(&data->list);
update_cond_flag(file);
params
trigger_ops,
strsep(&trigger,
(!tracing_is_on())
*)data->count,
event_trigger_callback,
unregister_trigger,
WARN_ON(ret
trace_dump_stack(STACK_SKIP);
ENABLE_EVENT_STR
clear_bit(FTRACE_EVENT_FL_SOFT_DISABLED_BIT,
set_bit(FTRACE_EVENT_FL_SOFT_DISABLED_BIT,
DISABLE_EVENT_STR,
event_enable_trigger_print,
event_enable_trigger_free,
out_disable;
trace->nr_entries;
generated;
type_info
support,
prof_shift;
(get_option(&str,
&par))
prof_shift
pr_info("kernel
(shift:
prof_shift);
prof_len
prof_buffer
blocking_notifier_chain_unregister(
per_cpu(cpu_profile_flip,
alloc_pages_exact_node(node,
__GFP_ZERO,
cpu)[0]
(per_cpu(cpu_profile_hits,
cpu_notifier_register_done();
write_seqlock(&jiffies_lock);
tick_next_period
do_timer
trace_tick_stop(0,
(!sched_clock_stable())
tick_nohz_full_running
(since
boot)
.tv64
__this_cpu_read(tick_cpu_device.evtdev);
next_jiffies
do_timer()
tick_do_timer_cpu)
TICK_DO_TIMER_NONE;
(ts->nohz_mode
ktime_add(next,
That's
rearm
*offs_real,
*offs_boot,
*)1)
disassociated
WORKER_PREP
Disabling
X:
A:
PR:
idle_list
rebinding
IDs
Destruction
work_color;
flush_color;
wq_flusher
flushers
rescue
wqs
demand
(({
}))
work_data_bits(work)))
pwq,
arbitrarily
atomic_long_read(&work->data);
WORK_STRUCT_PWQ)
pool_id
managing
WORKER_NOT_RUNNING))
recycled
reused
somebody
worker->current_func
move_linked_works(work,
pwq->flush_color
claim
get_work_pool(work);
work->data
wake_up_worker(pool);
current_wq_worker();
unbound_pwq_by_node(wq,
there,
WQ_UNBOUND)
wq->name,
@delay
unbind
worker->task
pwq->wq;
Pin
&pool->worklist,
manager_arb
requirement
UNBOUND
stage.
losing
Workqueue
finishes
*target,
commands,
wait;
wq->work_color);
full,
transfer
overflow.
arm
flush_work()
often.
@attrs:
attrs;
free_workqueue_attrs(attrs);
NUMA_NO_NODE;
pool->flags
destroyed.
free.
*cpumask)
attrs->cpumask);
dfl_pwq
pwq_tbl[node]
Workqueues
printf-style
garbages
%pf",
log_lvl,
NUMA_NO_NODE)
sysrq
unbinding
CONFIG_FREEZER
Grabs
monotonically
container_of(dev,
attrs->nice
wq->wq_dev
uevent
KOBJ_ADD);
**nl,
notifier_call_chain
Records
((ret
atomic_notifier_head
turn.
and'ed
%NOTIFY_STOP_MASK
notifier_call_chain(&nh->head,
boot-up,
(unlikely(system_state
SYSTEM_BOOTING))
raw_notifier_head
<asm/setup.h>
500
%8d
THREAD_SIZE
t_next(struct
*t_start(struct
SEQ_START_TOKEN;
the\n"
"#\n");
Chip
*ct->mask_cache
__iomem
*gc,
gc->irq_base
sz
numchips
Calc
hw_irq
@of_node:
device-tree
@host_data:
*host_data)
host_data);
domain);
IRQ_NOREQUEST);
synchronize_irq(irq);
domain->linear_revmap[hwirq]
mutex_lock(&revmap_trees_mutex);
mutex_unlock(&revmap_trees_mutex);
mapping,
irq_free_desc(virq);
irq_desc_get_chip(desc);
data->parent_data)
*domain
*irq_data;
irq_data;
@virq
(irq_data
&no_irq_chip;
@irq_data:
race,
unlink
Fast
nr_probes
nr_del
ERR_PTR(-ENOENT);
tracepoint.
Removed
*tp_mod;
mutex_lock(&tracepoint_module_list_mutex);
tp_mod);
mutex_unlock(&tracepoint_module_list_mutex);
MODULE_STATE_GOING,
taint
stride;
(base
SMP:
Kprobe
trace_kprobe,
*sc)
len++;
&probe_list,
event_call
back,
offs
__get_data_size(&tk->tp,
tk->tp.size
long)tk->rp.kp.addr;
store_trace_args(sizeof(*entry),
&tk->tp,
*)&entry[1],
(",
enable_trace_kprobe(tk,
disable_trace_kprobe(tk,
(tk->tp.flags
(file->event_call
lockless.
KPROBE_EVENT_SYSTEM);
(WARN_ON_ONCE(tk
find_trace_probe_file(tk,
top_trace_array());
(WARN_ON_ONCE(file
deleting
pr_cont("OK\n");
sequencer
Writes
mem,
gcov_persist
&val))
(node->num_loaded
(node->unloaded_info)
node->num_loaded;
mutex_lock(&node_lock);
mutex_unlock(&node_lock);
*result;
compensate
(info)
replaces
Two
Latency
<arjan@linux.intel.com>
twice,
identified
LT_BACKTRACEDEPTH;
Negative
(!bt)
*txc,
compat_timex
compat_timeval
tz)
(tv)
(tz)
&ts,
rmt;
restart->nanosleep.rmtp
(rmtp
rmtp))
rmtp)
HRTIMER_MODE_REL
conversion.
force_successful_syscall_return();
rlim,
sizeof(*rlim))
COMPAT_RLIM_INFINITY)
do_prlimit(current,
COMPAT_RLIM_INFINITY;
compat_rusage
sys_wait4(pid,
put_user(status,
timer_t
*restart;
bitmap_size
compat_time_t
status,
ring-buffer
results.
tracing_disabled
ftrace_dump_on_oops
MAX_TRACER_SIZE);
ring_buffer_expanded
do_div(nsec,
Shows
trace_types_lock
memcpy(&entry->buf,
bputs_entry
str;
ring_buffer_resize(tr->max_buffer.buffer,
tracing_reset_online_cpus(&tr->max_buffer);
cnt--;
nop
iter->cpu_file,
(ring_buffer_expanded)
trace_buf_size,
cmdline,
NO_CMDLINE_MAP)
arch_spin_lock(&trace_cmdline_lock);
FTRACE_STACK_ENTRIES;
trace.nr_entries
userstack_entry
percpu_buffer
trace_buffer_struct);
DEBUG
bprint_entry
(buf_iter)
*lost_events)
iter->ent_size
*ent,
iter->cpu_file;
next_ts
(iter->idx
iter->idx
iter->trace
(!iter->snapshot)
iter->pos)
tracing_iter_reset(iter,
*total
&iter->seq);
ftrace_find_event(entry->type);
(iter->iter_flags
iter->ts);
TRACE_TYPE_HANDLED)
TRACE_ITER_CONTEXT_INFO))
'2'
__seq_open_private(file,
sizeof(*iter));
iter->tr
iter->cpu_file
Output
TRACE_FILE_TIME_IN_NS;
opening
(!t)
neg)
example:
hit\n"
count:\n"
(*ptr
(cpu_id
per_cpu_ptr(size_buf->data,
sure,
(!ring_buffer_expanded)
&nop_trace;
pipe
*poll_table)
POLLRDNORM;
poll_table);
cat
mutex_unlock(&iter->mutex);
trace_access_lock(iter->cpu_file);
iter->seq.seq.len
spd
KB
file->private_data
.splice_read
&info->iter;
(!count)
kfree(ref);
ts:
s->buffer,
trace_seq_used(s));
sub
d_tracer);
RB_FL_OVERWRITE
buf->data
directories
200
1024,
paranoid
speed
unlock(wait_lock);
lock(wait_lock);
__rt_mutex_adjust_prio(task);
chwalk);
step.
rt_mutex_top_waiter(lock))
@waiter:
pre-initialized
&waiter,
&waiter);
race.
(*slowfn)(struct
rt_mutex_slowlock);
*timeout)
Clock
mult)
NMI.
rd.epoch_ns
'M';
suspended.
trace_seq_putmem(s,
validator
I.e.
<linux/stringify.h>
lock_time
stats;
*stats)
lookup:
VERBOSE
HARDIRQ_VERBOSE
SOFTIRQ_VERBOSE
RECLAIM_VERBOSE
nr_stack_trace_entries;
lockdep_subclass_key
str[KSYM_NAME_LEN];
(class->subclass)
usage);
class_idx
init_utsname()->release,
(int)strcspn(init_utsname()->version,
"),
lock_entry)
acts
hash_head,
hash_entry)
(!graph_lock())
hash:
class->key,
MAX_LOCKDEP_ENTRIES
*cq)
(list_empty(head))
checking,
unsafe_class
safe_class
scenario:\n\n");
***
DEADLOCK
***\n\n");
detected,
printk("\nbut
lock:\n");
printk("\nthe
this.parent
this.class
irq-safe
irq-unsafe
printk("%*s
at:\n",
ENABLED
new_bit
backwards-subgraph
forwards-subgraph
non-trylock
long)chain_key,
chain_key;
building
inconsistent
set:
new_bit,
printk("but
(%u):
hardirqs
prevented
++curr->irq_events;
(unlikely(!debug_locks
current->lockdep_recursion))
weren't
Lockdep
(hlock->references)
curr->curr_chain_key
lock)
(DEBUG_LOCKS_WARN_ON(!depth))
prev_hlock->irq_context
found_it;
found_it:
APIs
(nested)
iterators
usage.
kernel-base/COPYING
separates
perf_output_wakeup(handle);
lost;
handle->event
hardware.
local_read(&rb->aux_head);
aux_tail
local_read(&rb->aux_wakeup)
event->pending_disable
rb_free_aux(rb);
__GFP_NOWARN
(event->cpu
overwrite;
rb->aux_pgoff
pgoff)
kfree(rb);
Novell
allocator.
port
MAX_DMA_CHANNELS
TIMER_ABSTIME
routine,
clock_realtime
common_nsleep,
hrtimer_nanosleep_restart,
common_timer_create,
common_timer_set,
common_timer_get,
common_timer_del,
(timr->it.real.interval.tv64
timr->it_overrun
expires.
k_itimer,
want,
lock_timer(timer_id,
(WARN_ON_ONCE(!kc
flag);
TIMER_RETRY)
unlock_timer(timer,
reboot,
TAINT_PROPRIETARY_MODULE,
SMP.
'D'
slowpath_args
taint,
...\n");
Message
(disable
msi_alloc_info_t
desc;
Accounting
lack
AV.
Fixed
root.
fs_pin
acct
filp_close(file,
encode
exponent.
mantissa.
rnd;
exp++;
50
u;
ktime_get_ns();
group_dead)
vma->vm_start;
pacct->ac_flag
<linux/sysfs.h>
remote_function_call
1024);
max_samples_per_tick
allowed_ns
local_samples_len
perf_cgroup_info
perf_cgroup_from_task(task);
cpu_ctx_sched_out(cpuctx,
EVENT_ALL);
(cgrp1
&event->sibling_list,
(pmu->task_ctx_nr
perf_event::ctx
Those
children,
perf_event_ctx_lock()
mmap_sem
lockdep_assert_held(&ctx->lock);
(ctx
++ctx->pin_count;
total_time_running
update_event_times(event);
event->attr.sample_type;
PERF_SAMPLE_TID)
PERF_SAMPLE_TIME)
PERF_SAMPLE_STREAM_ID)
PERF_SAMPLE_CPU)
*group_leader
update_group_times(event);
maintained,
event->tstamp_running
event->tstamp_stopped
event->oncpu
event_sched_out(event,
remove_event
event->ctx->task
top-level
lockdep_assert_held(&ctx->mutex);
(!task_function_call(task,
retry.
event's
update_cgrp_time_from_event(event);
simulate
*leader
(leader
perf_pmu_disable(ctx->pmu);
(event_type
perf_pmu_enable(ctx->pmu);
ctx2
event_entry);
sec_fls
@count
hwc->sample_period);
PERF_EF_UPDATE);
event->pmu->start(event,
per_cpu_ptr(pmu->pmu_cpu_context,
(unlikely(err))
*rb);
cpu-wide
event->pmu;
PERF_SAMPLE_CALLCHAIN)
free_event(event);
*enabled
*running
read_format,
primary_event_id(event);
read_format
event->attr.read_format;
perf_event_for_each_child(event,
(!is_sampling_event(event))
perf_event_mmap_page
vma->vm_mm->pinned_vm
extra;
data->id);
*sample)
PERF_SAMPLE_STACK_USER)
(events
tells
.tid
perf_comm_event
min;
prot
kmalloc(PATH_MAX,
file_inode(vma->vm_file);
leaking
vma->vm_start,
hwc->sample_period;
(event->attr.config
event_id)
event_id);
*hlist;
(!hlist)
this_cpu_ptr(&swevent_htable);
(!head)
mutex_lock(&swhash->hlist_mutex);
mutex_unlock(&swhash->hlist_mutex);
PERF_TYPE_TRACEPOINT)
mutex_lock(&pmus_lock);
mutex_unlock(&pmus_lock);
perf_pmu_nop_void;
err_ns;
HW
(move_group)
perf_install_in_context(ctx,
perf_unpin_context(ctx);
placing
dst_cpu);
child's
child_ctx
child_ctx);
Reserved
<linux/hugetlb.h>
*futex;
*pi_state;
bitset;
futex.
Private
page_head
happy
*pi_state
list_del_init(&pi_state->list);
ps);
newval,
uaddr))
pi_state.
unqueue
Afterwards,
__unqueue_futex(q);
(cmpxchg_futex_value_locked(&curval,
(curval
uval)
hb2)
hb1
get_futex_key(uaddr,
plist_for_each_entry_safe(this,
this->rt_waiter)
wake_futex(this);
op_ret;
VERIFY_WRITE);
retry_private:
FLAGS_SHARED))
retry_private;
&key2))
curval;
hb2,
futex_requeue()
>=0
hb_waiters_dec(hb2);
free_pi_state(pi_state);
queue_me()
fixup_owner()
Too
(timeout)
*abs_time,
uaddr.
to);
queue_unlock(hb);
put_futex_key(&q.key);
-ERESTARTNOINTR;
Support
robust
robust_list_head
now:
functionality.
futex_hashsize
*sem);
(likely(sem->count
sem->count--;
acquired,
'state'
__down_common(sem,
(*threadfn)(void
kthread_create()
SIGCHLD);
complete()
@k
TASK_UNINTERRUPTIBLE
spin_unlock_irq(&worker->lock);
kthread_flush_work
__trace_function(tr,
(preempt_trace()
CALLER_ADDR1);
a0,
a1)
a1);
trusted
plen;
plen
(IS_ERR(key))
kdb_current_regs
ftrace_stub,
function_trace_op
ftrace_pid
ftrace_pid_function
ftrace_ops_list_func
ops->next)
ftrace_ops_list_func;
**list,
ops)
FTRACE_OPS_FL_ENABLED))
CONFIG_DYNAMIC_FTRACE_WITH_REGS
FTRACE_OPS_FL_CONTROL)
(ftrace_enabled)
update_ftrace_function();
stddev;
stat->pages
*stat,
(rec)
!!val;
EMPTY_HASH,
ENTRY_SIZE
free_hash_entry(hash,
free_ftrace_hash(hash);
(!hash)
(hash_contains_ip(ip,
long)start,
long)end);
in_hash
rec->ip))
trampoline.
REGS
TRAMP
filter_hash)
&global_ops.local_hash)
old_hash
sensitive
FTRACE_WARN_ON_ONCE(1);
FTRACE_FL_REGS)
FTRACE_FL_TRAMP)
(flag
trampoline,
long)FTRACE_ADDR;
iter->pg->next;
(command
ops->old_hash.filter_hash
ops->old_hash.notrace_hash
ftrace_run_update_code(command);
synchronization,
FTRACE_OPS_FL_ENABLED)
free_pages;
hnd
"####
####\n");
type))
old_hash);
*orig_hash;
orig_hash,
free_ftrace_hash_rcu(old_hash);
orig_hash
@len
FTRACE_FILTER_SIZE);
seq_release_private,
*fgd
tr->ops->func
ftrace_swapper_pid)
ftrace_graph_entry_stub;
t->ret_stack
curr_ret_stack
sleep_time
hrtimer_interrupt()
NSEC_PER_JIFFY
JIFFIES_SHIFT
shift_hz
*action
action->next;
irq_data_get_irq_chip(data);
Notification
Enables
unmask_irq(desc);
Useful
action->dev_id);
IRQTF_RUNTHREAD
action_ret
out_mask;
play
%d)
%08x
__setup_irq(irq,
receives
!irq_settings_is_per_cpu_devid(desc))
resources,
forwarded
stage
emulate
2006-2007,
kernel-base/COPYING.
(bc)
(newdev->features
(dev->features
dev->name);
clockevents_shutdown(bc);
tick_broadcast_force_mask);
now.tv64)
tick_broadcast_pending_mask);
mechanism,
Reprogram
TICKDEV_MODE_ONESHOT;
az,
az;
scalable
coherent
__acquires(torture_rwlock)
__releases(torture_rwlock)
cxt.nrealreaders_stress;
(max
(cxt.cur_ops->readlock)
lock_torture_print_module_parms(cxt.cur_ops,
King
spin_unlock(desc->lock);
cp->elements[cp->size
cp->free_cpus);
(set
!(desc->status_use_accessors
<linux/sched/sysctl.h>
regions
features
resides
task_rq_lock,
*busiest)
*l1,
(l1
*rq2)
Nokia
Rework
cgroup_mutex.
appended
@cgrp's
CGROUP_SUBSYS_COUNT
for_each_e_css(css,
for_each_root(root)
traversing
cgrp_link)
link->cgrp;
cgrp_cset_link,
@cgrp.
cgrp,
kernfs_root
&root->cgrp;
init_css_set.
new_ss_mask
*kn)
cgroup_addrm_files(cgrp,
spin_lock(&release_agent_path_lock);
spin_unlock(&release_agent_path_lock);
'all'
subsystems.
&opts);
*g;
cgroup_post_fork()
&init_css_set);
base_files
msleep(10);
taskset
cg_list);
->can_attach()
*tcred;
tcred
cgroup_kn_unlock(of->kn);
seq_css(seq)->cgroup;
task->group_leader;
out_finish:
'+')
offlining
cgrp->subtree_control
convenience
"%llu\n",
.seq_start
.seq_next
.seq_stop
cft->flags
((cft->flags
kf_ops
zero-length
cgroup_assert_mutex_or_rcu_locked();
(!pos)
->next
last;
@it:
it->cset_pos
CFTYPE_ONLY_ON_ROOT,
css_tryget_online()
offlined
css->flags
css->id
@child:
transfers
Unlink
agent
overflow:
CONFIG_MODULE_SIG
module_mutex.
SHF_ALLOC)
sh_addr
NOT_GPL_ONLY,
GPL_ONLY,
find_symbol_arg
owner;
'b'
blocking_notifier_call_chain(&module_notify_list,
&owner,
'E';
modprobe
symname)
version,
module_mutex
module_sect_attr
*kparam,
mod->core_text_size,
set_memory_rw);
mod->init_text_size,
Arch-specific
SHF_ALLOC
SHF_ALLOC,
(m
s->sh_entsize
pr_debug("\t%s\n",
debug_align(mod->core_size);
debug_align(mod->init_size);
*string,
(sechdrs[sym->st_shndx].sh_flags
sechdrs
simple,
*)info->hdr
elf
info->hdr
(stat.size
long)SHF_ALLOC;
ERR_PTR(-ENOEXEC);
async_synchronize_full()
uargs,
resolution.
(within_module(addr,
(modname)
*modname,
buf[bx++]
users.
m_next,
m_stop,
old_mems_allowed
traverse
callback_lock.
briefly
mempolicy
validate_change()
proposed
cpuset_for_each_descendant_pre(cp,
pos_css
css_rightmost_descendant(pos_css);
domains)
csa
cp;
rebuild_sched_domains_locked();
temp
new_cpus);
reserves
task_lock(tsk);
task_unlock(tsk);
nodemasks
*new_mems;
digits
*fmp)
fmp->cnt
get_seconds();
&cpuset_attach_nodemask_to);
cft->private;
cpuset_common_seq_show,
cpumask_copy(cs->effective_cpus,
cs->effective_mems
refusing
cpumask_copy(top_cpuset.cpus_allowed,
new_mems;
__GFP_HARDWALL
allowed;
z
"1"
%11lu
chains:
*dl
seq_time(m,
seq_lock_time(m,
%14lu
(i)
'0')
irq_state_clr_masked(desc);
desc->irq_data.chip->irq_mask(&desc->irq_data);
irq_state_set_masked(desc);
irqd_irq_disabled(&desc->irq_data)))
mask_ack_irq(desc);
(!(chip->flags
falling
Retrigger
td->opdata;
dat;
(tid
(td->opcode
min/max
Bill
sched_proc_update_handler,
&minolduid,
&maxolduid,
&two,
formatted
*kbuf;
__get_free_page(GFP_TEMPORARY);
(!kbuf)
lval;
proc_skip_spaces(&kbuf);
proc_get_long(&kbuf,
&neg,
proc_put_long(&buffer,
!err)
(USER_HZ
USER_HZ)
polled
ok;
<linux/fdtable.h>
(!tmp)
address_space
mm->owner
Main
mmput()
tsk->nivcsw;
*sig;
stack_start,
stepping
TIF_SYSCALL_TRACE);
p->exit_signal
PIDTYPE_PGID);
child_tidptr,
SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK,
unshare_flags
UTC
!(HZ
MSEC_PER_SEC);
Converts
minutes
"+rm"(nsec));
ts.tv_sec
MAX_JIFFY_OFFSET;
TICK_NSEC,
usec
div_u64(x
512)
100,
'1':
group_dead);
SIGHUP
(p->exit_state
(c->mm
reaper;
(thread)
thread;
p->exit_state
dead);
dumps
wo->wo_rusage
getrusage(p,
RUSAGE_BOTH,
wo->wo_rusage)
&infop->si_signo);
&infop->si_errno);
&infop->si_code);
&infop->si_pid);
&infop->si_uid);
&infop->si_status);
WNOWAIT))
((status
clockid,
.clockid
.get_time
.resolution
KTIME_LOW_RES,
tai
timer->base;
ktime
base->offset);
rbtree.
timer->start_pid
debug_deactivate(timer);
unlock_hrtimer_base(timer,
raw_spin_unlock(&cpu_base->lock);
raw_spin_lock(&cpu_base->lock);
container_of(node,
(rmtp)
*rqtp,
@delta:
(kstrtoul(buf,
.attrs
empty?
sp->running
rank
ACCESS_ONCE(per_cpu_ptr(sp->per_cpu_ref,
queue_delayed_work(system_power_efficient_wq,
spin_unlock_irq(&sp->queue_lock);
synchronize_srcu().
->batch_check1
t->comm,
box.
__stringify(DYN_FTRACE_TEST_NAME);
print_counts();
(trace_selftest_test_probe2_cnt
(trace_selftest_test_probe3_cnt
save_max
udelay(100);
trace->stop(tr);
save_max;
go;
!strncmp(buf,
CONFIG_PM_AUTOSLEEP
CONFIG_PM_WAKELOCKS
g,
setuid()
hashent);
new->uid
audit_backlog_limit
audit_backlog_wait_time
skbs
audit_reply
(audit_pid)
allow_changes
audit_default
kaudit
(skb)
net_generic(net,
skb;
*reply
*)arg;
reply
uid);
audit_send_reply(skb,
audit_log_n_untrustedstring(ab,
!err);
AUDIT_DISABLED)
(audit_pid
ab->skb;
avail;
audit_expand(ab,
audit_get_sessionid(current);
sessionid);
audit_log_d_path(ab,
obj=%s",
audit_panic
memset(waiter,
sizeof(*waiter));
*)lock,
(swap_offset
Swap
(!offset)
handle->k
LZO_HEADER,
*snapshot,
(%u
pages)...\n",
&bio);
(!(nr_pages
m))
nr_pages++;
done.\n");
crc_data
d->ret
thr,
go));
data[thr].thr
crc->thr
crc->run_threads
FALSE
&snapshot,
handle->maps
eof
Transition
@base:
alarmtimer_restart
(!alarmtimer_get_rtcdev())
Provides
TIMER_ABSTIME)
less,
power.
@j
round_jiffies_common(j,
j0
TVR_MASK;
per_cpu(tvec_bases,
timer_set_base(timer,
expires_limit
(slot
(lock)->break_lock
do_raw_spin_trylock,
1999
sub_info,
land
down_read(&umhelper_sem);
(wait
(table->data
in_suspend
restore.
centisecs
msg,
Enable_cpus;
Enable_cpus:
Thaw;
Resume_devices;
printed.
"%lu",
lock->ctx
ww_mutex,
spin_lock_mutex(&lock->wait_lock,
__mutex_lock_common(&lock->base,
ch
(dbg_io_ops->flush)
dbg_io_ops->flush();
wcount;
DBG_MAX_REG_NUM;
'X'
*(ptr++)
kgdb_usethread
(remcom_in_buffer[1]
int_to_threadref(thref,
thref);
',';
'C':
ks->threadid
ks->threadid);
&ks->threadid);
'1'
'0');
'D';
SPINLOCK_OWNER_INIT;
CPU#%d,
msg)
print_once
RWLOCK_BUG_ON(lock->magic
RWLOCK_MAGIC,
*cpu_data;
TRACE_GRAPH_PRINT_IRQS)
trace->depth
print_graph_proc(s,
NSEC_PER_SEC);
Cpu
ent);
print_graph_duration(0,
depth.
TRACE_GRAPH_INDENT;
*depth_irq
pr_warning("Warning:
kprobe_blacklist_entry
inserts
true(!0)
reset_kprobe_instance();
arch_remove_kprobe(p);
(kprobe_aggrprobe(p))
(!list_empty(&op->list))
cpu-hotplug
disarming
quiesence
(p->break_handler
unoptimize
ap->flags
kprobes_allow_optimization
(!kprobe_disabled(p))
"Kprobes
WARN(ret
kprobe-ftrace
p->addr,
hlist_del(&ri->hlist);
hlist_add_head(&ri->hlist,
KPROBE_HASH_BITS);
kretprobe_table_lock_ptr(hash);
hlist_for_each_entry_safe(ri,
orig_p
(!kprobes_all_disarmed)
aggrprobe
kprobe.
**rps,
namebuf[KSYM_NAME_LEN];
buf[0]
&value,
__reset_stat_session(session);
mutex_unlock(&session->stat_mutex);
mutex_unlock(&all_stat_sessions_mutex);
timekeeping:
timekeeping_last_warning
tk->tkr_mono.clock;
tk->tkr_mono.cycle_last
tk->tkr_raw.cycle_last
clock->mult;
WARN_ON(timekeeping_suspended);
*offset);
tk_xtime(tk);
timespec64_sub(tk->wall_to_monotonic,
CLOCK_SOURCE_VALID_FOR_HRES;
cycle_interval
clock_set
leap;
(!(txc->modes
commit;
consumer_fifo
event_status
EVENT_DROPPED;
(!bpage)
(consumer)
trace_printk("Running
ring_buffer_free(buffer);
(!uid_eq(uid,
cred->uid))
CONFIG_MULTIUSER
(rgid
ns_capable(old->user_ns,
(egid
new->egid
!gid_eq(kegid,
new->egid;
gid_eq(kgid,
old->sgid))
(ruid
(euid
!uid_eq(keuid,
new->euid;
kuid
ability
cred->suid);
->real_parent
(resource
(new_rlim)
(vma
user_auxv[AT_VECTOR_SIZE
arg4,
mem_total
<linux/bug.h>
task_work_add()
KDB_CMD_CPU
"0x%lx"
kdb_machreg_fmt0
*prefix_name,
PERF_NR_CONTEXTS;
exclusive_console
preferred_console
dictionary
continuation
facility;
syslog_idx;
syslog_prev;
syslog_partial;
LOG_LINE_MAX
printk_log)
msg->text_len;
dict_len,
dict,
(user->seq
'+'
'\\')
VMCOREINFO_OFFSET(printk_log,
__LOG_BUF_LEN);
S_IWUSR);
(prefix
LOG_NEWLINE))
textlen
saved_console_loglevel
LOGLEVEL_DEFAULT;
LOGLEVEL_DEFAULT)
cont.len
raw_spin_unlock(&logbuf_lock);
strcpy(buf,
@action:
console_may_schedule
console_idx
console_prev
(console_drivers
console_drivers;
*dumper)
dumper->cur_seq
oldest
(dumper->cur_seq
dumper->next_seq)
(CPUCLOCK_WHICH(which_clock)
incr;
CPUCLOCK_SCHED:
cpu_timer_list
(CPUCLOCK_PERTHREAD(timer->it_clock))
soft;
check_timers_list(++timers,
describing
*chan)
rchan_callbacks
(chan->buf[i])
n_subbufs;
@length:
produced
padding;
read_descriptor_t
total_len
MAX_SCHEDULE_TIMEOUT,
interruptible.
-ERESTARTSYS
Key
enter.
y;
SECS_PER_DAY;
spin_lock_irqsave(&watchdog_lock,
(!(cs->flags
spin_unlock_irqrestore(&watchdog_lock,
int64_t
wd_list)
curr_clocksource)
rating
@freq:
bits:
strcpy(buffer,
longest
(ped
escape_data
arrow
diag,
memcpy(tmpbuffer,
cp,
kdb_printf("\r");
(tab
')
kdb_buffer;
cp2
strlen(kdb_buffer);
kdb_buffer
RING
BUFFER
|page
RB_MAX_SMALL_DATA
event->array[0];
RB_LEN_TIME_EXTEND;
(BUF_PAGE_SIZE
"\tfield:
(full)
RB_PAGE_HEAD);
cpu_buffer->head_page
commit.
list_del_init(&bpage->list);
cpu_buffer->pages
INIT_LIST_HEAD(&cpu_buffer->new_pages);
kfree(buffer);
atomic_inc(&cpu_buffer->record_disabled);
cpu_buffer->reader_page)
cpu_buffer->read
atomic_dec(&cpu_buffer->record_disabled);
&cpu_buffer->new_pages,
long)event;
~PAGE_MASK)
cpu_buffer->reader_page->read
(iter->head_page
iter->head
TS_SHIFT;
&tail_page->write);
out_reset;
nr_loops
++nr_loops
cpu_buffer->reader_page;
cpu_buffer->lost_events
rb_reader_event(cpu_buffer);
rb_inc_iter(iter);
rb_advance_iter(iter);
dolock;
dolock
rb_ok_to_lock();
raw_spin_lock(&cpu_buffer->reader_lock);
raw_spin_unlock(&cpu_buffer->reader_lock);
total_lost
local_softirq_pending();
&(t->next));
__this_cpu_write(tasklet_hi_vec.tail,
&t->state))
tasklet_unlock(t);
cpu).head
do_for_each_event_file(tr,
is_signed,
while_for_each_event_file();
__ftrace_event_enable_disable(file,
*dir)
kfree(dir);
*match,
ftrace_event_name(call);
event_file_data(filp);
(!system)
(!*ptr)
'_')
**_data)
**pdata
**)_data;
*pdata;
event_enable_print,
event_enable_init,
event_enable_free,
data_rloc
*args,
(*createfn)(int,
tp->args[i].offset);
ent_size
.target_value
.default_value
.no_constraint_value
.notifiers
.constraints
spin_lock_irqsave(&pm_qos_lock,
spin_unlock_irqrestore(&pm_qos_lock,
prev_value,
@req:
@pm_qos_class:
&req->node,
PM_QOS_DEFAULT_VALUE);
pm_qos_class,
pm_qos_class;
seed
mutex_unlock(&cpu_hotplug.lock);
cpu_hotplug_disabled
nr_calls
&nr_calls);
hcpu);
CONFIG_PM_SLEEP_SMP
spin_lock_irqsave(&async_lock,
spin_unlock_irqrestore(&async_lock,
(initcall_debug
checkpointing
MAX_EXECVE_AUDIT_LEN
ctx->current_state
AUDIT_RECORD_CONTEXT;
ctx->tree_count
ctx->trees
31;
cred->fsuid);
audit_uid_comparator(tsk->loginuid,
audit_gid_comparator(cred->gid,
(f->type)
AUDIT_EUID:
AUDIT_SUID:
AUDIT_FSUID:
AUDIT_EGID:
AUDIT_SGID:
AUDIT_FSGID:
f->val)
AUDIT_OBJ_UID:
AUDIT_OBJ_GID:
AUDIT_LOGINUID_SET:
AUDIT_RECORD_CONTEXT)
sessionid,
to_send);
(has_cntl)
*len_sent
proctitle
(!audit_enabled)
audit_alloc_name(context,
AUDIT_NAME_FULL;
AUDIT_TYPE_UNKNOWN)
oldsessionid,
msgq
args,
(dev->set_mode)
dev->state
dev->id);
cluster
Notifies
Notified
VFP
co-processor,
extensions,
save/restore
__raw_notifier_call_chain.
read_lock(&cpu_pm_notifier_lock);
read_unlock(&cpu_pm_notifier_lock);
tick_length_base;
STA_UNSYNC;
(s)
(shift)
pps_freq
NTP_SCALE_SHIFT);
NTP_SCALE_SHIFT;
time_offset
ntp_update_frequency();
MAXFREQ
"hardpps:
elfcorehdr_addr
&end);
*ftbl)
listener_list
&listeners->list,
*rep_skb;
&rep_skb,
mk_reply(rep_skb,
(!stats)
.end
resting
-EADDRNOTAVAIL;
(mend
(!image)
image->control_page
out_free_image;
kimage_alloc_control_pages(image,
pr_err("Could
buf_len)
cmdline_ptr,
kimage_file_post_load_cleanup(image);
hole_start
*image->entry
IND_INDIRECTION)
ubytes
mbytes
kbuf,
dest_image
crashkernel
memparse(cur,
cur++;
*crash_size
ck_cmdline
temp_start,
->sh_offset
(!(sechdrs[i].sh_flags
align;
sechdrs[i].sh_size;
bss
curr_load_addr
memcpy((void
todo
*handle);
kallsyms_num_syms;
namebuf,
(is_ksym_addr(addr))
symbolsize,
@address
div_factor);
posth_val
div_factor)
rand1)
.kp.symbol_name
'N';
trace_seq_printf(
ns2usecs(iter->ts);
long)t;
trace_nop_print;
field->ip);
.binary
newdev,
timekeeping.
PN(x)
"%-45s:%14Ld.%06ld\n",
np;
rnp->exp_tasks
sync_rcu_preempt_exp_mutex.
(rnp->gp_tasks
spincnt
kthread@rcu_wait"));
kthread@rcu_yield"));
rcu_nocb_mask);
(!rcu_nocb_poll)
follower.
my_rdp->nocb_leader_sleep
spawn
nohz_full=
WARN_ON_ONCE(rdtp->dynticks_idle_nesting
defined(CONFIG_TASKS_RCU)
symtab->sym_name
KDB_STATE_CLEAR(SUPPRESS);
(8*sizeof(unsigned
alignment.
dah_first);
best->size
h_offset
h->next
POISON_END;
%Ld
%Lu
.%-15s:
pm_vt_switch
public_key_signature
pr_devel("<==%s()
MPI
*q++
modlen
CTL_ULONG,
kernel_read(file,
nlen;
pid_max
pidmap
spin_unlock_irq(&pidmap_lock);
libcap
p->child;
res->sibling;
root->start)
iomem
this->start
this->end
__adjust_resource(res,
old_count
ftrace_trace_probe_callback(ops,
stats->ac_flag
kdb_commands
kdb_max_commands
'match'
address-expression
argc)
(symbol
diag);
defcmd_set_count;
(defcmd_in_progress)
KDB_CMD_HISTORY_COUNT;
KDB_DEBUG_STATE("kdb_main_loop
count>
bytesperword,
(bytesperword
bytesperword)
fmtchar);
(!rname)
rname,
kdb_printf("buffer
(prev_state
(kdb_task_state(p,
for_each_kdbcmd(kp,
kdb_md,
"<vaddr>",
filterlist
f->op
entry->rule.listnr
(entry->rule.listnr
&remain,
(IS_ERR(str))
entry->rule.buflen
data->buflen
audit_pack_string(&bufp,
lsm_rule
rule.
dont_count
arch_uprobe
vaddr;
is_register)
UPROBE_SWBP_INSN_SIZE);
(!uprobe)
uc;
vaddr_to_offset(vma,
area->vaddr;
(slot_nr
xol_vaddr;
(!utask)
ri;
*utask;
utask
utask->state
child->jobctl
__TASK_TRACED);
access_process_vm(tsk,
unlock_task_sighand(child,
iovec
compat_long_t,
cap_setid,
mutex_lock(&userns_state_mutex);
@kuid
@kgid
@kprojid
extent->count);
(blk_tracer)
sizeof(*t)
t->action
bt->start_lba
blk_trace_free(bt);
bdev);
bt->trace_state
bdev,
blk_trace_remove(q);
nr_bytes,
rq->errors,
*bio)
driver-specific
cmd[TASK_COMM_LEN];
trace_find_cmdline(ent->pid,
*pqueue;
pqueue
*padata;
parallelization
pd;
padata_serial_queue
*squeue;
squeue
__padata_stop(pinst);
padata_alloc_pd(pinst,
pd);
PADATA_CPU_SERIAL
VERBOSE_TOROUT_STRING("Stopping
mutex_lock(&fullstop_mutex);
kmalloced_param
mutex_lock(&param_lock);
mutex_unlock(&param_lock);
param_attribute
to_module_kobject(kobj);
1998-2004
unassigned
quiesce
ago
longstanding
irq_startup(desc,
stage,
IRQS_AUTODETECT
IRQS_AUTODETECT)
IRQS_WAITING))
~IRQS_AUTODETECT;
irq_found
<manfred@colorfullife.com>
Kleen.
looping
undone
behavior.
->rcu_read_unlock_special
t->rcu_read_lock_nesting
debug_locks
section?
absence
disallowed
perspective,
rcu_read_lock_bh_held()
RCU-bh
Activation
call_rcu().
defined(CONFIG_RCU_TRACE)
c_old,
manipulations
like.
timeouts.
stall.
operational.
rcu-tasks
theory,
cond_resched().
;-)
put_task_struct(t);
stalls
sched_show_task(t);
desired.
blocked.
get_task_struct(t);
lastreport
schedule_timeout_interruptible(HZ);
->on_rq
BUG_ON(IS_ERR(t));
test_callback);
early_boot_test_counter++;
mutex_lock(&gcov_lock);
mutex_unlock(&gcov_lock);
initially.
kdb_set_current_task(p);
refers
manual
KDB_BADADDR;
kdb_ps1(p);
kdb_curr_task(cpu);
(task_curr(p))
*)argv[1],
~0UL,
kdb_printf("No
KDB_TSK(cpu));
<linux/version.h>
<asm/pgtable.h>
SPARE_PAGES
Preferred
impossible,
"resume"
auxiliary
resume,
allocated_unsafe_pages;
(page)
clear_nosave_free)
__packed;
grows
individually.
(!lp)
lp->next
browsing
zones
assumptions
BM_END_OF_MAP
start_pfn;
Root
cur;
safe_needed);
zone->levels;
alloc_rtree_node(gfp_mask,
ca,
((i
(!zone)
BM_BITS_PER_BLOCK);
bm->cur.zone
rtree_node,
aux,
contiguous
zone_end;
ext->end)
ext->start)
ext->start
(zone_end
Error:
Exit;
pfn.
BUG_ON(error);
set_bit(bit,
clear_bit(bit,
nosave_region
region->end_pfn
[mem
pages:
estimate
(is_highmem(zone))
(!pfn_valid(pfn))
mark_free_pages(zone);
pfn))
n++;
swsusp_page_is_free(page))
sizeof(long);
do_copy_page(dst,
kunmap_atomic(dst);
Page
suspending
(during
initially
fr_pfn
fb_pfn
loop;
alloc_normal
alloc_highmem
GFP_IMAGE
(GFP_KERNEL
GFP
value:
nr_pages--;
alloc;
alloc)
approximation
base)
count_data_pages();
save)
count_highmem_pages();
to_free_highmem
whichever
metadata
max_size;
avail_normal);
accommodate
hurt
high,
First,
separately.
pages_highmem;
(pages
exhausted
pages);
%u,
lesser
to_alloc;
memory_bm_set_bit(bm,
(nr_highmem
cold
_not_
(%d
snapshot_get_image_size();
macro.
condition,
nr_copy_pages)
kunmap_atomic(kaddr);
BM_END_OF_MAP)
mismatch:
disk,
*nr_highmem_p)
suspend_write_next()
last_highmem_page
(!pbe)
safe_pages_list;
sp_list
handle->sync_read
keys.
cgroupfs
ancestors
*freezer)
CGROUP_FREEZING;
atomic_inc(&system_freezing_cnt);
(freezer->state
atomic_dec(&system_freezing_cnt);
clear_frozen
freeze_task(task);
~CGROUP_FROZEN;
cgroup_freezer
skipping
@freezer
freezer_should_skip()
freeze,
files[]
.css_online
.css_offline
non-public
<linux/threads.h>
MAX_RCU_LVLS
RCU_FANOUT_1
CONFIG_RCU_FANOUT)
RCU_FANOUT_1)
rcu_num_lvls;
sysfs.
Groups
tail.
rcu_nocb_kthread()
upcoming
rcu_for_each_node_breadth_first(rsp,
((rnp)
(rnp)
(rnp)++)
singleton
rcu_data.
RCU_DONE_TAIL
RCU_NEXT_TAIL
RCU_NEXT_SIZE
Per-CPU
rsp->completed
gp
of.
adopted
blimit;
cacheline.
follower,
RCU_JIFFIES_TILL_FORCE_QS
(first
consisting
levels.
flavor.
Orphaned
Tail
_rcu_barrier().
ticket.
failures.
reluctant
Initial
RCU_TREE_NONCORE
declarations
rcu_preempt_blocked_readers_cgp(struct
rcu_print_detail_task_stall(struct
rcu_print_task_stall(struct
rcu_preempt_check_blocked_tasks(struct
rcu_initiate_boost(struct
rcu_preempt_boost_start_gp(struct
rcu_prepare_kthreads(int
print_cpu_stall_info(struct
zero_cpu_stall_ticks(struct
rcu_nocb_cpu_needs_barrier(struct
rcu_nocb_gp_set(struct
rcu_nocb_gp_cleanup(struct
rcu_init_one_nocb(struct
__call_rcu_nocb(struct
rcu_nocb_adopt_orphan_cbs(struct
rcu_nocb_need_deferred_wakeup(struct
do_nocb_deferred_wakeup(struct
rcu_boot_init_nocb_percpu_data(struct
rcu_spawn_all_nocb_kthreads(int
init_nocb_callback_list(struct
rcu_sysidle_enter(int
rcu_sysidle_exit(int
rcu_sysidle_check_cpu(struct
is_sysidle_rcu_state(struct
rcu_sysidle_report_gp(struct
maxj);
rcu_sysidle_init_percpu_data(struct
atomic_long_read(&rdp->nocb_q_count);
__LINUX_RCU_H
DYNTICK_TASK_NEST_MASK
DYNTICK_TASK_NEST_VALUE
offset));
resched_cpu(int
C_A_D
Notifier
machine_restart
drivers.
typical
@cmd:
syscore_shutdown();
Reboot
*pid_ns
superuser
magic2
hibernate();
argv_split(GFP_KERNEL,
UMH_WAIT_EXEC);
'g':
strchr(str,
CONFIG_DEBUG_RT_MUTEXES
long)lock->owner
Stops
made.
Changes
<linux/ratelimit.h>
caches
implicitly
SIG_DFL
SIGNAL_UNKILLABLE)
signal->sig[0]
blocked->sig[0];
recalculating
clear_thread_flag(TIF_SIGPENDING);
ffz(~x)
noop.
%JOBCTL_STOP_PENDING
together.
JOBCTL_STOP_PENDING)
JOBCTL_STOP_DEQUEUED;
participating
SIGNAL_STOP_STOPPED))
*q
@t
free_uid(user);
q->flags
user;
list_del_init(&q->list);
clear_tsk_thread_flag(t,
SIG_IGN;
ka->sa.sa_handler
SIG_DFL)
*priv),
*first
info->si_uid
Dequeue
delivery
Changing
noise
marker
handled.
((info->si_code
spin_unlock(&tsk->sighand->siglock);
spin_lock(&tsk->sighand->siglock);
death
(!valid_signal(sig))
wanting
taken.
re-trap
traps
delivered,
p->signal;
&t->pending);
task_clear_jobctl_pending(t,
finished,
CLD_STOPPED,
SIGNAL_STOP_STOPPED)
signal->flags
SIGKILL)
(!group
woken.
unblocked
killable
everybody
while_each_thread(p,
(current_user_ns()
user_ns),
from_ancestor_ns
SIGSTOP
(q)
q->info.si_pid
loss
signr)
show_regs(regs);
(&str,
send_signal(sig,
(lock_task_sighand(p,
unblock
local_irq_restore(*flags);
tsk->sighand
__exit_signal().
do_send_sig_info(sig,
sends
etc)
do_each_pid_task(pgrp,
group_send_sig_info(sig,
while_each_pid_task(pgrp,
pid_task(pid,
kill_pid_info(sig,
!uid_eq(cred->euid,
"current"
secid)
secid);
wrong.
__si_special(priv),
1].sa.sa_handler
&flags)))
do_notify_parent_cldstop
SIGCHLD)
0x7f)
CLD_KILLED;
CLD_EXITED;
sys_wait4
@tsk's
%false,
almost
current->last_siginfo
exit_code;
detach,
TASK_STOPPED
reacquire
memset(&info,
initiating
retries
ptrace.
(notify)
(new)
ksignal
spin_lock_irq(&sighand->siglock);
encodes
either,
duplicate.
&current->blocked,
kills
single-step
Tracing
notified.
blocked;
sigmask
group-wide
tsk->blocked;
notification.
points.
*newset)
directly,
sigmask(SIGKILL)
(how)
SIG_BLOCK:
&tsk->blocked,
SIG_UNBLOCK:
SIG_SETMASK:
old_set,
(copy_from_user(&new_set,
sigdelsetmask(&new_set,
sigmask(SIGKILL)|sigmask(SIGSTOP));
(copy_to_user(oset,
touches
sizeof(siginfo_t)))
(from->si_code
@which:
MAX_SCHEDULE_TIMEOUT;
(ts)
ready,
fine,
&mask);
synchronously
uts
task_tgid_vnr(current);
dies
solves
PIDs
pid))
CLONE_SIGHAND
*k;
*uss,
sp)
&uss->ss_sp)
&uss->ss_flags)
&uss->ss_size);
ss_flags
EFAULT
__ARCH_WANT_SYS_SIGPROCMASK
platforms
new_ka,
old_ka;
&new_ka
&old_ka
(t->state
risks
modified,
OP_EQ,
FILT_ERR_INVALID_OP,
FILT_ERR_UNBALANCED_PAREN,
FILT_ERR_BAD_SUBSYS_FILTER,
"No
"Unbalanced
"Too
'ip'
!!match
&pred->regex,
str_item
strlen
inverted
*search
*pred)
(enum
pred,
op->op
filter_match_preds_data
*preds;
WALK_PRED_PARENT;
matches,
filter->preds
preds
walk_pred_tree(preds,
newlen
filter->filter_string);
string);
system->filter;
*stack,
(!pred)
*dest
dest->op
FILTER_PRED_FOLD
dest->left
leafs
pred->fn
field->size,
strcmp(ps->ops[i].string,
"OP_NONE");
MAX_FILTER_STR_VAL);
*opstack_op;
(!filter_opstack_empty(ps))
elt
elt->op
((ch
(isspace(ch))
(strlen(curr_operand(ps)))
postfix_append_operand(ps,
curr_operand(ps));
postfix_append_op(ps,
top_op);
(!field)
list_for_each_entry(elt,
&ps->postfix,
check_pred_data
((move
fold_pred_data
(*err)
speeding
operand1
filter_list
fail_mem;
(fail)
list_for_each_entry_safe(filter_item,
&filter_list,
list_del(&filter_item->list);
kfree(filter_item);
set_str)
*filterp
return)
create_filter(call,
filter_str,
function_filter_data
*count)
(!str)
spaces.
pieces
*field
pred->field;
NO
"(a
(h
visited
d->filter,
<linux/seccomp.h>
CONFIG_HAVE_ARCH_SECCOMP_FILTER
container
general,
singly-linked
syscall_get_nr(task,
syscall_get_arch();
sock_filter
ftest->code
open.
NULL)))
evaluated
seccomp_mode)
(child
cred_guard_mutex
*thread,
*caller;
sync.
(thread->seccomp.mode
dies.
fprog->len
BPF_MAXINSNS)
CAP_SYS_ADMIN
CAP_SYS_ADMIN)
ERR_PTR(-EACCES);
fp
rewrite
requested,
SECCOMP_FILTER_FLAG_TSYNC)
orig;
emulation
syscall;
this_syscall)
audit_seccomp(this_syscall,
current->seccomp.mode;
phase1_result
rmb();
filter_ret
else,
skip.
exec.
prctl
uargs
prefix.
aliases
ARCH_TRACE_IGNORE_COMPAT_SYSCALLS
syscalls,
entry->nb_args;
kmalloc(len
entry->nr
event_trigger_unlock_commit(ftrace_file,
ret)
mapped,
.system
.define_fields
pr_info("event
Josh
<asm/byteorder.h>
<linux/torture.h>
"Holdoff
fqs_duration,
(s)");
disable.");
stall_cpu_holdoff,
test,
7,
nrealreaders;
ncbflooders;
1],
torture_runnable
defined(CONFIG_RCU_BOOST)
!defined(CONFIG_HOTPLUG_CPU)
do_div(ts,
Barrier
phase.
spin_unlock_bh(&rcu_torture_lock);
Operations
mdelay(longdelay_ms);
pipe.
primitives.
*rp
life!!!
deliberate
only!
1000000
RCUTORTURE_TASKS_OPS
priority-boost
occurred.
rcu_boost_inflight
->inflight
cycle.
(ULONG_CMP_LT(jiffies,
stutter_wait("rcu_torture_boost");
test_boost_duration
call_rcu_time
!kthread_should_stop())
test_boost_interval
(!kthread_should_stop()
death.
induces
occurrence
corner
Testing
rp
udelay(torture_random(&rand)
0x3ff);
rcu_dereference_check(rcu_torture_current,
writer_task);
cur_ops->exp_sync();
rcu_torture_pipe_update(old_rp);
cur_ops->sync();
ftrace_dump(DUMP_ALL);
rcutorture_trace_dump();
(completed
(irqreader
cur_ops->irq_capable)
running).
cpu)[i];
rcu_torture_current
Periodically
*tag)
"---
tag,
boost_tasks[cpu]
*args)
VERBOSE_TOROUT_STRING("rcu_torture_stall
specified.
lastphase
barrier_phase
n_barrier_cbs;
reader_tasks[i]);
interrupting
world
fqs_duration
disabled.\n");
nrealreaders
VERBOSE_TOROUT_ERRSTRING("out
reader_tasks
(stutter
stutter
onoff_interval
S.
stages
Anzinger
Division
Original
Grothe
debuggerinfo_struct
kgdb_io
kgdb_active
extremely
*)bpt->bpt_addr,
BP_SET)
kgdb_break[i].bpt_addr);
breakno
((kgdb_break[i].state
(kgdb_break[i].bpt_addr
calls:
dbg_deactivate_sw_breakpoints();
dbg_activate_sw_breakpoints();
online_cpus
kgdb_info[cpu].ret_state
(kgdb_info[cpu].exception_state
kgdb_info[cpu].exception_state
(trace_on)
DCPU_IS_SLAVE);
dbg_touch_watchdogs();
CPUs.\n");
quit
kgdb_var;
*ks
&kgdb_var;
(arch_kgdb_ops.enable_nmi)
memset(ks,
kgdb_state));
ks->cpu
ks->ex_vector
ks->linux_regs
kgdb_cpu_enter(ks,
debugging,
connected,
(kgdb_io_module_registered)
communicate
disabled\n",
dbg_io_ops->read_char();
frozen;
(!atomic_add_unless(&snapshot_device_available,
O_ACCMODE)
data->free_bitmaps
!error;
data->frozen
simple_read_from_buffer(buf,
(!capable(CAP_SYS_ADMIN))
lock_device_hotplug();
sys_sync();
printk("done.\n");
thaw_kernel_threads();
(loff_t
(data->swap
unlock_device_hotplug();
mutex_unlock(&pm_mutex);
.unlocked_ioctl
.compat_ioctl
"snapshot",
*tl)
sleep)
ERR_PTR(-EBADF);
(f.file->f_op
(!value)
(ptr)
free_value;
bpf_prog_type
bpf_prog_type_list
__bpf_call_base;
(BPF_JMP
supplied,
prototype
bpf_prog_put(prog);
get_user(val,
sched/core.c
overhead.
avenrun[0]
Due
machine,
sample.
accurately
cross-cpu
uninterruptible
calc_load
negate
contribution,
separating
trick
(!time_before(jiffies,
delta.
accounting,
window.
fractional
frac_bits;
a2
e^n
crossed
missed.
'old'
&calc_load_tasks);
idled
2^idx
((2^idx
efficient
precomputed
idx.
72,
98,
j++;
pending_updates
division
this_rq->last_load_update_tick
__update_cpu_load(this_rq,
Documentation/RCU
*op)
g=%ld
cpu_is_offline(rdp->cpu)
qll,
*)v);
r_start,
r_next,
r_stop,
rnp->qsmask,
ACCESS_ONCE(rsp->completed);
(rsp
"smpboot.h"
*idle_thread_get(unsigned
exist.
(IS_ERR(tsk))
smpboot_thread_data
cleanup,
*ht,
position,
&hotplug_threads,
DEFINE_PER_CPU(atomic_t,
atomic_read(&per_cpu(cpu_hotplug_state,
cpu_wait_death()
-EIO
atomic_set(&per_cpu(cpu_hotplug_state,
(atomic_read(&per_cpu(cpu_hotplug_state,
despite
surviving
five
CPU_DEAD)
11,
oldstate,
timing
"trace_probe.h"
long)offset));
*vaddr
string_size)(struct
ASSIGN_FETCH_TYPE_END
nargs,
[FETCHARGS]
'p')
(argv[0][1]
(arg)
*arg++
argv[1];
&path);
path_put(&path);
(!tail)
&tu->tp.args[i];
'=');
%s=%s",
ftrace_event_name(&tu->tp.call),
mutex;
*ucb;
ucb
sole
*ftrace_file)
&tu->tp.call;
WARN_ON(call
ftrace_file->event_call);
PAGE_SIZE))
trace_event_buffer_lock_reserve(&buffer,
memcpy(data,
container_of(event,
*link
~TP_FLAG_PROFILE;
FIELD_STRING_FUNC,
FIELD_STRING_RETIP,
FIELD_STRING_IP,
parg->type->fmttype,
parg->type->size,
parg->type->is_signed,
event->parent
*uc,
consumer);
this_cpu_ptr(call->perf_events);
perf_trace_buf_prepare(size,
call->event.funcs
call->class->define_fields
register_ftrace_event(&call->event);
trace_remove_event_call()
Profile
RET_PTR_TO_MAP_VALUE_OR_NULL,
defining
DEFINE_RCU_TPS(sname)
0UL
CONFIG_RCU_FANOUT_LEAF
rcu_nodes
rcu_cleanup_dead_rnp(struct
rcu_boost_kthread_setaffinity(struct
CONFIG_RCU_TORTURE_TEST_SLOW_INIT
ACCESS_ONCE(rsp->completed)
TPS("cpuqs"));
two.
QS.
Later
rcu_sched_qs();
10000;
ulong,
rsp->name,
terminated.
messages,
&rcu_sched_state;
advisory
rdp->nxtcompleted[i]))
*idle
ftrace_dump(DUMP_ORIG);
section.");
WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks)
DYNTICK_TASK_EXIT_IDLE;
exits,
underflow.
attention
watching
completing
trace_rcu_fqs(rdp->rsp->name,
acknowledged
enough,
assignments
lasting
rsp->gp_start
rcu_jiffies_till_stall_check();
gpa
cpu++)
ndetected
rcu_jiffies_till_stall_check()
rsp->name);
positives.
cleanup.
barriers,
js
gps
js))
complain.
irqs.
timely
earlier.
RCU_DONE_TAIL;
grouped
sublist,
(rdp->nxttail[i]
into.
(++i
rdp->nxttail[RCU_NEXT_TAIL];
compact
rdp->nxttail[RCU_DONE_TAIL];
rcu_advance_cbs(rsp,
rdp->completed
unlikely(ACCESS_ONCE(rdp->gpwrap)))
rdp->passed_quiesce
__this_cpu_read(rcu_qs_ctr);
ACCESS_ONCE(rnp->completed)
rnp->qsmaskinitnext
oldmask
(rcu_preempt_has_tasks(rnp))
finishes,
maxj
&isidle,
&maxj);
(ACCESS_ONCE(rsp->gp_flags)
needgp
WARN_ON_ONCE(rnp->qsmask);
Declare
letting
rcu_preempt_blocked_readers_cgp(rnp))
(rnp->parent
rnp->lock.
rcu_state_p
nonexistent
rcu_report_qs_rnp(mask,
interest.
__this_cpu_read(rcu_qs_ctr))
lies
rdp->grpmask;
((rnp->qsmask
Released
note_gp_changes(rsp,
Was
Orphan
counts.
rsp->qlen_lazy
rsp->qlen
ACCESS_ONCE(rdp->qlen)
*rdp->nxttail[RCU_DONE_TAIL];
rsp->orphan_nxttail
period:
rsp->orphan_donetail
rsp->qlen;
rcu_barrier().
*rnp_leaf)
guarantee.
rcu_boost_kthread_setaffinity(rnp,
appropriate.
rdp->qlen,
is_idle_task(current),
handlers.
debug_rcu_head_unqueue(list);
list))
(need_resched()
rdp->blimit
rsp->n_force_qs;
remaining.
rcu_sched_qs()
__rcu_process_callbacks(struct
rcu_process_callbacks(struct
(cpu_is_offline(smp_processor_id()))
raise_softirq(RCU_SOFTIRQ);
(irqs_disabled_flags(flags)
waiting.
force_quiescent_state()
__call_rcu(struct
*rcu),
Offline
&head->next;
rdp->qlen);
named
Until
then,
rcu-sched
kernels,
(rcu_gp_is_expedited())
meantime.
harmless.
billion
Brute-force
elapse,
synchronize_sched_expedited()
row,
(!try_get_online_cpus())
atomic_long_inc(&rsp->expedited_normal);
&per_cpu(rcu_dynticks,
atomic_long_read(&rsp->expedited_done);
(ULONG_CMP_GE((ulong)s,
RCU-related
non-NULL,
snap_done
computes
you,
invocations
per_cpu_ptr(rcu_state_p->rda,
PM_HIBERNATION_PREPARE:
256)
PM_POST_HIBERNATION:
(kthread_prio
Limited
sched_setscheduler_nocheck(t,
&sp);
wake_up_process(t);
fanout,
ccur;
cpustride
Silence
buf[i]);
compile-time
(rcu_fanout_leaf
Adjusting
easier.
CPU_UP_PREPARE,
*lg,
arch_spin_lock(lock);
lock_release(&lg->lock_dep_map,
arch_spin_unlock(lock);
'watchdog_thresh'
SOFT_WATCHDOG_ENABLED
hardlockup_panic
5))
simple_strtoul(str,
extreme
resolution,
resetting
(watchdog_enabled
watchdog_nmi_enable(unsigned
watchdog_nmi_disable(unsigned
hogging
print_modules();
print_irqtrace_events(current);
hung
*hrtimer
raw_cpu_ptr(&watchdog_hrtimer);
watchdog_nmi_enable(cpu);
hrtimer_start
HRTIMER_MODE_REL_PINNED);
online)
watchdog_nmi_enable()
enabled?
out_enable;
displaying
(PTR_ERR(event)
(cpu%i):
-ENOENT)
out_enable:
(!watchdog_running)
.park
Enable/disable
detectors.
-------------------|-----------------------|-----------------------------
or'ed
(!write)
@page:
Adding
bio_put(bio);
(bio_chain
(bio)
0x01,
0x02,
call_function_data
*csd;
llist_head,
smp-call-function
i));
csd->flags
llist_node
%pS
remotely
his
careful,
cpumask_next_and(cpu,
@wait
next_cpu,
(next_cpu
NR_CPUS;
nr_cpus;
rest.
processor.
cpus);
matter,
starve
locality.
decreases
thundering
herd
attack
run,
automatic
recorded.
alloc_percpu(int);
->rw_sem
fast-path
preempt-disabled
acquire/release
down_write
Finally
update_fast_ctr()
succeed.
*stop)
JUMP_LABEL_ENABLE);
JUMP_LABEL_DISABLE);
iter_stop)
definitely
long)iter->key;
key->next
mod->jump_entries
key->next;
__jump_label_update(key,
jlm
MODULE_STATE_LIVE:
@end:
users,
CAP_FULL_SET,
*cred)
atomic_read(&cred->usage),
read_cred_subscribers(cred));
__builtin_return_address(0);
tsk->pid,
put_cred(cred);
tsk->cred
pinning
Accessing
blank
atomic_set(&new->usage,
memcpy(new,
execve()
new->thread_keyring
new->process_keyring
can,
(!(clone_flags
get_cred(new);
alter_cred_subscribers(new,
put_cred(new);
overridden
atomic_read(&new->usage),
read_cred_subscribers(new));
old->fsuid))
old->fsgid))
alter_cred_subscribers(old,
refs
Override
Revert
initialise
they'll
afterwards
inodes
}\n",
line)
filename,
low2highuid(user),
high2lowuid(from_kuid_munged(cred->user_ns,
suid
high2lowgid(from_kgid_munged(cred->user_ns,
egid
sgid
*group_info,
gidsetsize)
*group_info;
debug_mutex_lock_common(struct
mutex_set_owner(struct
mutex_clear_owner(struct
__add_wait_queue(q,
__add_wait_queue_tail(q,
opaque
differs
soon,
spin_unlock()
(list_empty(&wait->task_list))
(signal_pending_state(state,
list_del_init(&wait->task_list);
default_wake_function(wait,
WQ_FLAG_WOKEN;
smp_mb(),
autoremove_wake_function(wait,
waiting,
Nonzero
finish_wait(wq,
&q->wait);
bit_waitqueue(word,
DEFINE_WAIT_BIT(wait,
&key);
6;
CONFIG_HARDIRQS_SW_RESEND
resend
nr_irqs))
~IRQS_PENDING;
trace_iterator)
trace_iterator,
(cpu_file
cpu_file;
skip_lines
simple_strtol(argv[1],
(*cp)
(dhowells@redhat.com).
rwsem_acquire_read(&sem->dep_map,
rwsem_acquire(&sem->dep_map,
__down_write_trylock,
__down_write);
downgrade
lockdep:
OpenVZ,
SWsoft
nsproxy.
clone.
(!ns_capable(user_ns,
Unshare
(!(unshare_flags
user_ns
new_cred
(IS_ERR(file))
PTR_ERR(file);
copied,
DEBUGFS_FILENAME
uint64_t
time_passed
iters
"echo
non-pinned
Pinned
any,
{0};
bp->attr.bp_type
Simulate
@triggered:
old_addr
old_addr;
*bp;
PERF_EF_START))
bp->hw.state
TYPE_MAX;
err_alloc:
*match
@thread_fn:
@irqflags:
detach.
thread_fn,
dr
(!dr)
devres_free(dr);
dr);
dev_id
Class
Earliest
dl_rq->rb_leftmost
to_ratio(global_rt_period(),
dl_rq->overloaded
*next_node;
cpumask_any_and(cpu_active_mask,
tsk_cpus_allowed(p));
pull_dl_task(struct
informed
*pi_se)
deadlines
(time
etc.).
dl_se->dl_new
bandwidth.
typically,
large.
(dl_se->runtime
(dl_time_before(dl_se->deadline,
replenish
dl_se->dl_yielded
dl_se->dl_throttled
promised
equation
right;
pi_se);
rq_clock(rq);
act);
range,
dl_rq,
latter,
update_curr_dl()
curr->se.exec_start;
schedstat_set(curr->se.statistics.exec_max,
max(curr->se.statistics.exec_max,
delta_exec));
curr->se.sum_exec_runtime
account_group_exec_runtime(curr,
curr->se.exec_start
cpuacct_charge(curr,
sched_rt_avg_update(rq,
recompute
throttled,
enqueue_pushable_dl_task(rq,
semantic
update_rq_clock()
important.
*left
(!left)
re-start
RETRY_TASK;
(prev->sched_class
(hrtick_enabled(rq))
NEED_RESCHED
retain
hrtimer_cancel(timer);
algorithms
DEFINE_PER_CPU(cpumask_var_t,
(best_cpu
latest
topology.
cache-hot
(!cpumask_test_cpu(this_cpu,
SD_WAKE_AFFINE)
preempting
migrating.
cpumask_test_cpu(this_cpu,
rq->cpu))
Retry
task_cpu(p));
migrated.
WARN_ON(p
deactivate_task(src_rq,
rq->rd;
cpumask_weight(new_mask);
Nobody
any.
(!task_on_rq_queued(p)
(task_on_rq_queued(p)
.rq_online
.rq_offline
.switched_from
Completely
Interactivity
Galbraith
Srivatsa
Vaddagiri
<linux/cpuidle.h>
ilog(ncpus)),
concept
timeslices
field)
(default
Minimal
SCHED_OTHER
averaged
Amount
*lw,
lw->weight
min_t(int,
num_online_cpus(),
update_sysctl();
lw.weight
(delta_exec
shift--;
hint
/**************************************************************
"owned"
update_cfs_rq_blocked_load(struct
(se_depth
pse_depth)
account_cfs_rq_runtime(struct
se->vruntime;
slices
task_h_load(struct
init_task_runnable_average(struct
cfs_rq->curr;
curr->vruntime
account_cfs_rq_runtime(cfs_rq,
stats:
dequeueing
sysctl_numa_balancing_scan_size
256;
nr_scan_pages;
MAX_SCAN_WINDOW
floor
task_scan_min(p);
task_node(p));
faults.
NR_NUMA_HINT_FAULT_TYPES
locality
occupy
(!p->numa_faults)
0)]
fancy
for_each_online_node(node)
topology,
closer
mesh
placement.
total_faults
src_nid,
p->numa_group;
dst_nid
last_cpupid,
samples
fault,
4);
effective_load(struct
wl,
numa_stats
weighted_cpuload(cpu);
cores
capacity,
dst_nid;
env->best_imp
dst_load,
---------
->curr
taskimp
env->src_nid,
env->dst_nid,
(imp
env->best_imp)
assign;
balanced.
around,
win.
env->dst_cpu
env->dst_cpu);
cur,
groupimp);
.p
.src_cpu
groupweight;
SD_NUMA
elsewhere,
interleaved
locations.
(env.best_cpu
rescheduled
(task_node(p)
migrations,
max_faults)
rate,
majority
shared,
period_slot;
(local
migrations
magnitude
produces
(node_distance(a,
winner
max_nid
seq)
numa_faults
f_weight
*priv)
(!grp)
grp->faults[i]
grp->total_faults
p->numa_faults
Got
@node.
counted
cares
p->mm
(time_before(now,
inaccessible
(!(vma->vm_flags
skipped.
(vma)
tg_weight;
throttled_hierarchy(struct
shares);
half-life
y^n
(unlikely(n
63))
local_n
combine
LOAD_AVG_PERIOD;
spanning
be:
1/2
1ms
1024us
u_i
runnable.
u_1*y
u_2*y^2
*sa,
runnable,
running)
runnable_contrib;
decayed
((s64)delta
1us
(runnable)
atomic64_read(&cfs_rq->decay_counter);
decays);
cfs_rq->tg_load_contrib
averages
runnable_avg;
co-ordinating
update_rq_runnable_avg(struct
runnable)
se->avg.utilization_avg_contrib;
cfs_rq_clock_task(struct
cfs_rq->utilization_load_avg
appropriately
decay_count
constructed
(se->avg.decay_count)
load-balancer
idle_enter_fair(struct
idle_exit_fair(struct
idle_balance(struct
check_enqueue_throttle(struct
ENQUEUE_WAKEUP);
place_entity(cfs_rq,
(cfs_rq->nr_running
(cfs_rq->last
(cfs_rq->next
(cfs_rq->skip
return_cfs_rq_runtime(struct
DEQUEUE_SLEEP);
Preempt
margin
__pick_first_entity(cfs_rq);
entity_before(curr,
check_cfs_rq_runtime(struct
cfs_b->lock
sched_clock_cpu(smp_processor_id());
__refill_cfs_bandwidth_runtime(cfs_b);
__start_cfs_bandwidth(cfs_b,
cfs_b->runtime_expires;
(cfs_rq->runtime_remaining
expired.
cfs_b->runtime_expires)
delta_exec)
dest_cpu)
task_delta;
rq->idle
unthrottle_cfs_rq(cfs_rq);
runtime_expires;
cfs_b->timer_active
ensured
occurring
precedes
(!cfs_bandwidth_used())
unthrottle
hrtimer_cb_get_time(timer);
overrun);
init_cfs_bandwidth(struct
restarted
hrtick_start_fair(rq,
loss.
HZ))
@tg
using:
rw_i
differences
'S'
W
pull.
prev_cpu,
prev_eff_load
sd_flag)
*group
avg_load
toward
group->sgc->capacity;
(group
latest_idle_timestamp
sched_group_cpus(group),
tsk_cpus_allowed(p))
rq->idle_stamp;
rq->idle_stamp
weighted_cpuload(i);
target)
CPU1
(tmp->flags
w(c,
triggering
idle_balance()
vruntime,
set_next_entity(cfs_rq,
load-balancing
i,j
\Sum_j
[XXX
2^i
aggressive
s_k,i
-----
patches
*dst_rq;
(src_nid
tsk_cache_hot
sched_group.
TASK_ON_RQ_MIGRATING;
detach_one_task()
(env->imbalance
sched_nr_migrate_break;
activate_task(rq,
attaches
removal.
Helpers
find_busiest_group
group_type
(idle)
((sd->flags
used;
build_sched_domains()
sgc
covering
Something
Clearly
busiest;
tricky
group_is_overloaded
load_idx,
busier
&sds->busiest_stat;
regular;
&sds->local_stat;
packing
minor
calculated.
*local,
(busiest->load_per_task
local->avg_load
busiest->group_capacity
min(busiest->load_per_task,
local->group_capacity
throughput
(busiest->group_type
SD_BALANCE_NEWIDLE
busiest_load
busiest_capacity
(balance_cpu
.dst_cpu
parallel.
task_rq_lock()
given_cpu.
practice
moved.
pollute
sd->nr_balance_failed
interval,
get_sd_balance_interval(sd,
curr_cost
update_next_balance(sd,
&next_balance);
domain_cost;
nohz_flags(cpu));
rcu_dereference(per_cpu(sd_busy,
CPU_IDLE);
interval))
due,
rq->next_balance))
significantly
*h)
queued);
se->parent
migrates
ongoing
free_fair_sched_group(struct
alloc_fair_sched_group(struct
unregister_fair_sched_group(struct
sched_group_set_shares(struct
runqueue:
nice;
autogroup_init(struct
autogroup_free(struct
autogroup_path(struct
kzalloc(PAGE_SIZE,
ns_alloc_inum(&ns->ns);
ns->ns.ops
ns->level
ns->nr_hashed
ns_free_inum(&ns->ns);
*old_ns)
unnecessarily
SIGCHLD,
last_pid
cmd)
container_of(ns,
*nsproxy,
proc_ns_operations
.put
.install
descriptor.
cpumask_pr_args(mask));
cpumask_copy(mask,
(!alloc_cpumask_var(&new_value,
cpumask_parse_user(buffer,
accidentally
new_value)
free_cpumask_var(new_value);
desc->action
action->next)
action->name
register_handler_proc(unsigned
[MAX_NAMELEN];
memset(name,
MAX_NAMELEN);
register_irq_proc(unsigned
(desc->irq_data.chip
unregister_irq_proc(unsigned
"%u",
unregister_handler_proc(unsigned
for_each_irq_desc(irq,
for_each_online_cpu(j)
(!action
%8s",
<linux/skbuff.h>
AUDIT_NAMES
names_list
ppid;
nargs;
mqd_t
*prefix,
audit_pid;
AUDIT_NAME_FULL
*payload,
audit_netlink_list
*watch);
*watch,
ino,
AUDIT_DISABLED
'bpf_exit'
program,
semantics.
R1-R5
bpf_context
BPF_REG_1,
recognize
-20
constant.
UNKNOWN_VALUE
map's
ARG_PTR_TO_MAP_KEY
'key'
safely,
knowing
BPF_REG_2,
CONST_PTR_TO_MAP
NOT_INIT
prune
'insn_idx'
"fp",
MAX_BPF_REG;
NOT_INIT)
STACK_SPILL)
-MAX_BPF_STACK
BPF_ALU64)
BPF_MEM)
%+d)
BPF_IND)
BPF_JMP)
BPF_CALL)
pc%+d\n",
sizeof(env->cur_state));
insn_idx,
invalid\n",
BPF_DW)
*state,
value_regno)
(value_regno
state->spilled_regs[(MAX_BPF_STACK
i]
reg_state)
misc
mark_reg_unknown_value(state->regs,
size=%d\n",
bpf_access_type
(state->regs[regno].type
PTR_TO_CTX)
BPF_WRITE,
state->regs;
arg_type
map_ptr,
check_stack_boundary(env,
regno
func_id);
verbose("unknown
(fn->ret_type
BPF_END
(BPF_CLASS(insn->code)
sub,
BPF_JEQ)
fallthrough
other_branch->regs[insn->dst_reg].type
insn\n");
CTX
SRC
procedure
for:
edges
labelled
vertex
14
17
0x20
DISCOVERED
0x10,
0x20,
(e
bug\n");
cfg
conservative
do_print_state
%d:",
dst_reg
insn->dst_reg
class);
BPF_IMM
env->used_map_cnt;
map.
global,
skip_full_check;
issue.
__field_struct
parent_ip
<--
__entry->func,
Context
FTRACE_CTX_FIELDS
prev_prio
next_state
ctx_switch_entry,
==>
F_printk("%ps:
*)__entry->ip,
TRACE_MMIO_RW,
%x
%x",
TRACE_MMIO_MAP,
Menage
*cpuusage;
*ca;
css_ca(css);
per_cpu_ptr(ca->cpuusage,
*sf,
cpuacct_charge(struct
cpuacct_account_field(struct
.early_init
mutex_remove_waiter(lock,
CONFIG_MUTEX_SPIN_ON_OWNER
tc->cc->mask;
tc->mask,
SUSE
GmbH
GPLv2
stop_two_cpus
spin_lock_irqsave(&stopper->lock,
wake_up_process(p);
spin_unlock_irqrestore(&stopper->lock,
.done
cpu_stop_queue_work(cpu,
wait_for_completion(&done.completion);
done.executed
done.ret
irq_cpu_stop_queue_work_info
msdata
.active_cpus
multi_cpu_stop,
set_state(&msdata,
MULTI_STOP_PREPARE);
@cpumask.
mutex_unlock(&stop_cpus_mutex);
(work)
*arg
kallsyms_lookup((unsigned
*cpus)
<linux/rbtree.h>
mutex_lock(&wakelocks_lock);
mutex_unlock(&wakelocks_lock);
CONFIG_PM_WAKELOCKS_LIMIT
300
(*str
NSEC_PER_MSEC);
sched_clock_irqtime
decrementing
cputime_scaled)
p->utime
p->utimescaled
cputime_scaled;
cpustat.
cpustat
__account_system_time(p,
CONFIG_PARAVIRT
steal;
solely
user_tick,
*ut
*st
((p
@ticks:
rtime)
big.
grow
shrink
precision.
(utime
task_cputime(p,
CONFIG_VIRT_CPU_ACCOUNTING_GEN
delta_cpu
get_vtime_delta(tsk);
tsk->vtime_snap_whence
VTIME_SYS;
(t->vtime_snap_whence
sdelta;
<linux/moduleloader.h>
BPF_R6
helper.
gfp_flags
__GFP_HIGHMEM
(aux
aux;
reallocated
bpf_binary_header
*hdr;
hole
@insn:
SRC,
SRC);
IMM);
*(SIZE
*(u64
skb.
bpf_load_pointer((struct
CTX,
(likely(ptr
Sort
kernel's
(!e)
long)_stext
@addr
others.
(core_kernel_text(addr))
dereferencing
detected\n");
dead;
References
anchors
MSB
difference.
tree->goner
chunk->count;
*chunk;
p->index
(!size)
(owner->root
chunk)
fsnotify_destroy_mark(entry,
list_del_init(&owner->same_root);
p->owner
(tree->goner)
&tree->chunks);
&chunk->trees);
fsnotify_put_mark(chunk_entry);
list=%d
rlist)
q)
list_entry(p,
p->next;
*mnt,
&tree_list)
kern_path(tree->pathname,
&tree->chunks,
trim_marked(tree);
AUDIT_FILTER_EXIT
Audit_equal
*mnt;
list_add(&tree->list,
&path2);
cookie)
audit_panic("cannot
(htab->n_buckets
htab->n_buckets;
htab_map_hash(key,
(!l)
(map_flags
BPF_EXIST)
spin_unlock_irqrestore(&htab->lock,
alias
'%')
mod[fmt_cnt]++;
trace_printk_init_buffers();
profiler
"trace_stat.h"
love
unloaded,
slower,
p--;
TASK-PID
percent;
p1;
p2;
worse
.stat_start
.stat_next
.stat_headers
.stat_show
Lai
Jiangshan
start_index
Optimistic
<linux/osq_lock.h>
#readers
#active
ACTIVE_WRITE_BIAS
Readers
CONFIG_RWSEM_SPIN_ON_OWNER
'active
part'
adjustment;
list_entry(sem->wait_list.next,
RWSEM_WAITING_FOR_WRITE)
list_entry(next,
pairing
wake_up_process(tsk);
raw_spin_lock_irq(&sem->wait_lock);
RWSEM_WAITING_BIAS
raw_spin_unlock_irq(&sem->wait_lock);
__set_task_state(tsk,
(!owner)
(!owner
taken;
pm_mutex
freezing,
try_to_freeze()
spin_unlock_irqrestore(&freezer_lock,
(!(p->flags
notices
obj->mod;
klp_find_arg
args->addr
(%lu
klp_verify_args
*obj,
disregard
func->old_name,
reloc->name,
fops);
func->old_addr,
list_del_rcu(&func->stack_node);
func->state
KLP_DISABLED))
(!ops)
FTRACE_OPS_FL_SAVE_REGS
KLP_ENABLED;
obj->state
klp_disable_object(obj);
patch->mod->name);
patch->state
(!klp_is_patch_registered(patch))
unregister:
*patch;
PAGE_SIZE-1,
limit;
Initializes
pmod->name,
unloading
MODULE_STATE_COMING
off.\n");
understanding,
source:
gcov_info.
*gcov_info_filename(struct
gcov_info_version(struct
*gcov_info_next(struct
gcov_info_link(struct
gcov_info_unlink(struct
gcov_action
Iterator
*gcov_iter_new(struct
gcov_iter_free(struct
gcov_iter_start(struct
gcov_iter_next(struct
gcov_iter_write(struct
*gcov_iter_get_info(struct
gcov_info_reset(struct
gcov_info_is_compatible(struct
*info1,
gcov_info_add(struct
*gcov_info_dup(struct
gcov_info_free(struct
OBJ_TREE,
<linux/rtc.h>
FIXME
suspend_test_finish(const
%d.%03d
issues.
alarms
%d\n";
printk(info_test,
PM_SUSPEND_MIN;
(dev)
GCOV_COUNTERS
comdat
instrumented
Information
version;
info->version;
.gcno
Doesn't
result++;
*ci_ptr;
ci_ptr
info->functions[fi_idx]->ctrs;
ct_idx))
@dest:
sci_ptr->num;
dup->filename
dup->functions
%NULL,
iterated
gcov_iter_get_info
seq_write(seq,
up_read(&clk->rwsem);
vma);
.mmap
.clock_set
<linux/netlink.h>
watch.
audit_parent,
long)-1;
respresentation.
krule->inode_f
"auid=%u
watches
&parent->watches,
wlist)
inode-based
ino
watch,
list_for_each_entry_safe(r,
container_of(r,
audit_watch_group);
audit_filter_mutex.
krule->watch;
dname,
*array;
(!array)
array->elem_size
*)key;
array->map.max_entries)
(u32
_LINUX_KERNEL_TRACE_H
CONFIG_FTRACE_SYSCALLS
regfn)
TRACE_FLAG_NEED_RESCHED
policy;
max_buffer
reached,
traces.
function_enabled;
boolean
(echo
(*init)(struct
*m);
*flags;
recursion:
occurs,
recursion.
checks,
*ent_cpu,
trace_empty(struct
ftrace_trace_stack(struct
ftrace_trace_stack_regs(struct
ftrace_trace_userstack(struct
__trace_stack(struct
ftrace_update_tot_cnt;
selftests
nsec);
0x4
TRACE_GRAPH_PRINT_ABS_TIME
print_graph_function_flags(struct
ftrace_graph_count;
ftrace_graph_notrace_count;
ftrace_create_function_files(struct
ftrace_destroy_function_files(struct
ftrace_init_global_array_ops(struct
ftrace_reset_array_ops(struct
defined(CONFIG_DYNAMIC_FTRACE)
cont;
(parser->idx
parser->cont
trace_iterator_flags
TRACE_ITER_OVERWRITE
TRACE_ITER_FUNCTION
races,
is_signed;
'trigger'
@trigger_type
triggers,
user.
*cmd_ops,
enabled);
FTRACE_ENTRY(call,
trace_event_enum_update(struct
**map,
*ag)
ag
autogroup_kref_put(ag);
nice)
MIN_NICE
nice);
kernel/sched/core.c
schedule_timeout()
Robert
Gregory
<linux/cpuset.h>
Debugging:
sysctl_sched_features
(strcmp(cmp,
(neg)
single_open(filp,
__acquires(rq->lock)
__hrtimer_start_range_ns(timer,
rq->hrtick_csd_pending
hrtick_start(struct
init_hrtick(void)
sense.
max_t(u64,
HRTIMER_MODE_REL_PINNED,
__val
cpu_of(rq);
pinned)
IRQ.
FIFO
SCHED_RR)
sched_avg_update(struct
optimising
__iter_div_u64_rem()
&parent->children,
up:
parent->parent;
up;
(p->policy
sched_info_queued(rq,
irq_time
timestamp,
(task_has_rt_policy(p))
p->normal_prio
normal_prio(p);
unchanged.
p->sched_class)
schedule.
new_cpu)
migration_swap_arg
nonzero,
queued;
relax
p->nvcsw
(unlikely(running))
done!
Cause
->cpus_allowed
possible:
do_set_cpus_allowed(p,
PF_WQ_WORKER)
ttwu_do_wakeup(rq,
Its
__task_rq_lock(p);
traditionally
per_cpu(sd_llc_id,
@p's
state))
->state
referencing
this_rq()
dl_se->dl_deadline
dl_se->dl_period
dl_se->flags
too:
numabalancing_enabled;
numabalancing_enabled
(task_has_dl_policy(p)
p->rt_priority
set_load_weight(p);
p->sched_reset_on_fork
(period
*dl_bw_of(int
yes,
(dl_policy(policy)
__dl_add(dl_b,
prepare_task_switch
flipped
this_rq
tsk->state
twice.
mmdrop(mm);
function-return
oldmm;
_THIS_IP_);
footprint.
migration_cpu_stop,
->on_cpu
project
frequency.
rq->last_sched_tick
now))
Spinlock
switch_count
to_wakeup
(likely(prev
sched_preempt_enable_no_resched();
CONFIG_CONTEXT_TRACKING
exception_enter();
__preempt_count_add(PREEMPT_ACTIVE);
__preempt_count_sub(PREEMPT_ACTIVE);
user_exit()
enqueue_flag
oldprio
p->sched_class;
p->sched_class->set_curr_task(rq);
check_class_changed(rq,
prev_class,
SCHED_RR:
side.
p->policy;
attr->sched_deadline
zero).
reset_on_fork;
SCHED_RR
change;
deboost
.sched_policy
hack.
capability.
*uattr,
nice.
sizeof(*attr))
-EFBIG;
(!alloc_cpumask_var(&new_mask,
out_free_new_mask;
free_cpumask_var(new_mask);
cpumask_and(mask,
cpumask_size());
low-level
set_current_state(TASK_RUNNING);
yielded
p_rq);
debug_show_all_locks();
problem,
*trial)
Old
deallocate
cpu_active_mask))
dest_cpu);
dead_cpu)
rq->stop;
pick_next_task()
entry->procname
entry->mode
rq->online
(rq->rd)
subsystem,
CPU_ONLINE,
sched_domains_tmpmask;
Following
SD_BALANCE_FORK
SD_SHARE_CPUCAPACITY
pflags
*rd)
*first;
(!sg)
sd->parent)
'cpu'
tmp->parent;
parent))
SD_PREFER_SIBLING;
rd);
sched_domain_span(sd);
(!cpumask_test_cpu(i,
canonical
(child)
*per_cpu_ptr(sdd->sg,
correctly,
sdd,
for_each_cpu(j,
cpu_capacity
numa_topology_type
sd_weight,
sd->cache_nice_tries
sd->busy_idx
&tl->data;
distance)
affects
hops
sched_numa_topology_type
curr_distance
strong
A.
'level'
excluding
Here,
dangerous
kzalloc(nr_node_ids
.mask
cpu_to_node(j));
cpu_map,
ndoms_cur;
'doms_cur'
ndoms)
*doms;
(!doms)
Detach
"default"
SD_ATTR_INIT;
doms_new[]
ndoms_new
j))
suspend/resume,
CPU_ONLINE_FROZEN:
partition_sched_domains(1,
dl_bw_of(cpu);
non_isolated_cpus);
global_rt_period(),
filesystem,
(i.e
be,
preempt_offset)
default,
debug_show_held_locks(current);
defined(CONFIG_IA64)
rt_schedulable_data
ktime_to_ns(tg->rt_bandwidth.rt_period);
d->tg)
rt_runtime);
undo;
sched_rr_timeslice
sched_move_task(task);
copy_process()
poke
(!(task->flags
quota;
.read_s64
.write_s64
%d:\n",
wakeup_graph_entry(struct
wakeup_graph_return(struct
trace_opts[]
tracers.
**data,
wakeup_trace;
(!func_prolog_preempt_disable(tr,
&pc))
!(trace_flags
*tracer
TRACE_FN
print_graph_function_flags(iter,
wakeup_print_header(struct
print_graph_headers_flags(s,
wakeup_cpu
arch_spin_lock(&wakeup_lock);
wakeup_cpu);
CALLER_ADDR0,
data->preempt_timestamp;
arch_spin_unlock(&wakeup_lock);
tracing_record_cmdline(current);
(!tracer_enabled
data->preempt_timestamp
kernel_sched_switch\n");
wakeup_reset(tr);
(wakeup_busy)
__wakeup_tracer_init(tr);
TRACE_ITER_LATENCY_FMT;
wakeup_tracer_reset,
wakeup_tracer_start,
wakeup_tracer_stop,
wakeup_print_header,
wakeup_print_line,
wakeup_set_flag,
wakeup_flag_changed,
trace_selftest_startup_wakeup,
wakeup_trace_open,
wakeup_trace_close,
Futex
futexes:
futex_offset);
compat_robust_list_head
exists:
pending)
(unlikely(len
val3)
FUTEX_CMD_MASK;
FUTEX_WAIT_BITSET
(compat_get_timespec(&ts,
&t;
FUTEX_CMP_REQUEUE_PI
rt_rq->overloaded
rt_rq->push_flags
rt_rq->rt_queued
rt);
free_rt_sched_group(struct
alloc_rt_sched_group(struct
rq_of_rt_se(rt_se);
pull_rt_task(struct
rq->rd->rto_mask);
RQ
&rq->rt.pushable_tasks);
p->prio);
rq->rt.highest_prio.next
(rt_rq->rt_nr_running)
(on_rt_rq(rt_se))
*sched_rt_period_mask(void)
sched_rt_period_rt_rq(rt_b,
rt_rq)
(rt_rq->rt_runtime
raw_spin_unlock(&iter->rt_runtime_lock);
more;
rt_rq_iter_t
(rt_rq->rt_time
lest
(rt_rq->rt_throttled)
rt_rq->highest_prio.curr;
prev_prio);
rt_se_prio(rt_se);
array->queue
(head)
enqueue_pushable_task(rq,
requeue_task_rt(rq,
cache,
gave
perhaps
rq->curr->prio)
plist_head
lowest_mask))
task->prio)
lowest_rq);
rq->rt.push_cpu
rq->cpu)
broadcast);
functional
clock_event_state
tick_device_uses_broadcast(struct
tick_install_broadcast_device(struct
tick_is_broadcast_device(struct
tick_shutdown_broadcast(unsigned
tick_set_periodic_handler(struct
tick_broadcast_update_freq(struct
broadcast)
tick_handle_periodic;
tick_setup_oneshot(struct
tick_program_event(ktime_t
*));
tick_check_oneshot_change(int
tick_broadcast_setup_oneshot(struct
tick_shutdown_broadcast_oneshot(unsigned
(!ns)
utsname
KDB_STATE_SET(DOING_SS);
bp->bph_length
((argc
(strncasecmp(argv[nextarg],
(!bp->bp_type)
(bp->bp_enabled)
(bpno
bp->bp_free
(strcmp(argv[1],
kdbgetularg(argv[1],
(bp->bp_addr
bp->bp_enabled
kdb_printf("Breakpoint
KDB_STATE_CLEAR(SSBPT);
kdb_bp,
kdb_bc,
"<bpnum>",
Breakpoint",
KDB_ENABLE_FLOW_CTRL);
"Disable
*brl_options)
**str,
_braille_register_console(struct
*console,
*c);
_braille_unregister_console(struct
held).
tt
data->ops->func(data);
event_file
strsep(&next,
buff,
filter_str
wrapped
(long)data;
(WARN_ON_ONCE(data->ref
data->ref--;
(!data->ref)
trace_event_trigger_enable_disable(file,
data->ops->free(data->ops,
TRIGGER_COND
@glob:
associate
(tracing_is_on())
register_trigger,
"stacktrace",
FTRACE_EVENT_FL_SOFT_DISABLED))
SOFT_MODE
kfree(enable_data);
(!trigger)
strcmp(cmd,
ENABLE_EVENT_STR)
spaces,
checksum;
@function:
function;
(iter)
iter->type
9;
sizeof(data));
Molnar,
open-addressed
hashtables
(prof_buffer)
blocking_notifier_chain_register(
particularly
distinct
meant
finite
collision
hits[i].pc
*__pc,
nr_hits)
hits[i
buffer:
cpu)[1])
cpu)[0])
cpu)[1]);
cpumask_test_cpu(smp_processor_id(),
(prof_len+1)*sizeof(unsigned
cpu_notifier_register_begin();
happened.
last_jiffies_update;
tick_period.tv64)
jiffies_lock
last_jiffies_update
(unlikely(delta.tv64
ktime_divns(delta,
incr);
update_wall_time();
themself
TICK_DO_TIMER_NONE)
login
(ts->tick_stopped)
"perf
irq_work,
tick_nohz_full_kick();
timers,
cpumask\n");
tick_nohz_full_mask);
CPUs:
tick_nohz_enabled
tick_nohz_active
*last_update_time)
(ts->idle_active)
ts->idle_entrytime);
(last_update_time)
(ts->idle_active
rcu_delta_jiffies;
read_seqbegin(&jiffies_lock);
last_jiffies
(read_seqretry(&jiffies_lock,
last_jiffies;
retrieved
NOHZ_MODE_HIGHRES)
hrtimer_cancel(&ts->sched_timer);
HRTIMER_MODE_ABS_PINNED);
(hrtimer_active(&ts->sched_timer))
ts->nohz_mode
hrtimer_set_expires(&ts->sched_timer,
hrtimer_forward(&ts->sched_timer,
slept
Offset
num_possible_cpus());
tick_cancel_sched_timer(int
%lu",
itererator
n--;
Christoph
<linux/idr.h>
<linux/hashtable.h>
intensive
pool->attrs
(min
rules.
P:
writes.
WR:
MD:
nr_idle
pwq;
two's
max_active;
wq->maydays
flush.
requesting
lockdep_map;
keyed
workqueue_sysfs_unregister(struct
(pool)
guaranteeing
if/else
clause
for_each_pool(pool,
lockdep_assert_held(&pool->attach_mutex);
fixup_init
debug_object_init(work,
fixup_free
.debug_hint
.fixup_init
.fixup_free
[0,
successfully,
color)
pwq.
canceled.
canceled,
mb
wq_pool_mutex,
WORK_OFFQ_POOL_SHIFT;
nr_running,
manager.
Safe
WORKER_NOT_RUNNING)
pool->cpu
manipulating
@worker->flags
(oflags
unrelated
@pwq.
destruction.
WQ_UNBOUND)))
warning,
subclass
lock/unlock
%NULL)
(pwq)
spin_lock_irq(&pwq->pool->lock);
put_pwq(pwq);
spin_unlock_irq(&pwq->pool->lock);
work_struct,
flushing.
(pwq->nr_active
claimed
moment,
canceling
*dwork
(!test_and_set_bit(WORK_STRUCT_PENDING_BIT,
(!pool)
spin_lock(&pool->lock);
spin_unlock(&pool->lock);
get_pwq(pwq);
processed.
Grabbing
per_cpu_ptr(wq->cpu_pwqs,
(wq->flags
spin_unlock(&pwq->pool->lock);
irqsafe
&dwork->work);
*dwork,
add_timer(timer);
delay);
LOCKING:
POOL_DISASSOCIATED)
worker_clr_flags(worker,
(worker)
(!worker)
(pool->cpu
worker->task->flags
expires))
(need_to_create_worker(pool))
@pool->lock
wins
worker->current_work
rescuer->task->flags
asking
pool_workqueue,
(get_work_pwq(work)
items,
inbetween.
@barr
color,
no-op
@flush_color
negative,
(flush_color
next_color;
wq->work_color;
this_flusher.flush_color);
flush,
wq->flush_color
pwqs.
short.
verifying
cwt_wait
canceled
cancel_delayed_work_sync()
Delayed
*dwork)
schedule_work_on(cpu,
(must
pool->node
through,
put_unbound_pool(pool);
system_wq
lockdep_assert_held(&wq->mutex);
pwq->max_active
alloced
pwq);
cpumask);
old_pwq
new_attrs);
alloc_unbound_pwq(wq,
numa_pwq_tbl_install(wq,
exclusion.
cpumask))
pr_warn("workqueue:
max_active,
err_destroy;
Safely
congested
unreliable
@log_lvl:
pr_cont("%s
difficult
Sched
wake-ups
worker_flags
REBOUND
premature
work_for_cpu
*arg;
workqueues.
*wq_dev
&val)
wq_sysfs_prep_attrs(wq);
method.
Disallow
Suppress
enforced
these,
&((*nl)->next);
n->next
rcu_assign_pointer(*nl,
nb
nb->notifier_call(nb,
locks).
down_write(&nh->rwsem);
up_write(&nh->rwsem);
racy
leak.
fentry
Depth
stack_dump_index[i]
tracer_frame;
this_size;
arch_spin_lock(&max_stack_lock);
(!found)
arch_spin_unlock(&max_stack_lock);
cpu)--;
tracer,
ftrace_regex_release,
register_ftrace_function(&trace_ops);
implementing
<linux/io.h>
ct->regs.mask);
ct->regs.ack);
@on:
(on)
irq_base;
num_ct
gc
irq_gc_flags
@clr:
irq_domain_chip_generic
Hardware
dgc->irqs_per_chip;
&ct->chip;
chip,
Bitmask
gc->irq_base;
list_for_each_entry(gc,
&gc_list,
irq_gc_get_irq_data(gc);
irq_domain_check_hierarchy(struct
*of_node,
revmap
controller's
first_irq
irq_descs
irq_domain_associate_many(domain,
life
"error:
virq))
firmware
mapped.
pr_info("%s
hwirq_base,
irq_base
of_node_to_nid(domain->of_node));
pr_debug("->
irq_find_mapping(domain,
IRQ_TYPE_NONE;
irq_data->domain;
irq_domain_get_irq_data(domain,
%-16s
(chip
xlate
specifier
translation
*ctrlr,
*intspec,
intsize,
*out_hwirq,
*out_type)
(WARN_ON(intsize
*out_hwirq
intspec[0];
*out_type
variants
Parent
@size
domain->flags
(irq_data)
irq_data->parent_data;
domain->parent;
irq_domain_free_irq_data(virq,
domain)
@chip:
*chip,
irq_data->chip
step,
recursive.
spinning.
"no
encoded
optimistic_spin_queue
encode_cpu(smp_processor_id());
unlock()/unqueue()
WRITE_ONCE(prev->next,
moment
Mathieu
Desnoyers
tracepoints_mutex
tp_probes
kmalloc(count
(nr_probes
tp_func->func
tp_func->data)
*funcs
(IS_ERR(old))
PTR_ERR(old);
unregistering
"struct
list_for_each_entry(tp_mod,
&tracepoint_module_list,
MODULE_STATE_COMING,
begin;
(trace_module_has_bad_taint(mod))
notifier\n");
nblocks;
nblocks
*b;
gid);
subscription
CAP_SETGID)
Masami
Hiramatsu
tk->symbol
':';
update_symbol_cache(struct
free_symbol_cache(struct
NOKPROBE_SYMBOL(FETCH_FUNC_NAME(memory,
*symbol,
(is_return)
tk;
list_for_each_entry(tk,
(trace_probe_is_enabled(&tk->tp))
Cleanup
Delete
args:
free_trace_kprobe(tk);
&tk->tp.args[i];
entry->func
ri,
*tp;
(!seq_print_ip_sym(s,
tp->args[i].name,
(tracing_is_disabled())
6);
(autosleep_state
tight
__TRACE_STAT_H
statistic
sorting
retrieval
strlen(str);
TRACE_SEQ_BUF_LEFT(s))
Power
exports
basename
*seq;
simple.
(node)
remove_node(node);
*target;
*filename,
ext->ext);
External
Needed
*/;
node->links
node->num_loaded
node->loaded_info
kfree(node);
pr_warn("out
kfree(node->loaded_info);
noop_llseek,
human
Top
MAXLR
raw_spin_lock_irqsave(&latency_lock,
raw_spin_unlock_irqrestore(&latency_lock,
q,
q++)
(caused
(bt
*offs)
kernels.
<linux/timex.h>
*cts)
*rmtp;
HRTIMER_MODE_REL,
CLOCK_MONOTONIC);
supply
restart->nanosleep.compat_rmtp
*o,
it)
sizeof(tmp)))
compat_rlimit
&rlim->rlim_cur)
&rlim->rlim_max))
(r.rlim_cur
r.rlim_cur
(r.rlim_max
r.rlim_max
ru;
timer_event_spec,
tp))
txc;
&txc);
((err
BITS_PER_COMPAT_LONG);
<linux/writeback.h>
Pipe
enums
trace_enum_maps
ending
default_bootup_tracer
allocate_snapshot
expanded
global_trace
*this_tr)
ring_buffer_discard_commit(buffer,
irqsoff
accurate.
trace_types
garbage
concurrently.
(tr->trace_buffer.buffer)
@ip:
(unlikely(tracing_selftest_running
tracing_disabled))
with:
Basically
permanent
here!
cpu_id);
&tr->trace_buffer,
max_tr
alloc_snapshot(tr);
buf_size;
kmalloc(size,
parser->size
get_user(ch,
ubuf++);
TODO
&tr->max_buffer;
per_cpu_ptr(trace_buf->data,
tsk->pid;
arch_spin_lock(&tr->max_lock);
arch_spin_unlock(&tr->max_lock);
full)
corrupted
"PASSED\n");
pr_info("Tracer
trace_types;
tracing_selftest_disabled
"Disabling
ring_buffer_reset_cpu(buffer,
savedcmd
pid.
entry->flags
*current_rb
trace_buffer_lock_reserve(*current_rb,
anywhere.
(unlikely(__this_cpu_read(ftrace_cpu_disabled)))
entry->parent_ip
trace.skip
use_stack
save_stack_trace(&trace);
ups.
(unlikely(in_nmi()))
pause_graph_tracing();
unpause_graph_tracing();
lost_events);
ring_buffer_event_length(event);
*ent;
buf_iter
((event
*s_start(struct
iter->leftover
s_next(m,
trace_event_read_lock();
trace_event_read_unlock();
per_cpu_ptr(buf->data,
_-----=>
_----=>
_---=>
_--=>
||||
sym_flags
TRACE_ITER_SYM_MASK);
"---------------------------------\n");
buf->cpu,
TRACE_FILE_LAT_FMT)
entry->type);
(iter->ent->type
TRACE_ITER_PRINTK_MSGONLY)
TRACE_ITER_VERBOSE))
BE
print_trace_line(iter);
succeeded.
s_start,
s_next,
snapshot;
(trace_clocks[tr->clock_id].in_ns)
"snapshot"
kfree(iter->trace);
__trace_array_put(tr);
t_next(m,
trace_options[i];
opts
neg);
events\n"
suffix
CONFIG_STACK_TRACER
traced\n"
Directory
enable\t\t-
events/block/block_unplug/trigger\n"
s)
(!val
(skip
mutex_lock(&trace_enum_mutex);
trace_enum_maps;
mutex_unlock(&trace_enum_mutex);
**map;
cpu_id)
(!tr->dir)
nop_trace
kzalloc(sizeof(*iter),
wait_on_pipe(iter,
mutex_lock(&iter->mutex);
Consumer
sret;
(sret
TRACE_TYPE_NO_CONSUME)
*spd,
pipe_buf_operations
.can_merge
.confirm
generic_pipe_buf_confirm,
.steal
generic_pipe_buf_steal,
rem,
partial_page
.partial
.nr_pages_max
PIPE_DEF_BUFFERS,
.spd_release
(splice_grow_spd(pipe,
&spd))
spd.nr_pages
splice_to_pipe(pipe,
&spd);
splice_shrink_spd(&spd);
%lu)\n",
r,
straight
_THIS_IP_;
tr->clock_id
kfree(m);
info->spare
info->read
pipe_buffer
*ref
ring_buffer_free_read_page(ref->buffer,
ref->page);
usec_rem);
"now
"%ld
*)ip);
":unlimited\n");
":count=%ld\n",
unregister_ftrace_function_probe_func(glob+1,
register_ftrace_function_probe(glob,
*t_options;
t_options
trace_options_init_dentry(tr);
(!t_options)
cnt++)
tracefs_create_dir(name,
mounted
(!type)
(ftrace_dump_on_oops)
paranoid.
crashes.
RB_FL_OVERWRITE);
transitional
(!rt_mutex_has_waiters(lock))
clear_rt_mutex_waiters(lock);
mark_rt_mutex_waiters(lock);
lock->waiters_leftmost
chwalk)
@top_task
proxy
task->pi_blocked_on;
waiter->lock;
unlock(task->pi_lock);
[11]
[12]
[13]
*top_waiter
(!next_lock)
rt_mutex_dequeue(lock,
became
rt_mutex_enqueue_pi(task,
waiters,
rt_mutex_top_waiter(lock));
rt_mutex_set_owner(lock,
*next_lock;
chain_walk
optimization.
chwalk))
rt_mutex_adjust_prio_chain()!
get_task_struct(owner);
next_lock,
(try_to_take_rt_mutex(lock,
-ETIMEDOUT;
remove_waiter(lock,
try_to_take_rt_mutex()
fixup_rt_mutex_waiters(lock);
try-lock
necessary:
(likely(rt_mutex_cmpxchg(lock,
current)))
serializing
proxy_owner);
-EDEADLK
@to:
@mult:
seqcount
clock_data
seqcount_t
rate;
.mult
cyc,
cyc
*only*
cd.actual_read_sched_clock();
update_sched_clock();
nanosecs
r_unit
%pF
read);
&(x),
sizeof(x))
cpu_idle_force_poll
re-enabling
measuring
stop_critical_timings();
framework
dev))
use_default;
frozen,
bypass
cpuidle
governor
deepest
entered_state
exit_idle;
start_critical_timings();
schedule_preempt_disabled();
canary
Runtime
<linux/kmemcheck.h>
arch_spin_lock(&lockdep_lock);
(!debug_locks)
DEBUG_LOCKS_WARN_ON(1);
nr_list_entries;
Mutex
simplifies
LOCKSTAT_POINTS;
lock_classes];
get_lock_stats(hlock_class(hlock));
put_lock_stats(stats);
hash-table
Example
!strcmp(class->name,
validator.\n");
MAX_STACK_TRACE_ENTRIES
'+';
'-';
__get_key_name(class->key,
printk("#%d",
class->subclass);
__print_lock_name(class);
lock->name;
#%d:
generation
hash-table,
Static
lock->key
subclass;
look_up_lock_class(lock,
printk("INFO:
safe:
graph_lock
look_up_lock_class()
shortest
lockdep_dependency_gen_id;
CQ_MASK;
nr_list_entries);
(*match)(struct
forwards-direction
depth);
scenario,
__print_lock_name(target);
this:\n\n");
*this)
lockdep_count_forward_deps(struct
lockdep_count_backward_deps(struct
target_entry);
subgraph
at:\n");
depth--;
__print_lock_name(safe_class);
task_pid_nr(curr),
%s-irq-safe
%s-irq-unsafe
hlock_class(prev),
bit_backwards,
bit_forwards,
2]
USED_IN
hardirq-safe
<prev>
<next>
[==
validations
forwards
variable,
L1
L2
list_for_each_entry(entry,
graph_lock();
context's
nr_chain_hlocks;
chain_hlocks[chain->base
(very_verbose(class))
%016Lx
MAX_LOCKDEP_CHAINS
chain->depth
chain_head,
hlock))
curr->lockdep_depth,
print_lock(this);
new_bit);
excl_bit
enabled:
(disabled
(!(gfp_mask
GFP_FS
lock->name
register_lock_class(lock,
Nested
*nest_lock,
chain_head
(!class)
lockstat_clock();
unlock.
MAX_LOCK_DEPTH);
detected!
print_unlock_imbalance_bug(curr,
sites
hlock->prev_chain_key;
apart
unlock,
module:
CLASSHASH_SIZE;
zap_class(class);
mem_from,
mem_len,
mem_from
%s/%d
careful.
%s!\n",
illegally
count:
print_symbol("%s\n",
Performance
Mackerras,
<paulus@au1.ibm.com>
licensing
"internal.h"
publish
handle->wakeup
mmap()
STORE
$data
->data_tail
WMB
(unlikely(head
(event->attr.sample_id_all)
page_order(rb);
page_shift)
handle->addr
event->id;
perf_data_size(rb);
output_event
(aux_head
ring_buffer_put(rb);
posting
page->mapping
pgoff_t
pgoff,
max_order
contexts,
safely.
CONFIG_PERF_USE_VMALLOC
Back
perf_mmap()
rb_free(struct
pgoff
ring_buffer,
rb->user_page;
irq_pm_check_wakeup(struct
IRQD_WAKEUP_ARMED);
desc->depth++;
irq_disable(desc);
irq_pm_install_action(struct
IRQF_NO_SUSPEND)
irq_pm_remove_action(struct
synchronize_irq().
unused,
pm
__enable_irq(desc,
<linux/elf.h>
don't,
reserving
Posix
Timer
SIGEV_THREAD_ID
ENOTSUP
following:
1.)
&timr->it.real.timer;
++timr->it_requeue_pending;
->sigq
inconsistency
clock_id
"POSIX
clock_id);
*new_timer)
new_timer_id;
which_clock;
cease
lookup.
(!timr)
overrun.
new_setting,
rtn
spin_unlock(&current->sighand->siglock);
nanosleep
*restart_block)
i_next
panic,
tnt
TAINT_WARN,
'R'
'M'
'C'
@flag:
OK.
msecs)
mdelay(1);
pause_on_oops_flag
oopses
side-effect
pr_warn("WARNING:
%s:%d
warn_slowpath_common(file,
__builtin_return_address(0),
irq_chip_write_msi_msg(irq_data,
&msg);
domain->host_data;
info->ops;
.alloc
(info->flags
for_each_msi_entry(desc,
user-level
1997
remount
bunch
better.
<asm/div64.h>
100);
*acct
mutex_unlock(&acct->lock);
comp2_t
(24
(exp
pacct_struct
run_time
etime
(u16)
&pos);
vsize
PF_SIGNALED)
0x1,
EVENT_PINNED,
unpriv
perf_sample_period_ns
update_perf_cpu_limits();
local_samples_len/NR_ACCUMULATED_SAMPLES;
atomic64_t
CONFIG_CGROUP_PERF
cgroups.
event->cgrp
ctx->timestamp;
this_cpu_ptr(pmu->pmu_cpu_context);
(cpuctx->unique_pmu
perf_cgroup_events
perf_pmu_disable(cpuctx->ctx.pmu);
cpuctx->cgrp
ctxsw
cpu_ctx_sched_in(cpuctx,
perf_pmu_enable(cpuctx->ctx.pmu);
cgrp1
cgrp2
cgrp2)
perf_cgroup_switch(task,
(group_leader
perf_detach_cgroup(event);
event->shadow_ctx_time
event->tstamp_enabled
(sub->state
smp_processor_id()))
perf_sw_context)
(timer
perf_event_context::mutex
concerned
ctx->lock,
ctx->generation++;
cope
raw_spin_lock_irqsave(&ctx->lock,
ctx->timestamp
&leader->sibling_list,
PERF_FORMAT_GROUP)
PERF_SAMPLE_IP)
PERF_SAMPLE_PERIOD)
PERF_SAMPLE_IDENTIFIER)
PERF_SAMPLE_ID)
event->group_leader,
&group_leader->sibling_list,
sibling,
ctx->orphans_remove_sched
event->cpu
perf_pmu_disable(event->pmu);
cpuctx->exclusive
perf_pmu_enable(event->pmu);
&group_event->sibling_list,
perf_group_detach(event);
list_del_event(event,
ctx->is_active
cpu_function_call(event->cpu,
Reload
perf_event_context_sched_out().
(ctx->task
group_sched_out(event,
Strictly
update_context_time()
cleaner
perf_log_throttle(event,
perf_cpu_hrtimer_restart(cpuctx);
ctx_sched_in(ctx,
task_ctx);
task_ctx
task_ctx,
perf_event_sched_in(cpuctx,
Enabling
__perf_event_mark_enabled(event);
&ctx->pinned_groups,
&ctx->flexible_groups,
ctx1
event->pmu->read(event);
task->perf_event_ctxp[ctxn];
ctx_sched_out(ctx,
perf_pmu_enable(pmu);
EVENT_FLEXIBLE);
nsec,
frequency_fls
30;
(count_fls
dividend
event->pmu->stop(event,
event->count
rotate
*clone_ctx
(!ctx
clone_ctx
(clone_ctx)
put_ctx(clone_ctx);
(e.g.,
perf_event_count(event);
(ctxn
task_ctx_data
perf_event_free_filter(struct
perf_event_free_bpf_prog(struct
static_key_slow_dec_deferred(&perf_sched_events);
(event->attr.mmap
event->attr.mmap_data)
(!(pmu->capabilities
PERF_PMU_CAP_EXCLUSIVE))
(!event->parent)
(event->rb)
refcount,
exposed
annotation
WARN_ON_ONCE(ctx->parent_ctx);
perf_remove_from_context(event,
mutex_lock(&parent_event->child_mutex);
mutex_unlock(&parent_event->child_mutex);
mutex_lock(&event->child_mutex);
atomic64_read(&event->child_total_time_enabled);
atomic64_read(&event->child_total_time_running);
list_for_each_entry(child,
mutex_unlock(&event->child_mutex);
sizeof(u64)))
WARN_ON_ONCE(event->ctx->parent_ctx);
event->rb;
list_for_each_entry(sibling,
(active)
perf_event_set_filter(struct
*arg);
perf_event_set_bpf_prog(struct
perf_event_set_output(event,
mutex_lock(&current->perf_event_mutex);
owner_entry)
mutex_unlock(&current->perf_event_mutex);
(event->hw.state
PERF_HES_STOPPED)
ctx_time
vma->vm_file->private_data;
rb);
rb,
(rb_has_aux(rb)
redirect
user_extra
vma_size
vma->vm_flags
event->pending_kill
perf_swevent_get_recursion_context();
perf_swevent_put_recursion_context(rctx);
perf_guest_info_callbacks
(!regs)
customize
(data->callchain)
PERF_SAMPLE_BRANCH_STACK)
PERF_SAMPLE_REGS_USER)
PERF_SAMPLE_REGS_INTR)
fork/exit
perf_task_event
event_id;
.event_id
.time
mmap
vma->vm_file;
(vma->vm_start
online;
&event->count);
perf_swevent_overflow(event,
event->attr.config;
perf_swevent_event(event,
rctx)
perf_sample_data_init(&data,
(is_sampling_event(event))
hlist);
PERF_TYPE_SOFTWARE)
.capabilities
PERF_PMU_CAP_NO_NMI,
refcount.
dev_get_drvdata(dev);
pmu->type);
free_dev;
put_device(pmu->dev);
pmu->type
event->pmu
static_key_slow_inc(&perf_sched_events.key);
account_event_cpu(event,
*parent_event,
overflow_handler,
nmi_safe
0};
event_fd;
group_leader,
group_leader)
&per_cpu_ptr(pmu->pmu_cpu_context,
*child_event,
notified,
*child_ctx,
child_event->state
ctxn);
*parent_ctx,
allocations,
cleanups
<linux/magic.h>
lock(hash_bucket(futex));
unlock(hash_bucket(futex));
-.
Y
bitset
plist_node
FUTEX_KEY_INIT,
waiters;
Reflects
spinlock.
rw)
key->both.offset
sizeof(u32))
address;
get_futex_key_refs(key);
&page);
truncated
newval);
(!pi_state)
pi_mutex
current->pi_state_cache
raw_spin_lock_irq(&pi_state->owner->pi_lock);
raw_spin_unlock_irq(&pi_state->owner->pi_lock);
cleans
raw_spin_lock_irq(&curr->pi_lock);
(!list_empty(head))
hash_futex(&key);
raw_spin_unlock_irq(&curr->pi_lock);
WARN_ON(list_empty(&pi_state->list));
exit_pi_state_list()
Owner
**ps)
(!pi_state->owner)
Bail
cleanup:
WARN_ON(!list_empty(&pi_state->list));
list_add(&pi_state->list,
futex_top_waiter(hb,
(match)
futex_lock_pi_atomic()
*match;
FUTEX_TID_MASK)
exists,
FUTEX_OWNER_DIED;
vpid;
FUTEX_WAITERS;
plist_del(&q->list,
&hb->chain);
hb_waiters_dec(hb);
lock_ptr
uninitialized_var(curval),
fiddled
(pi_state->owner
(hb1
nr_wake,
(!bitset)
out_put_key;
(match_futex
(&this->key,
(this->pi_state
put_futex_key(&key);
*uaddr2,
&key1,
get_futex_key(uaddr2,
out_put_keys:
@key2:
key2,
(FLAGS_SHARED,
keys:
(requeue_pi
drop_count++;
-EFAULT:
-EAGAIN:
rt_mutex.
unqueue_me()
fault_in_user_writeable(uaddr);
late.
-EWOULDBLOCK
hash-bucket
*to
futex_q_init;
&timeout;
hrtimer_init_on_stack(&to->timer,
hrtimer_init_sleeper(to,
(to)
destroy_hrtimer_on_stack(&to->timer);
(time)
spin_lock(q.lock_ptr);
Unqueue
-EFAULT)
atomicity
was.
val3
FUTEX_BITSET_MATCH_ANY;
futex_requeue(uaddr,
implementation,
succeed,
Acquires
sem->count++;
semaphore_waiter
__set_task_state(task,
MAX_SCHEDULE_TIMEOUT);
TASK_KILLABLE,
Creation
Result
now?
kthread_stop().
kthread_should_stop()
*kthread
kthread's
kfree(create);
__set_current_state(TASK_UNINTERRUPTIBLE);
wake_up_process()
spin_lock(&kthread_create_lock);
spin_unlock(&kthread_create_lock);
killer
kthreadd
bind
Format
(IS_ERR(p))
&kthread->flags);
&kthread->flags))
@k:
to_live_kthread(k);
(kthread)
exited.
spin_lock_irq(&worker->lock);
work->func(work);
insert_kthread_work(worker,
max_sequence;
(!func_prolog_dec(tr,
per_cpu(tracing_cpu,
irqsoff_print_header(struct
parent_ip)
start_critical_timing(CALLER_ADDR0,
stop_critical_timing(CALLER_ADDR0,
__irqsoff_tracer_init(tr);
irqsoff_tracer_reset,
irqsoff_tracer_start,
irqsoff_tracer_stop,
irqsoff_print_header,
irqsoff_print_line,
irqsoff_set_flag,
irqsoff_flag_changed,
irqsoff_trace_open,
irqsoff_trace_close,
register_tracer(&trace)
"module-internal.h"
compiled-in
initialised
dodgy_cert;
get_char_func
kdb_initial_cpu
kdb_current_task
kgdb_info[ks->cpu].task;
kdb_stub(struct
kdb_dbtrap_t
KDB_DB_BPT;
(reason
KDB_REASON_BREAK
ks->err_code,
KDB_CMD_KGDB)
DBG_PASS_EVENT;
gdbstub_state(ks,
ftrace_kill();
(ftrace_ops_list
ops->next
ftrace_update_trampoline(struct
CONFIG_FUNCTION_PROFILER
ftrace_profile_enabled
deviation
do_div(stddev,
(pg)
*)get_zeroed_page(GFP_KERNEL);
hash_long(ip,
(hlist_empty(hhd))
ftrace_find_profiled_func(stat,
Append
(index)
kfree(name);
recs
size_bits;
Trampolines
kmalloc(sizeof(*entry),
hlist_for_each_entry_safe(entry,
new_hash);
ops->func
pg->next)
rec->ip,
FTRACE_OPS_FL_SAVE_REGS)
(!match)
__ftrace_hash_rec_update(ops,
__ftrace_hash_update_ipmodify(ops,
*old_hash
"%s%02x",
rec->flags);
FTRACE_FL_TRAMP_EN)
(ops)
(enable
FTRACE_FL_ENABLED;
!(rec->flags
(update)
op->func_hash))
Ftrace
FTRACE_UPDATE_TRACE_FUNC;
*old_hash)
saved_ftrace_func
ftrace_startup_enable(command);
FTRACE_UPDATE_CALLS;
__unregister_ftrace_function(ops);
~FTRACE_OPS_FL_ENABLED;
ftrace,
pr_warn("
ourselves.
start_pg
pr_info("ftrace:
parser;
FTRACE_ITER_HASH)
((iter->flags
FTRACE_ITER_NOTRACE)
FTRACE_ITER_FILTER
Unfortunately,
iter->ops
&global_ops;
@ops.
@flag
iter->hash
regex,
(strstr(str,
ftrace_lookup_ip(hash,
&modname,
search_len
FTRACE_UPDATE_CALLS,
FTRACE_FUNC_HASHSIZE;
*hhd
&ftrace_func_hash[i];
old_hash_ops;
old_hash_ops.filter_hash
old_hash_ops.notrace_hash
alloc_and_copy_ftrace_hash(FTRACE_HASH_DEFAULT_BITS,
__unregister_ftrace_function_probe(glob,
mutex_lock(&ftrace_cmd_mutex);
&ftrace_commands,
mutex_unlock(&ftrace_cmd_mutex);
out_regex_unlock;
(buf
ftrace_set_regex(ops,
*count;
mutex_lock(&graph_lock);
mutex_unlock(&graph_lock);
fgd->size
ftrace_shutdown(ops,
*ipa
*ipb
*b,
tr->ops
op->func(ip,
trace_clear_recursion(bit);
&ftrace_pids,
ftrace_graph_return
trace_func_graph_ent_t
__ftrace_graph_entry
kmalloc(FTRACE_RETFUNC_DEPTH
ftrace_ret_stack),
t->curr_ret_stack
timestamp;
per_cpu(idle_ret_stack,
(!ret_stack)
"timekeeping_internal.h"
scenario:
.shift
John
Stultz
overflows.
nsec_per_tick
(u64)NSEC_PER_SEC
answer
(irqd_irq_inprogress(&desc->irq_data))
(desc)
IRQTF_AFFINITY
(action->thread)
set_bit(IRQTF_AFFINITY,
&action->thread_flags);
irq_data_to_desc(data);
irq_affinity_notify
setup_affinity(unsigned
Disables
Wakeup
"suspend
RAM".
desc->irq_data.chip;
IRQD_LEVEL);
IRQS_INPROGRESS
threads_oneshot
action->thread_fn(action->irq,
irq_finalize_oneshot(desc,
\"%s\"
desc->threads_active
WARN_ON(irq_settings_is_per_cpu_devid(desc)))
dev_id)
&desc->irq_data;
task_struct.
new->flags)
((old->flags
IRQF_TRIGGER_MASK)
(hard
new->name,
__irq_set_trigger(desc,
IRQS_SPURIOUS_DISABLED
IRQS_ONESHOT
desc->irq_count
module_put(desc->owner);
(!action)
entries:
line:
action->handler(irq,
(irqflags
action->percpu_dev_id
Broadcast
tick_broadcast_clear_oneshot(int
tick_resume_broadcast_oneshot(struct
timer_list.c
CLOCK_EVT_FEAT_DUMMY)
clockevents_handle_noop;
(!cpumask_empty(tick_broadcast_mask))
tick_clock_notify();
pr_warn_once("%s
tick_broadcast_start_periodic(bc);
tick_broadcast_setup_oneshot(bc);
Periodic
(!clockevents_program_event(dev,
(!dev
tick_broadcast_on);
(cpumask_test_and_clear_cpu(cpu,
clockevents_set_state(bc,
tick_broadcast_oneshot_mask))
(td->mode
tmpmask);
(dev->next_event.tv64
tick_broadcast_oneshot_mask);
min_delta
az)
implemented,
serialized,
jiffies_64
cpu_clock()
cxt.cur_ops->flags
cxt.cur_ops->flags);
lock_is_write_held
rare,
lock_is_read_held
cxt.nrealreaders_stress
fullstop
cxt.debug_lock
cxt.nrealwriters_stress;
wanted.
Detailed
Documentation/DocBook/genericirq
<trace/events/irq.h>
unhandled
storm
spin_lock(desc->lock);
heap
cpudl_exchange(cp,
Notes:
cp->size
_IRQ_NOREQUEST
_IRQ_NOPROBE);
SCHED_LOAD_RESOLUTION)
large,
sched_entities
cpupri;
ascending
task_rq(p)
(skip)
sched_feat(x)
~10%
double_rq_lock(struct
*l2)
l2)
swap(l1,
l2);
(rq1
rq2)
raw_spin_lock(&rq1->lock);
Portions
COPYING
Protects
hidden
roots
csses.
ss_mask);
*idr,
spin_lock_bh(&cgroup_idr_lock);
spin_unlock_bh(&cgroup_idr_lock);
@ss.
(ss)
&cgrp->self;
css.
(css
*cft
&cgrp->flags);
@ssid:
((ssid)
(ssid)
(ssid)++)
cgrp_cset_links
@cgrp->populated_cnt
*cset)
*link,
*tmp_link;
list_for_each_entry_safe(link,
tmp_link,
&cset->cgrp_links,
list_del(&link->cset_link);
@old_cset:
*old_cset,
(root->subsys_mask
get_css_set(cset);
changing.
cgroup_attach_task()
S_IRUGO;
subtree_control
cur_ss_mask;
non-default
cgrp->child_subsys_mask
(kernfs_type(kn)
kn->priv;
active_ref
cfts,
ss_mask)
(dst_root
ss));
ss->root
cgroup_root_from_kf(kf_root);
(root->flags
root->name);
(!*token)
opts->flags
forbidden
strlen(name);
(ss->disabled)
Option
opts;
*cset
&cset->tasks);
set_bit(CGRP_CPUSET_CLONE_CHILDREN,
css_release,
super_block
restart_syscall();
ignored\n");
@root's
cgroup_path(cgrp,
committed.
threadgroup
dst_cset
@preloaded_csets:
cgroup_migrate_add_src()
*preloaded_csets)
*tmp_cset;
tmp_cset,
mg_preload_node)
@dst_cgrp:
threadgroup_lock
Threads
*dst_cgrp,
@preloaded_csets.
compete
threadgroup)
&tset.src_csets,
leader;
&tset);
cg_list)
LIST_HEAD(preloaded_csets);
cgroup_migrate_finish(&preloaded_csets);
cgroup_kn_lock_live(of->kn);
out_unlock_cgroup;
throw
seq_putc(seq,
cgroup_print_ss_mask(seq,
css_for_each_descendant_pre(css,
new_ss;
kill_css(css);
kernfs_ops
-ENOTDIR;
Break
creator
css->cgroup;
cfts);
traversal)
on/offlining.
descendants.
visited.
locking,
rightmost
*last;
it->task_pos
Initiate
CGROUP_FILE_PROCS,
sizeof(pid_t),
lockdep_assert_held(&cgrp->pidlist_mutex);
duplicates
l->list
*stats,
*of
of->priv;
cgroup_pidlist_start,
cgroup_pidlist_next,
cgroup_pidlist_stop,
cgroup_pidlist_show,
.max_write_len
destroy_work);
INIT_WORK(&css->destroy_work,
queue_work(cgroup_destroy_wq,
&css->destroy_work);
css->id);
cgrp->id);
cgroup_idr_alloc(&ss->css_idr,
out_destroy;
killed.
refcnts
-ENAMETOOLONG;
system)
call_usermodehelper()
*token;
2002,
PFN_DOWN((unsigned
sig_enforce
param_set_bool(val,
unset
KERNEL_PARAM_OPS_FL_NOARG,
module_addr_min
*sechdrs;
_ddebug
module_put(mod);
*shdr
Section
*syms,
arr[]
mod->syms,
mod->syms
*crc;
api
b->name);
syms->start,
(crc)
*crc
list_for_each_entry(mod,
find_module_all(name,
align,
mod->field
scnprintf(buffer,
.attr
source_list)
&mod->target_list,
target_list)
*i
MODULE_STATE_GOING;
(mod->init
printed_something
*module)
(module)
kobject_uevent(&mk->kobj,
*crc_owner)
*symname,
(!crc)
try_to_force_load(mod,
occasional
MODULE_NAME_LEN);
module_sect_attrs
attrs[0];
module_notes_attrs
notes;
mod_kobject_put(mod);
num_params)
(end_pfn
ro_size,
set_memory_ro);
NX
set_page_attributes(mod->module_core,
set_page_attributes(mod->module_init,
unset_module_init_ro_nx(mod);
module_arch_freeing_init(mod);
module_memfree(mod->module_core);
s->name,
sizeof(Elf_Sym);
relocation
SHT_REL)
Additional
text_size
get_offset(mod,
&mod->init_size,
string;
get_modinfo(info,
ks
*sechdrs
(sym->st_shndx
info->sechdrs
long)ret
(info->len
weird
umod,
vfree(info->hdr);
stat.size)
object\n");
mod->extable
mod->init_size);
(strcmp(mod->name,
mod_initfree
wake_up_all(&module_wq);
ignored\n",
free_copy;
free_modinfo;
ELF
(!best)
get_ksymbol(mod,
-ERANGE;
comma-separated
*e
user-configured
task_lock()
cpuset_mutex,
cpuset_mutex.
callback_lock,
*pmask)
mems
free_cpus;
cs->effective_cpus);
mems_allowed,
cur)
@cp
background
errors,
theory
dattr
(dattr)
Continue
apn
(apn
'cpus'
membership
css_task_iter_start(&cs->css,
cs)
parent->effective_cpus);
(!*buf)
trialcs->cpus_allowed);
validate_change(cs,
trialcs);
cpumask_copy(cs->cpus_allowed,
cpuset_being_rebound
&newmems);
mempolicies
get_task_mm(task);
cs->mems_allowed
decimal
FM_MAXTICKS
fmp->val
cpuset_attach()
(!is_cpuset_online(cs))
hotunplug
mems_updated)
(cpus_updated)
cpu_active_mask
node_states[N_MEMORY];
&new_cpus);
@tsk.
spin_lock_irqsave(&callback_lock,
spin_unlock_irqrestore(&callback_lock,
cpu_possible_mask
yes.
hardwalled
__alloc_pages()
hardwall
(gfp_mask
print_name(m,
list_for_each_entry(class,
lock_stat_seq
"%c",
(stats->read_holdtime.nr)
"%40s
40
(14
vfree(data);
!CONFIG_SPARSE_IRQ
@entry:
IRQD_IRQ_DISABLED);
irq_state_set_disabled(desc);
(desc->irq_data.chip->irq_disable)
desc->irq_data.chip->irq_disable(&desc->irq_data);
desc->irq_data.chip->irq_unmask(&desc->irq_data);
(unlikely(!desc->action
(!irqd_irq_disabled(&desc->irq_data)
(irqd_irq_disabled(&desc->irq_data)
((desc->istate
out_eoi;
each.
MAX_RT_TEST_MUTEXES
9,
*td,
RTTEST_LOCK:
RTTEST_LOCKNOWAIT:
RTTEST_LOCKINT:
RTTEST_LOCKINTNOWAIT:
(mutex
&mutexes[dat])
(td->mutexes[dat]
td->mutexes[dat]
sprintf(curr,
%4d,
(MAX_RT_PRIO
.dev_name
Chris
Horn.
Changed
<linux/kobject.h>
CONFIG_SPARC
proc_doulongvec_minmax
CONFIG_DETECT_HUNG_TASK
CONFIG_COMPACTION
"0"
CONFIG_UEVENT_HELPER
proc_dointvec_jiffies,
defined(CONFIG_X86)
CONFIG_AIO
maxlen)
"This
warn_sysctl_write(table);
*size;
"-"
(*lvalp
*i,
!write))
left))
kbuf[left]
neg;
&lval,
do_proc_dointvec(table,write,buffer,lenp,ppos,
do_proc_dointvec_minmax_conv_param
*param
-*lvalp
table->extra1
(min)
table->extra2
(max).
'|')
convmul,
bitmap_len
',',
&c);
(left)
irqfixup
poller
IRQ_HANDLED;
SPURIOUS_DEFERRED
IRQS_POLL_INPROGRESS
desc->threads_handled_last
<linux/task_io_accounting_ops.h>
<linux/taskstats_kern.h>
max_threads;
(tsk->mm)
CONFIG_BLK_DEV_IO_TRACE
pprev
oldmm);
up_write(&mm->mmap_sem);
fail_nomem:
CONFIG_MEMCG
mm))
x);
execve
*exe_file;
vfork
put_task_struct(child);
mmput
Biederman
January
CONFIG_FUTEX
tsk->mm
current->fs;
spin_unlock(&fs->lock);
current->files;
CLONE_SIGHAND)
trace)
clone_flags);
current->timer_slack_ns;
TIF_SYSCALL_EMU);
init_task_pid(p,
attach_pid(p,
ERR_PTR(retval);
pid_link
PIDTYPE_PID;
CLONE_VFORK)
SYSCALL_DEFINE5(clone,
tls_val)
do_sysvsem
PLL
Updated
technical
memorandum
Jan
Mills
<linux/math64.h>
compatibility?
adjust;
(firsttime)
(MSEC_PER_SEC
MSEC_PER_SEC)
(USEC_PER_SEC
USEC_PER_SEC)
t.tv_nsec
calendar
sec,
modulo
MAX_JIFFY_OFFSET
multiply
TICK_NSEC
(sec
(TICK_NSEC
USER_HZ))
512,
years.
90,
ULLONG_MAX
command-line
curr->next;
Version:
timer_stats_active
*pe;
pe
(!pe)
ACCESS_ONCE(rcp->jiffies_stall)
vector,
vec->mask);
<linux/resource.h>
detach_pid(p,
signal_struct,
tsk->nvcsw;
zombie,
SIGCONT.
(p->signal->flags
(POSIX
pgrp);
assign_new_owner;
core_state
*father,
*dead)
EXIT_DEAD)
ptrace_entry)
(free
notifications.
PIDTYPE_PID)
!(wo->wo_flags
*infop;
infop
wo->wo_info;
put_user(SIGCHLD,
put_user((short)why,
put_user(pid,
put_user(uid,
task_pid_vnr(p);
task_uid(p));
wait_noreap_copyout(wo,
tgutime,
&tgutime,
&tgstime);
wo->wo_stat)
wo->wo_stat);
%TASK_TRACED
ptrace,
(!unlikely(wo->wo_flags
subthreads
notask_error
p->parent)
PIDTYPE_PGID;
WNOHANG
tai;
ktime_add(mono,
(likely(base
raw_spin_lock_irqsave(&base->cpu_base->lock,
raw_spin_unlock_irqrestore(&base->cpu_base->lock,
cpu_base->lock
*new_base)
new_base))
sft;
debug_object_activate(timer,
timerqueue_getnext(&base->active);
hrtimer_hres_enabled
hrtimer_interrupt
cpu_base
*offs_real
*offs_boot
*offs_tai
(!hrtimer_hres_active())
current->pid;
timer->start_comm,
below)
interval);
(re)start
__remove_hrtimer(timer,
delta_ns,
lock_hrtimer_base(timer,
enqueue_hrtimer(timer,
new_base);
(restart
Acquire
printk_once(KERN_WARNING
functions:
t->task
(rem.tv64
hrtimer_init_on_stack(&t.timer,
destroy_hrtimer_on_stack(&t.timer);
Absolute
@clock:
CLOCK_MONOTONIC
set_current_state()).
%TASK_UNINTERRUPTIBLE
%TASK_INTERRUPTIBLE
user_return_notifier
4))
*brl_options
__ATTR(_name,
b->tail
sp->per_cpu_ref
srcu_readers_seq_idx()
srcu_struct.
trycount)
spin_lock_irq(&sp->queue_lock);
synchronize_srcu(),
SRCU_INTERVAL
->batch_check0
drain.
init_utsname()->version);
TRACE_PRINT:
trace_selftest_test_global_cnt
PPC64
len1
len1,
*func_name;
PASSED
func_name
pr_cont("PASSED\n");
func_name,
pr_cont("*Could
filter*
pr_cont("*could
callback*
msleep(1);
Wrap
tracing_stop()
parallels
wakeup_test_data
Lab
notifier_to_errno(ret);
pm_test_level
"none",
sprintf(s,
memchr(buf,
last_dev
last_errno
last_step
CONFIG_PM_SLEEP_DEBUG
accepts
CONFIG_PM_TRACE
init_user_ns
.nr_extents
.extent[0]
.first
.lower_first
4294967295U,
CONFIG_PERSISTENT_KEYRINGS
(CONFIG_BASE_SMALL
uidhash_lock
*up,
uid))
uid_hash_find(uid,
spin_lock_irq(&uidhash_lock);
spin_unlock_irq(&uidhash_lock);
user-space.
audit_initialized
portid
audit_buffers
audit_sig_uid
ways:
AUDIT_MAXFREE
AUDIT_BUFSIZ
*nlh
last_msg
audit_backlog_limit);
AUDIT_LOCKED)
AUDIT_OFF)
limit)
skb);
kfree_skb(skb);
nlmsg_data(nlh);
exceeded");
skb,
audit_sock
clients
*nlh;
(!skb)
audit_make_reply(portid,
AUDIT_ADD_RULE:
AUDIT_DEL_RULE:
msg_type
msg_type);
AUDIT_FEATURE_TO_MASK(i);
new_lock,
audit_log_task_info(ab,
nlmsg_data(nlmsg_hdr(skb));
new_lock;
af.lock
memset(&s,
s.enabled
*bufp
bufp
(audit_sig_sid)
ab->skb
Audit
DECLARE_WAITQUEUE(wait,
__GFP_WAIT
audit_log_lost("out
ab->gfp_mask);
BUG_ON(!ab->skb);
avail)
skb_put(skb,
new_len
memcpy(ptr,
@string:
character,
(2
(prefix)
prefix);
auid
auid=%u
ses=%u",
auid,
(key)
dentry);
(n->name)
component
&context->pwd);
n->name->name,
(n->ino
ogid=%u
sid;
&sid);
-EINVAL)
fput(exe_file);
get_task_comm(comm,
mutex_clear_owner(lock);
lockdep_init_map(&lock->dep_map,
CONFIG_SPARSE_IRQ
irq_mark_irq(unsigned
_IRQ_DESC_PERCPU)
swsusp_header
swsusp_extent
ext
swap)
hib_bio_read_page(swsusp_resume_block,
hib_bio_write_page(swsusp_resume_block,
blkdev_put(hib_resume_bdev,
*)__get_free_page(__GFP_WAIT
__GFP_NORETRY);
alloc_swapdev_block(root_swap);
handle->reqd_free_pages
reqd_free_pages();
LZO_HEADER
LZO_CMP_PAGES
data_of(*snapshot),
err2
*thr;
run_threads;
wait_event(d->go,
atomic_read(&d->ready)
kthread_should_stop());
d->thr
atomic_set(&d->ready,
cmp_data
data[thr].unc_len
atomic_set(&crc->ready,
wake_up(&crc->go);
data[thr].cmp_len
length\n");
data[thr].cmp_len;
wait_event(crc->done,
atomic_read(&crc->stop));
atomic_set(&crc->stop,
SF_NOCOMPRESS_MODE)
snapshot_write_next(snapshot);
dec_data
ring_size
read_pages
_DEBUG_CORE_H_
*ks);
clock_nanosleep
(!rtcdev)
&alarm->node);
alarm->state
ALARMTIMER_NORESTART;
alarm->node.expires);
alarmtimer_enqueue(base,
alarm->node.expires
defined(CONFIG_DEBUG_LOCK_ALLOC)
Ditto
bouncing.
base->timer_jiffies
base->tv1.vec
(void)catchup_timer_jiffies(base);
(!tbase_get_deferrable(timer->base))
detach_timer(timer,
lock_timer_base(timer,
detach_if_pending(timer,
del_timer_sync()
spin_unlock(&base->lock);
spin_lock(&base->lock);
__mod_timer(timer,
TIMER_NOT_PINNED);
Here's
base->running_timer
list_for_each_entry_safe(timer,
any)
spin_unlock_irq(&base->lock);
base->timer_jiffies))
(!index
NEXT_TIMER_MAX_DELTA;
BUG_ON(cpu_online(cpu));
inlines
spin_acquire(&lock->dep_map,
LOCK_CONTENDED(lock,
do_raw_spin_lock);
_LINUX_CPUDL_H
exits.
call_usermodehelper_freeinfo(sub_info);
*sub_info
spin_lock(&umh_sysctl_lock);
spin_unlock(&umh_sysctl_lock);
umh_complete(sub_info);
status.
umh_disable_depth
running_helpers
up_read(&umhelper_sem);
usermodehelper_disabled.
call_usermodehelper_exec
sub_info
UMH_NO_WAIT)
GFP_KERNEL;
kernel/power/hibernate.c
HIBERNATION_SHUTDOWN;
(pm_test_level
!CONFIG_PM_DEBUG
Recover
drivers'
"noirq"
reappears
"aborting
Enable_irqs;
Enable_irqs:
Platform_finish:
pm_restrict_gfp_mask();
Close:
Thaw:
(pm_wakeup_pending())
(1);
Resume_devices:
HIBERNATION_REBOOT:
HIBERNATION_PLATFORM:
HIBERNATION_SHUTDOWN:
HIBERNATION_SUSPEND:
(hibernation_ops)
image.\n");
swsusp_close(FMODE_READ);
Finish:
Finish;
square
"%llu",
'unlocked'
'locked'
spinners
*ww_ctx,
use_ww_ctx)
ww_ctx);
xchg
*lock_count)
container_of(lock_count,
negated
xmitcsum
(BUFMAX
gdbstub_read_wait();
'#')
buf.
mem.
*long_val
dbg_reg_def[i].size;
shadow
gdbserial
'S';
*thread;
local_debuggerinfo
kgdb_mem2hex((char
*)gdb_regs,
regnum;
pack_threadid(ptr,
'T':
'R':
&remcom_in_buffer[2];
getthread(ks->linux_regs,
'Z'
(*bpt_type
'c';
LLLL
AA..AA
'D':
default_handle;
'3':
'W';
CPU");
loops;
__delay(1);
suspected:
UP:
"trylock
UP");
Overhead
DURATION
TRACE_GRAPH_PRINT_FILL_SHIFT,
frame_pointer)
(current->curr_ret_stack
FTRACE_NOTRACE_DEPTH;
*depth
corrupted.
long)panic;
atomic_inc_return(&data->disabled);
.depth
trace_print_lat_fmt(s,
print_graph_cpu(s,
cpu_data
cpu_data->depth
call->depth
FTRACE_RETFUNC_DEPTH)
cpu)->depth_irq);
(*depth_irq
print_graph_prologue(iter,
(lat)
TIME
kzalloc(sizeof(*data),
Prasanna
Panchamukhi
<prasanna@in.ibm.com>
<linux/stddef.h>
sizeof(kprobe_opcode_t));
&c->pages,
slots_per_page(c))
slots_per_page(c);
kip->nused
CONFIG_OPTPROBES
kprobe_disabled(p)
likely(!kprobe_disabled(kp)))
set_kprobe_instance(kp);
arch_remove_optimized_kprobe(op);
Unoptimize
Disarm
5:
kick_kprobe_optimizer();
post_handler
op->kp.flags
list_add(&op->list,
unoptimized
~KPROBE_FLAG_OPTIMIZED;
*ap)
optimize_kprobe(ap);
p->addr;
unoptimize_kprobe(p,
long)p->addr);
(add
reopt)
disarm
&rp->free_instances);
&kretprobe_inst_table[hash];
kretprobe_table_lock(hash,
kretprobe_table_unlock(hash,
kfree(ri);
free_rp_inst(rp);
&ap->list);
ap->break_handler
(p->post_handler
ap->post_handler
!kprobe_disabled(p))
p->addr)
&ap->list,
valid;
prohibit
arm_kprobe(p);
disarmed,
probepoint
raw_spin_unlock_irqrestore(&rp->lock,
*pi,
kprobe_type
seq_printf(pi,
modname
namebuf);
mutex_lock(&session->stat_mutex);
f);
mutex_lock(&all_stat_sessions_mutex);
copyright
set_normalized_timespec64(&tmp,
printk_deferred("WARNING:
clocksource,
*tkr)
tkr->read(tkr->clock);
overflows
clocksource_delta(cycle_now,
tk->tkr_mono.read(clock);
ntpinterval
tk->xtime_interval
tk->ntp_tick
tk->ntp_err_mult
CONFIG_ARCH_USES_GETTIMEOFFSET
timekeeper.
tkr,
base_mono
automatically.
wall_to_monotonic
tk->tkr_mono.cycle_last,
tk->tkr_mono.mask);
clocksource_cyc2ns(delta,
timespec64.
tk->tkr_mono.base;
offs)
timekeeping_get_ns(&tk->tkr_raw);
tv->tv_sec
__timekeeping_set_tai_offset(tk,
now.tv_nsec
persistent_clock_exists
began
sleeptime_injected
tk->cycle_interval;
adj_scale;
timekeeping_apply_adjustment(tk,
shift,
raw_nsecs;
raw_nsecs
producer");
MAX_NICE;
(*entry
!kill_test)
wake_up_process(consumer);
(!disable_reader)
(producer_fifo
missed);
USEC_PER_MSEC);
producer
cred->euid))
*pgrp;
set_one_prio(p,
task_pgrp(current);
nice_to_rlimit(task_nice(p));
retval)
niceval;
kegid;
new->gid
kegid)
new->sgid
keuid;
new->euid
keuid)
old->uid))
set_user(new);
new->suid
security_task_fix_setuid(new,
(!uid_eq(kuid,
!uid_eq(kruid,
old->suid))
!uid_eq(ksuid,
from_kuid_munged(cred->user_ns,
cred->euid);
!gid_eq(krgid,
!gid_eq(ksgid,
from_kgid_munged(cred->user_ns,
cred->sgid);
uid_eq(kuid,
(copy_to_user(name,
override_release(name->release,
sizeof(name->release)))
override_architecture(name))
down_write(&uts_sem);
*u
utsname();
up_write(&uts_sem);
&value);
RLIM64_INFINITY;
rlim->rlim_max
resource;
new_rlim->rlim_cur
signal->
r->ru_nvcsw
r->ru_nivcsw
r->ru_minflt
r->ru_majflt
r->ru_inblock
r->ru_oublock
RUSAGE_THREAD)
&mm->flags))
*)((char
<,
<=,
(check_data_rlimit(rlimit(RLIMIT_DATA),
rights
(arg4
arg3,
info->totalram
qrwlock
_QW_WMASK)
_QW_LOCKED)
(atomic_cmpxchg(&lock->cnts,
clockevents_program_event(dev,
td->mode
KDB_CMD_GO
Symbol
kdb_func_t
String
Help
kdb_task_state_char
CONFIG_KDB_KEYBOARD
*tp_event,
tp_event->perf_events
perf_trace_buf[i]
MAX_CMDLINECONSOLES
selected_console
52
45
ts_nsec;
dict_len;
__LOG_BUF_LEN
*)(log_buf
msg->len;
log_first_idx
*dict,
(msg->flags
127
pr_info("log_buf_len
*msg,
((prev
text;
(syslog_seq
syslog_seq
syslog_idx
syslog_prev
next_seq;
for_each_console(con)
Console
logbuf_cpu
newline,
vprintk_emit(0,
vscnprintf(buf,
consoles.
(!console_suspend_enabled)
(console_suspended)
(console_seq
Releases
console_seq
CON_ENABLED;
"real"
for_each_console(bcon)
CON_BOOT))
(newcon->index
newcon->index
CON_CONSDEV;
((newcon->flags
CON_CONSDEV)
newcon;
consoles,
console)
dumper's
kmsg_dump_get_line()
dumper->next_seq
dumper->next_idx
@syslog:
"<4>"
prefixes
Consecutive
*dumper,
*len)
spin_lock_irq(&task->sighand->siglock);
CPUCLOCK_PROF,
spin_unlock_irq(&task->sighand->siglock);
(!p
timer->it.cpu.expires)
timer->it_overrun
(CPUCLOCK_WHICH(which_clock))
raw_spin_lock_irqsave(&cputimer->lock,
cputime_to_expires(cputime.utime);
cputime.sum_exec_runtime;
release/switch
exit/exec
reload.
arm_timer()
cpu_clock_sample(timer->it_clock,
cpu_timer_sample_group(timer->it_clock,
bump_cpu_timer(timer,
(new_expires
posix_cpu_timer_kick_nohz();
hard)
cpu_itimer
it->incr;
it->error
it->expires;
@sample
cputimer
*oldval
spin_lock_irq(&timer.it_lock);
posix_cpu_timer_del(&timer);
@vma:
aligned,
n_pages;
free_buf;
buf->start
kref_put(&chan->kref,
relay_destroy_channel);
buf->subbufs_consumed)
buf->subbufs_consumed;
(chan->is_global
chan->buf[0])
(subbuf_size
rchan_percpu_buf_dispatcher
(buf->offset
buf->offset;
subbuf_size)
subbuf_size))
(consumed
@read_pos:
read_subbuf,
buf->padding[read_subbuf];
end_pos
splice
nonpad_end)
spin_lock_irqsave(&x->wait.lock,
spin_unlock_irqrestore(&x->wait.lock,
x->done
(!x->done)
-ERESTARTSYS)
->wait.lock
mouse
((scancode
Left
kbd_last_ret
caps
Up
kdb_printf("Unknown
ENTER,
0xe0
ENTER.
Platform_wake;
suspend_devices_and_enter(suspend_state_t
"please
broken-down
math_div(y1
math_div(y2
month
365
_LINUX_CPUPRI_H
@to
curr_clocksource
CONFIG_CLOCKSOURCE_WATCHDOG
schedule_work(&watchdog_work);
list_add(&cs->wd_list,
&watchdog_list,
Clocksource
cs->name);
mask:
CLOCK_SOURCE_IS_CONTINUOUS)
list_del_init(&cs->wd_list);
lots
do_div(sec,
UINT_MAX)
freq,
memcpy(dst,
sysfs_get_uname(buf,
strlen(buffer);
"kgdb");
KDB_STATE_SET(DOING_KGDB);
escape_data;
&kdb_poll_funcs[0];
(escape_delay
--escape_delay;
(mapkey
tmpbuffer,
lastchar;
++cp;
p_tmp
strlen(p_tmp);
len_tmp
prompt,
kdb_grep_trailing
searchfor,
len2))
strchr(cp,
got_printf_lock
*cphold
replaced_byte
linecount
sizeof(kdb_buffer)
kdb_buffer);
(len--)
(logging)
filtering,
bits\n");
|reader|
|------------------v
20)
RINGBUF_TYPE_DATA_TYPE_LEN_MAX)
RINGBUF_TYPE_PADDING;
(rb_null_event(event))
array[0]
local_set(&bpage->commit,
BUF_PAGE_HDR_SIZE;
"offset:%u;\tsize:%u;\tsigned:%u;\n",
int)offsetof(typeof(field),
head_page
forward.
RB_PAGE_HEAD;
~RB_FLAG_MASK;
old_flag)
rb_head_page_set(cpu_buffer,
cpu_buffer->pages;
old_write
cpu_buffer->tail_page)
~RB_WRITE_MASK;
next_page);
rb_head_page_deactivate(cpu_buffer);
&bpage->list))
rb_head_page_activate(cpu_buffer);
__GFP_NORETRY,
cpu_buffer->nr_pages
rb_check_pages(cpu_buffer);
cpu_buffer->tail_page
cpu_buffer->commit_page
DIV_ROUND_UP(size,
BUF_PAGE_SIZE);
buffer->flags
RB_WRITE_MASK;
head_bit
(cpu_buffer->tail_page
tail_page;
next_page
(cpu_buffer->nr_pages_to_update
rb_update_pages(cpu_buffer);
BUF_PAGE_SIZE;
(atomic_read(&buffer->record_disabled))
mutex_unlock(&buffer->mutex);
(cpu_buffer->commit_page
iter->head_page
NORMAL
BUF_PAGE_SIZE)
local_sub(length,
new_index
rb_event_ts_length(event);
local_clock_stable
-EAGAIN))
rb_end_commit(cpu_buffer);
(clearing
(ring_buffer_flags
RB_BUFFERS_ON)
trace_recursive_unlock();
irq_work_queue()
supplies
ring_buffer_unlock_commit
rb_iter_reset(iter);
arch_spin_lock(&cpu_buffer->lock);
3))
(!reader)
arch_spin_unlock(&cpu_buffer->lock);
*reader;
rb_get_reader_page(cpu_buffer);
(iter->head
rb_page_size(iter->head_page))
cpu_buffer->commit_page)
rb_test_started
rb_item
event_len;
data->max_size
total_size
(RB_WARN_ON(buffer,
t1;
t1
t2
SOFTIRQ_OFFSET
(softirq_count()
MAX_SOFTIRQ_RESTART
in_hardirq
*h;
network
wakeup_softirqd();
*__this_cpu_read(tasklet_vec.tail)
__this_cpu_write(tasklet_vec.tail,
*__this_cpu_read(tasklet_hi_vec.tail)
*a)
&per_cpu(tasklet_vec,
early_irq_init(void)
system->ref_count
~SYSTEM_FL_FREE_NAME;
__common_field(unsigned
ftrace_event_buffer
FTRACE_EVENT_FL_ENABLED))
soft_disable
!call->class
!call->class->reg)
(match
call->class->system)
<name>
ftrace_set_clr_event(tr,
FTRACE_EVENT_FL_ENABLED)
strcat(buf,
list_for_each_entry(dir,
&tr->systems,
put_system(dir);
subsystem_release,
seq_ops);
(strcmp(system->name,
trace_create_file("enable",
d_events
trace_array,
trace_array.
d_events,
event_test_stuff();
system->name);
new_value
sizeof(field.item),
((u32)(dl)
data_loc
FETCH_FUNC_NAME(method,
ftype,
sign,
uprobes
PM_QOS_CPU_DMA_LAT_DEFAULT_VALUE,
PM_QOS_NETWORK_LAT_DEFAULT_VALUE,
PM_QOS_NETWORK_THROUGHPUT_DEFAULT_VALUE,
PM_QOS_MEMORY_BANDWIDTH_DEFAULT_VALUE,
*req;
@value:
prev_value
&c->list);
curr_value);
*req)
*req
/*guard
(!pm_qos_request_active(req))
cancel_delayed_work_sync(&req->work);
d,
bm_total;
bm_first
(bm_cnt
scnprintf(bm_str,
BENCHMARK_EVENT_STRLEN,
bm_total
bm_totalsq
do_div(delta,
cpu_add_remove_lock
(cpu_hotplug.active_writer
atomic_inc(&cpu_hotplug.refcount);
take_cpu_down_param
to_cpumask(cpu_active_bits));
prime
v1
async_entry
newcookie;
starttime
audit_aux_data_pids
audit_aux_data_bprm_fcaps
native
AUDIT_PERM_WRITE)
biarch
ctx->prio
ctx->first_trees;
audit_uid_comparator(cred->euid,
cred->fsgid);
audit_gid_comparator(cred->egid,
rule->field_count;
AUDIT_PID:
Audit_equal)
Audit_not_equal)
AUDIT_ARCH:
AUDIT_INODE:
(f->lsm_rule)
&e->rule,
&state,
tsk->audit_context;
(context->in_syscall
tsk->audit_context
context->aux
context->prio
arg_num_len
has_cntl
send_sig(SIGKILL,
(len_left
max_execve_audit_len)
room_left
nargs
"mqdes=%d
call_panic
context->current_state
(inode
(!context->in_syscall)
n->name
n->name_len
audited
AUDIT_TYPE_PARENT
@mqdes:
mqdes,
memset(p,
audit_get_loginuid(t);
audit_get_sessionid(t);
security_task_getsecid(t,
bprm
new->cap_effective;
current));
ce_unbind
latch,
evt->mult
dev->mode
CONFIG_GENERIC_CLOCKEVENTS_MIN_ADJUST
(dev->min_delta_ns
dev->min_delta_ns);
clc;
CLOCK_EVT_STATE_SHUTDOWN)
dev->mult)
dev->shift;
clc,
list_del(&dev->list);
*ced,
clockevents
((dev->features
CLOCK_EVT_STATE_DETACHED);
*tick_get_tick_dev(struct
cpu_pm_enter
(usecs):
time_esterror
(scaled
discipline
stability
time_constant);
PPS_INTMIN;
pps_tf[0]
txc->jitter
second_length
NTP_INTERVAL_FREQ);
secs)
(secs
(NTP_SCALE_SHIFT
secs);
STA_PLL))
freq_adj
(SHIFT_PLL
NTP_SCALE_SHIFT,
adjtime()
next.tv_nsec
txc->status
anything,
txc->offset
MONOTONIC_RAW
pps_fbase
*raw_ts;
__sched_clock_stable_early
(sched_clock_stable())
sched_clock_running
scd
crazy
remote_clock;
cpu_clock().
(unlikely(!sched_clock_running))
"0x%x")
"%d")
*dummy,
bitfield
(bw
NLA_U32
rc,
down_write(&listeners->sem);
up_write(&listeners->sem);
memset(stats,
(!first
nlattr
TASKSTATS_NEEDS_PADDING
na
na);
prepare_reply(info,
send_reply(rep_skb,
taskstats_packet_size();
TASKSTATS_CMD_NEW,
Version
kexec_purgatory_size
kernel",
kexec_on_panic
image->segment[i].memsz;
corrupt
crashk_res.start)
&image->head;
image->type
(kexec_on_panic)
crashk_res.start;
kimage_free_page_list(&image->control_pages);
kfree(image);
(pages)
image->entry++;
for_each_kimage_entry(image,
indirection
page_to_pfn(page)
*segment)
*kbuf
uchunk,
mchunk
~PAGE_MASK));
KEXEC_ARCH_MASK)
(nr_segments
KEXEC_SEGMENT_MAX)
(!mutex_trylock(&kexec_mutex))
KEXEC_ON_CRASH)
*ksegment;
ksegment
((start
3)/4;
commandline
parses
*crash_base
&cur);
unrecognized
char\n");
*suffix)
end_p
__parse_crashkernel(cmdline,
crash_base,
"crashkernel=",
VMCOREINFO_OFFSET(zone,
kbuf->memsz
memsz,
buf_align,
top_down,
out_free_digest;
kfree(desc);
modifications
buf_align
bss_align
bss_sz
align)
(bss_align
bss_pad
bss_addr
pi->sechdrs;
refrigerator.
pm_nosig_freezing
**bio_chain);
*result
kallsyms_expand_symbol(off,
*symbolsize,
get_symbol_pos(addr,
kallsyms_expand_symbol(get_symbol_offset(pos),
"0x%lx",
@address:
__sprint_symbol(buffer,
iter->name,
*pos))
kp
(posth_val
cleard
target2(rand1);
jph_val
rand1;
(jph_val
pr_err("jprobe
pr_err("kretprobe
pr_info("The
bug!\n");
__LINUX_MCS_SPINLOCK_H
field->str);
trace_seq_bprintf(s,
field->fmt,
"%s0x%x",
sym_flags)
(entry->flags
trace_find_cmdline(entry->pid,
iter->ts;
list_for_each_entry(e,
event->type
field->parent_ip);
.hex
S,
T;
task_state_char(field->next_state);
task_state_char(field->prev_state);
_KERNEL_WORKQUEUE_INTERNAL_H
newdev);
curdev
out_bc;
-nsec;
1000000);
P(F)
long)F)
PN(F)
long)F))
.%-40s:
avg_atom
avg_per_cpu
per-rcu_node
t->rcu_read_unlock_special.b.blocked
rdp->grpmask)
rcu_preempt_qs();
np
(&t->rcu_node_entry
rcu_node_entry);
wait_rcu_gp(call_rcu);
rnp->expmask
->exp_tasks
t's
rnp->boost_kthread_status
RCU_KTHREAD_WAITING;
outgoingcpu)
!defined(CONFIG_RCU_FAST_NO_HZ)
idle-entry
RCU_IDLE_GP_DELAY
*dj
rdtp->last_accelerate
wake_nocb_leader(rdp,
__call_rcu_nocb_enqueue(rdp,
trace_rcu_nocb_wake(my_rdp->rsp->name,
my_rdp->cpu,
my_rdp;
rdp->nocb_next_follower)
leader-follower
need_rcu_nocb_mask
pr_info("\tOffload
rdp->nocb_leader
(rdtp->dynticks_idle_nesting
ACCESS_ONCE(full_sysidle_state);
non-idle.
idle!
non-idle,
CONFIG_NO_HZ_FULL_SYSIDLE_SMALL)
maxj,
rcu_sysidle_head
oldrss
(rss
kdb_printf("kdbgetsymval:
symtab->sym_start
kdbnearsym
kdb_printf("kdbnearsym:
knt1
prefix_name
prev_len
prefix_len)
__u8
TASK_TRACED;
symtab;
sparse
dap_locked
(!get_dap_lock())
__release(dap_lock);
dah_overhead;
bestprev
dah_first
spin_unlock(&dap_lock);
debug_alloc_pool
POISON_FREE,
(debug_alloc_pool
debug_kusage_one_time
h_used,
kdb_flags
%Lu\n",
mutex_lock(&vt_switch_mutex);
&pm_vt_switch_list,
mutex_unlock(&vt_switch_mutex);
[enum
Length
sig_len;
modlen)
bytes.
(rq)
(unlikely(sched_info_on()))
sum_exec_runtime
thread_group_cputime
(!cputimer_running(tsk))
raw_spin_lock(&cputimer->lock);
raw_spin_unlock(&cputimer->lock);
pr_debug("in
mmio_reset_data(tr);
(!hiter)
mmiotrace_rw
rw->map_id,
long)rw->phys,
rw->pc,
(rw->value
0xff,
mmiotrace_map
_LINUX_NTP_INTERNAL_H
_KERNEL_EVENTS_INTERNAL_H
arch_perf_out_copy_user
CONFIG_SYSCTL_SYSCALL
newlen);
BUFSZ
"flush"
"gc_interval"
"forwarding"
bin_net_ipv4_conf_vars_table
NET_PROTO_CONF_DEFAULT,
"default",
"conf",
bin_net_ipv6_conf_var_table
"debug"
kernel_write(file,
memcpy(path,
compat_sysctl_args
BITS_PER_PAGE_MASK;
BITS_PER_PAGE)
(unlikely(!map->page))
spin_lock_irq(&pidmap_lock);
mk_pid(pid_ns,
ns->level;
task_active_pid_ns(current));
permitted,
tocopy;
tocopy
CAP_LAST_U32_VALID_MASK;
resource.
p->sibling;
root->end
&iomem_resource;
root->end)
%pR\n",
__request_resource(root,
conflict;
res->start,
res->end;
res.flags
orig_end
res.end;
((res.start
res.end)
RAM"
(p->end
*constraint)
tmp.start
old->start
new->start
__request_resource(parent,
new->start)
new->end)
(!conflict)
(conflict->start
res->name
conflict->end
IORESOURCE_BUSY))
(!(res->flags
strict_iomem_checks
resource_entry
function_trace_call;
tr->function_enabled
integral
_TIMEKEEPING_INTERNAL_H
KDB_BASE_CMD_MAX
"Environment
defined"),
__nenv;
ep
ep;
*endp;
kdb_check_regs();
&symtab);
(*nextarg)++;
'defcmd'
s->usable
kdb_printf("Could
cmdstr);
argv[0]);
(isspace(*cp))
kdb_printf("invalid
grephelp\n");
strlen(cp);
ignore_errors
kdb_max_commands)
cmd_tail)
(cmdptr
cmdptr
db_result);
cmdbuf
CMD_BUFLEN,
'go'
addr++;
(phys)
(!valid)
fmtchar
KDB_BADCPUNUM;
rsize
(adjust
printed\n",
(start_cpu
kdb_printf("%d
kdb_tm
tm->tm_mday
KDB_PCU(cpu)
(kp->cmd_name
kp->cmd_name
"Backtrace
variables",
AUDIT_BITMASK_SIZE
entry->rule.mask));
switch(f->type)
AUDIT_MSGTYPE:
f->val
audit_free_rule(entry);
&list);
audit_put_tree(tree);
(watch)
entry->rule.prio
Audit_equal:
Audit_not_equal:
Audit_lt:
Audit_le:
Audit_gt:
Audit_ge:
Audit_bitmask:
Audit_bittest:
parentlen
task->comm,
(llist_add(&work->llnode,
chained;
(vaddr
copy_from_page(page,
@mm:
@vaddr:
opcode.
@vaddr.
errno.
put_old;
*n
n->rb_left;
n->rb_right;
inode:offset
atomic_inc(&u->ref);
down_write(&uprobe->consumer_rwsem);
up_write(&uprobe->consumer_rwsem);
*uc;
(uc
uc->next)
set_bit(MMF_RECALC_UPROBES,
(!valid_vma(vma,
offset_to_vaddr(vma,
info->vaddr);
!valid_vma(vma,
register_for_each_vma(uprobe,
@uc:
@uprobe
down_write(&uprobe->register_rwsem);
up_write(&uprobe->register_rwsem);
find_uprobe(inode,
area;
mm->uprobes_state.xol_area;
(!area)
slot_addr;
slot_nr
utask->return_instances;
t->utask
pr_warn("uprobe:
"dup
xol
trampoline_vaddr
uretprobe
pid/tgid=%d/%d\n",
current->tgid);
instruction_pointer_set(regs,
restart.
singlestep
STOPPED
child->ptrace
ptrace_signal_wake_up(child,
PT_OPT_FLAG_SHIFT);
clear_tsk_thread_flag(child,
buf[128];
spin_lock_irq(&child->sighand->siglock);
spin_unlock_irq(&child->sighand->siglock);
((request)
PTRACE_SYSEMU
->exit_code
wait_task_stopped()
CONFIG_HAVE_ARCH_TRACEHOOK
user_regset
STOP.
out_put_task_struct;
map->nr_extents;
map->extent[idx].first;
map->extent[idx].count
extents)
map->extent[idx].lower_first;
kuid.
Maps
INVALID_UID
from_kuid_munged
INVALID_GID
from_kgid_munged
INVALID_PROJID
kprojid_t
projid.
from_kprojid_munged
*lower_ns;
lower;
((lower_ns
lower_ns->parent)
lower_ns->parent;
%10u\n",
extent->first,
m_start(seq,
lower_first
extent->count
simple_strtoul(pos,
&pos,
*seq_ns
(!ns->parent)
((seq_ns
(seq_ns
ns->parent))
map_write(file,
blk_tracer_enabled
blk_tracer
BLK_IO_TRACE_MAGIC
trace_note(bt,
*bt)
sector
__name)
what;
pdu_len;
(bt->trace_state
*bdev)
hd_struct
bt->end_lba
blk_user_trace_setup
alloc_percpu(unsigned
bt);
bt->act_mask
mutex_lock(&bdev->bd_mutex);
mutex_unlock(&bdev->bd_mutex);
(rq->cmd_type
REQ_TYPE_BLOCK_PC)
blk_rq_pos(rq),
!bio_flagged(bio,
bio->bi_iter.bi_sector,
bio->bi_iter.bi_size,
bio->bi_rw,
pdu_start(ent);
te_blk_io_trace(iter->ent);
pdu_len
"[%s]\n",
t_sec(ent),
t_error(ent));
&dev_attr_act_mask)
cpu_index;
seq_nr
padata_priv,
list_del_init(&padata->list);
parallelized
list_add_tail(&padata->list,
per_cpu_ptr(pd->squeue,
*pd
padata_reorder(pd);
*pcpumask,
*cbcpumask)
cbcpumask)
notification_mask
padata_validate_cpumask(pinst,
pcpumask);
padata_replace(pinst,
__padata_start(pinst);
PADATA_CPU_PARALLEL
pinst->cpumask.pcpu)
free_cpumask_var(pinst->cpumask.pcpu);
kobj2pinst(kobj);
_show_name,
min_offline
min_online
starttime;
VERBOSE_TOROUT_STRING("torture_onoff
trsp->trs_state
*stp;
mutex_lock(&shuffle_task_mutex);
mutex_unlock(&shuffle_task_mutex);
shuffle_tmp_mask);
*title)
(ACCESS_ONCE(fullstop)
shutdown_secs
ACCESS_ONCE(fullstop)
ACCESS_ONCE(stutter_pause_test)
torture_stutter
torture_type
*params,
min_level,
max_level,
in_quote
args[i-1]
`%s'\n",
*)kp->arg
kparam_array
*arr
container_of(n,
__modinit
mk->mp
free_module_param_attrs(&mod->mkobj);
kobject_put(&mk->kobj);
dot
*per_cpu_ptr(desc->kstat_irqs,
@irq.
scanned
spurious.
for_each_irq_desc_reverse(i,
irq_settings_can_probe(desc))
mutex_unlock(&probing_active);
(unused)
Dipankar
Sarma
<dipankar@in.ibm.com>
inputs
Papers:
CONFIG_TINY_RCU
Intended
sysfs/boot
rcu_gp_is_expedited()
idea.
synchronize_rcu_expedited()
Preemptible
rcu_read_unlock_special(t);
rrln
debug_lockdep_rcu_enabled()
(!debug_lockdep_rcu_enabled())
(!rcu_lockdep_current_cpu_online())
irqs_disabled();
Awaken
init_completion(&rcu.completion);
me
wait_for_completion(&rcu.completion);
debug_object_init(head,
debug_object_free(head,
debug_object_activate(head,
informs
auto
heap.
builds.
scope.
init_rcu_head_on_stack(),
RCU_STALL_DELAY_DELTA
(5
(till_stall_check
ACCESS_ONCE(rcu_cpu_stall_timeout)
300;
(rcu_cpu_stall_suppress
call_rcu_tasks()
&rcu_tasks_cbs_head;
raw_spin_lock_irqsave(&rcu_tasks_cbs_lock,
rhp;
rcu_tasks_cbs_tail
raw_spin_unlock_irqrestore(&rcu_tasks_cbs_lock,
synchronize_rcu_tasks(),
primitive,
hooks.
easy.
needreport,
t->rcu_tasks_nvcsw
ACCESS_ONCE(t->rcu_tasks_holdout)
%lu/%lu
__noreturn
housekeeping
housekeeping_affine(current);
some,
(!list)
t->nvcsw
suffices
synchronize_sched(),
voluntarily
job,
scans
rtst
time_after(jiffies,
t1,
->nvcsw
reordering
->rcu_tasks_holdout
mutex_unlock(&rcu_tasks_kthread_mutex);
rcu_early_boot_tests(void)
tests\n");
(rcu_self_test)
(rcu_self_test_bh)
(rcu_self_test_sched)
rcu_barrier();
rcu_barrier_sched();
gcov-kernel
Hubertus
Franke
<frankeh@us.ibm.com>
Nigel
Hinds
<nhinds@us.ibm.com>
Rajan
Ravindran
<rajancr@us.ibm.com>
Larson
"gcov:
gcc-generated
reports.
0x%x\n",
(gcov_events_enabled)
gcov_event(GCOV_ADD,
gcov_event()
replay
((info
gcov_info_next(info)))
mod->module_core,
old_lvl
CONSOLE_LOGLEVEL_MOTORMOUTH;
old_lvl;
<pid>
btt
bta
[DRSTCZEUIMA]
mds
handy
examining
traceback
p->pid);
"Enter
(buffer[0]
'q')
Prompt
kdb_task_state_string(argc
kdb_ps_suppressed();
(kdb_bt1(p,
btaprompt))
find_task_by_pid_ns(pid,
~0;
kdb_parse,
!cpu_online(cpu))
kdb_printf("no
"btt
0x%p\n",
kdb_parse(buf);
<asm/tlbflush.h>
swsusp_page_is_free(struct
swsusp_set_page_forbidden(struct
swsusp_unset_page_forbidden(struct
(tunable
reserved_size;
hibernate_reserved_size_init(void)
reserved_size
image_size;
hibernate_image_size_init(void)
PBEs
PageNosaveFree
*)get_zeroed_page(gfp_mask);
swsusp_set_page_forbidden(virt_to_page(res));
swsusp_set_page_forbidden(page);
swsusp_set_page_free(page);
swsusp_unset_page_forbidden(page);
swsusp_unset_page_free(page);
*lp
'the
*chain;
safe_needed;
ca->chain
(LINKED_PAGE_DATA_SIZE
*lp;
bitmaps.
zone_bitmap
bm_block
bitmap,
organized
((1UL
Zone
Bitmap
BM_RTREE_LEVEL_SHIFT
zone->blocks
block_nr;
zone->blocks;
levels_needed
&zone->nodes);
zone->rtree;
BM_RTREE_LEVEL_SHIFT);
BM_RTREE_LEVEL_MASK;
block;
free_zone_bm_rtree(struct
Allocated
nr_blocks;
zone->start_pfn
free_zone_bm_rtree(zone,
zone;
structs.
free_image_page(node->data,
mem_zone_bm_rtree,
list_entry(bm->cur.zone->leaves.next,
memory_bm_free(struct
@list
*ext,
kfree(ext);
zone_end
list_for_each_entry(ext,
(zone_start
zone_start;
zone's
merging
ca;
chain_init(&ca,
INIT_LIST_HEAD(&bm->zones);
bm->p_list
ca.chain;
Exit:
memory_bm_free(bm,
occupied
&bm->zones,
**addr
Jumps
start_pfn,
*region;
(they
init,
nosave_region),
region->start_pfn
nosave
%#010llx-%#010llx]\n",
start_pfn
(free_pages_map)
page_to_pfn(page))
(forbidden_pages_map)
Marking
(pfn_valid(pfn))
*bm1,
*bm2;
bm1
memory_bitmap),
bm2
kfree(bm2);
memory_bm_free(bm1,
kfree(bm1);
number)
snapshot_additional_pages(struct
*saveable_highmem_page(struct
(page_zone(page)
zone)
(page_is_guard(page))
count_highmem_pages(void)
(!is_highmem(zone))
page_address(s_page));
kernel_map_pages(s_page,
saveable_page(zone,
copy_data_page(unsigned
dst_pfn,
src_pfn)
kmap_atomic(d_page);
kmap_atomic()
s_page);
copy_page(dst,
*copy_bm,
nr_copy_pages;
reinitialized
memory_bm_next_pfn(free_pages_map);
memory_bm_next_pfn(forbidden_pages_map);
(fb_pfn
(fr_pfn
@nr_pages:
(avail_normal
alloc_normal)
preallocate_image_pages(alloc,
preallocate_image_highmem(unsigned
__GFP_HIGHMEM);
(an
multiplier,
multiplier;
preallocate_highmem_fraction(unsigned
(to_free_normal
memory_bm_position_reset(&copy_bm);
Estimate
hard,
[number
(4)
saveable)
drivers,
tunable
DIV_ROUND_UP(reserved_size,
preallocation
alloc,
memory...
GFP_IMAGE,
memory_bm_create(&copy_bm,
save_highmem;
(unless
preallocate
max_size);
Next,
preallocate_image_memory(alloc,
highmem.
preallocate_image_highmem(alloc);
count_pages_for_highmem(unsigned
free);
get_highmem_buffer(int
alloc_highmem_pages(struct
count_free_highmem_pages();
(to_alloc
(to_alloc--
nr_highmem);
drain_local_pages(NULL);
pages\n",
nr_highmem))
&orig_bm);
space!
nr_pages);
init_header_complete(struct
*check_image_kernel(struct
LINUX_VERSION_CODE)
"kernel
"system
memset(info,
info->num_physpages
info->image_pages
info->pages
info->size
@buf[]
(unlikely(buf[j]
data_of()
stream
snapshot_read_next(struct
memory_bm_position_reset(&orig_bm);
again).
*kaddr;
handle->cur++;
allocated_unsafe_pages
memory_bm_next_pfn(src);
(reason)
count_highmem_image_pages(struct
prepare_highmem_image(struct
"safe",
page's
@buffer,
get_highmem_page_buffer(struct
*pbe;
pbe->copy_page
safe_pages_list->next;
pbe->next
highmem_pblist;
pbe;
copy_last_highmem_page(void)
last_highmem_page_copied(void)
free_highmem_data(void)
free_image_page(buffer,
chain_alloc()
ERR_PTR(-EFAULT);
snapshot_write_next(struct
&copy_bm);
get_buffer(&orig_bm,
&ca);
(IS_ERR(handle->buffer))
PTR_ERR(handle->buffer);
copy_last_highmem_page();
page_key_write(handle->buffer);
snapshot_write_finalize(struct
snapshot_image_loaded(struct
restore_highmem(void)
FREEZING_SELF
FREEZING_PARENT
reason.
CGROUP_FREEZING_SELF
CGROUP_FREEZING_PARENT
*freezer;
@css.
CGROUP_FROZEN;
update_if_frozen()
@tset
__thaw_task(task);
upwards
Losing
migration.
lazily
lockdep_assert_held(&freezer_mutex);
CGROUP_FROZEN))
frozen?
css_freezer(pos);
determining
condition.
bottom-up
(!css_tryget_online(pos))
css_put(pos);
css_task_iter_start(&freezer->css,
@freezer:
@freeze:
unfreeze
@cgroup
thawing
*freezer,
Freeze
affected.
freezer_apply_state(pos_f,
CGROUP_FREEZING_SELF);
CGROUP_FREEZING_PARENT);
(strcmp(buf,
(bool)(freezer->state
.fork
(tree-based
<mingo@elte.hu>
<linux/cache.h>
<linux/seqlock.h>
NR_CPUS,
practice,
RCU_FANOUT_2
RCU_FANOUT_3
RCU_FANOUT_4
RCU_FANOUT_2)
insufficient
rcu_num_nodes;
irq/process
odd.
advanced.
RCU_KTHREAD_YIELDING
Definition
following.
gpnum.
Per-GP
qsmask
grpmask;
qsmask.
propagating
tree?
Counts
&(rsp)->node[0];
&(rsp)->node[rcu_num_nodes];
no-op.
(rsp)->level[rcu_num_lvls
nxttail
Highest
invocations.
wrap.
partitions
sublists.
qlen_lazy;
qlen;
Upper
Kicked
Defer
activity.
RCU_SAVE_DYNTICK
RCU_JIFFIES_FQS_DIV
Very
RCU_STALL_RAT_DELAY
Small
Hierarchy
(*call)(struct
Subject
gp.
Commands
Done
_done.
force_quiescent_state().
gp_flags
DECLARE_PER_CPU(unsigned
rcu_cpu_kthread_status);
rcu_cpu_kthread_loops);
rcu_cpu_has_work);
call_rcu(struct
*rcu));
rcu_spawn_one_boost_kthread(struct
rcu_organize_nocb_kthreads(struct
rcu_kick_nohz_cpu(int
*maxj);
rcu_nohz_full_cpu(struct
lengths
*ql
*qll
RCU_TRACE(stmt)
DYNTICK_TASK_NEST_WIDTH
principle
DYNTICK_TASK_MASK
DYNTICK_TASK_FLAG
DYNTICK_TASK_EXIT_IDLE
(LLONG_MAX
((DYNTICK_TASK_NEST_VALUE
debug_rcu_head_queue(struct
debug_object_active_state(head,
&rcuhead_debug_descr,
debug_rcu_head_unqueue(struct
Reclaim
case)
rcu_lock_release(&rcu_callback_map);
head->func(head);
Strings
consumption,
2013
DEFAULT_REBOOT_MODE
privately
reboot=
DMI
reboot_default
reboot_cpu;
machine_emergency_restart();
blocking_notifier_call_chain(&reboot_notifier_list,
usermodehelper_disable();
device_shutdown();
Info
Hook
Handler
guidelines
do_kernel_restart
Multiple
exist;
restarts
cpu_hotplug_disable();
pr_emerg("Restarting
power_off
sync:
yourself
LINUX_REBOOT_CMD_RESTART:
LINUX_REBOOT_CMD_HALT:
kernel_halt();
LINUX_REBOOT_CMD_POWER_OFF:
LINUX_REBOOT_CMD_RESTART2:
ie
**argv;
*envp[]
"HOME=/",
call_usermodehelper(argv[0],
argv_free(argv);
issue\n");
emergency_sync();
shutdown:
syncing
emergency
DECLARE_WORK(poweroff_work,
(force)
"true"
Having
&reboot_cpu);
'm'
str[2]
'p'
'k':
'e':
'p':
'f':
(str)
__KERNEL_RTMUTEX_COMMON_H
<linux/rtmutex.h>
scenarios.
schedule_rt_mutex_test(struct
schedule_rt_mutex(_lock)
(!(current->flags
rt_mutex,
tree_entry);
pi_tree_entry);
Constants
RT_MUTEX_FULL_CHAINWALK,
*rt_mutex_next_owner(struct
rt_mutex_init_proxy_locked(struct
*proxy_owner);
rt_mutex_proxy_unlock(struct
rt_mutex_start_proxy_lock(struct
rt_mutex_finish_proxy_lock(struct
rt_mutex_timed_futex_lock(struct
"rtmutex.h"
Richard
Henderson
Concurrent
<linux/signalfd.h>
(handler
!force)
Tracers
signal->sig[1]
blocked->sig[1];
&t->blocked))
TIF_SIGPENDING,
harmless
serviced.
*pending,
(!x)
@task->jobctl.
%JOBCTL_STOP_CONSUME
signo
BUG_ON(mask
JOBCTL_STOP_CONSUME
PF_EXITING)))
trapping
TRACED.
JOBCTL_TRAPPING))
wake_up_bit()
consumption
JOBCTL_STOP_CONSUME;
JOBCTL_STOP_PENDING);
sig->flags
spin_lock_irqsave(&t->sighand->siglock,
spin_unlock_irqrestore(&t->sighand->siglock,
list_for_each_entry_safe(q,
&pending->list,
q->info.si_signo;
spin_lock_irqsave(&tsk->sighand->siglock,
spin_unlock_irqrestore(&tsk->sighand->siglock,
spin_lock_irqsave(&current->sighand->siglock,
current->notifier_data
current->notifier
spin_unlock_irqrestore(&current->sighand->siglock,
list_for_each_entry(q,
fast-pathed
info->si_errno
info->si_pid
ourselves,
itimer
DoS
itimers,
reducing
*tmr
&tsk->signal->real_timer;
tsk->signal->it_real_incr.tv64
overruled
SIGKILL).
__SI_MASK)
info->si_sys_private)
signal..
->blocked
t->state
TASK_INTERRUPTIBLE))
(uid_eq(cred->euid,
uid_eq(cred->euid,
(ns_capable(tcred->user_ns,
*sid;
seized
listening
PTRACE_CONT.
assert_spin_locked(&t->sighand->siglock);
task_set_jobctl_pending(t,
JOBCTL_TRAP_NOTIFY);
JOBCTL_LISTENING);
process-wide
effects
blocking,
SIG_DFL.
dying,
siginitset(&flush,
&signal->shared_pending);
queues,
(likely(!(t->ptrace
PT_SEIZED)))
__TASK_STOPPED);
ptrace_trap_notify(t);
CLD_CONTINUED
continued.
(why)
->siglock,
signal->group_stop_count
signal->group_exit_code
SIG.
fatal,
!(signal->flags
SIGNAL_GROUP_EXIT;
JOBCTL_PENDING_MASK);
sigaddset(&t->pending.signal,
SIGRTMIN)
CONFIG_USER_NS
userns_fixup_signal_uid(struct
make_kuid(current_user_ns(),
TRACE_SIGNAL_IGNORED;
(!prepare_signal(sig,
&t->signal->shared_pending
&t->pending;
TRACE_SIGNAL_ALREADY_PENDING;
TRACE_SIGNAL_DELIVERED;
SIGKILL.
EAGAIN
override_rlimit
list_add_tail(&q->list,
&pending->list);
q->info.si_signo
q->info.si_errno
q->info.si_code
task_tgid_nr_ns(current,
task_active_pid_ns(t));
q->info.si_uid
silent
signalfd_notify(t,
sigaddset(&pending->signal,
complete_signal(sig,
trace_signal_generate(sig,
CONFIG_PID_NS
__send_signal(sig,
signr);
action->sa.sa_handler
local_irq_save(*flags);
SLAB_DESTROY_BY_RCU
spin_lock(&sighand->siglock);
spin_unlock(&sighand->siglock);
check_kill_permission(sig,
readlock
*pgrp)
!err;
-ESRCH))
find_vpid(pid));
*pcred
(!uid_eq(cred->euid,
pcred->suid)
!uid_eq(cred->uid,
unspecified,
__kill_pgrp_info(sig,
task_pgrp(current));
for_each_process(p)
!same_thread_group(p,
++count;
-EPERM)
BUG_ON(!(q->flags
SIGQUEUE_PREALLOC));
dequeued,
self-reaping.
task_uid(tsk));
info.si_utime
info.si_stime
(tsk->exit_code
(psig->action[SIGCHLD-1].sa.sa_handler
SA_NOCLDWAIT
flag:
wait4
Rather
tsk->parent);
__wake_up_parent(tsk,
autoreap;
notifies
why)
tsk->group_leader;
tsk->real_parent;
generated,
coredump
dequeued.
current->sighand->siglock
stops.
__releases(&current->sighand->siglock)
gstop_done
Meanwhile,
do_wait().
bookkeeping.
TASK_TRACED
JOBCTL_STOP_PENDING))
NOTIFY
task_clear_jobctl_pending(current,
JOBCTL_TRAP_STOP);
duplicates.
preempt_enable_no_resched();
touching
exit_code)
(unlikely(current->task_works))
task_work_run();
@signr:
participated
@current->sighand->siglock
SIGNAL_STOP_STOPPED
group_exit_code
sig->group_exit_code
gstop))
sig->group_stop_count++;
while_each_thread(current,
trap.
PTRACE_EVENT_STOP
siginfo.
eight
signal;
SIGTRAP;
WARN_ON_ONCE(!signr);
8),
ptrace_stop(signr,
*sighand
current->sighand;
freezing()
ptrace_stop()
(unlikely(signal->flags
continuing.
per-process
*ka;
(unlikely(current->jobctl
&ksig->info);
(ka->sa.sa_handler
container.
posted.
Anything
ksig->sig
*ksig,
stepping)
*which)
sigandsets(&retarget,
(sigisemptyset(&retarget))
(t->flags
group_stop
PF_EXITING;
threadgroup_change_end(tsk);
retarget_shared_pending(tsk,
*restart
__set_task_blocked(tsk,
happily
friends.
Lockless,
@how:
@nset:
@oset:
non-null
oset,
sigprocmask(how,
&new_set,
&old_set,
(sigset_t
uset,
do_sigpending(&set,
&set,
sigsetsize))
sys_rt_sigpending((sigset_t
pad
member.
__put_user(from->si_ptr,
&to->si_ptr);
non-null,
signal's
suspension
dequeue_signal(tsk,
@uinfo:
(uts)
(copy_from_user(&ts,
sizeof(ts)))
(copy_siginfo_to_user(uinfo,
&info))
tiny,
do_send_specific(tgid,
reused.
Nor
impersonate
kill()/tgkill(),
SI_TKILL)
(task_pid_vnr(current)
(copy_from_user(&info,
do_rt_sigqueueinfo(pid,
do_rt_tgsigqueueinfo(tgid,
action)
sigemptyset(&mask);
sigaddset(&mask,
"Setting
ss_size;
uss,
(ss_flags
uoss,
do_sigaltstack(uss,
current_user_stack_pointer());
*uss)
squash
__put_user(sas_ss_flags(sp),
__put_user(t->sas_ss_size,
uss_ptr,
uoss_ptr)
__ARCH_WANT_SYS_SIGPENDING
sys_sigprocmask
arguments;
oset)
new_blocked;
new_blocked
sigaddsetmask(&new_blocked,
sigdelsetmask(&new_blocked,
set_current_blocked(&new_blocked);
new_sa,
old_sa;
compat_sigaction
restorer;
new_ka.sa.sa_handler
compat_ptr(handler);
new_ka.sa.sa_restorer
compat_ptr(restorer);
sizeof(mask));
old_sigaction
sizeof(*act))
&act->sa_handler)
&act->sa_restorer)
__get_user(new_ka.sa.sa_flags,
&act->sa_flags)
__get_user(mask,
&act->sa_mask))
__ARCH_HAS_KA_RESTORER
new_ka.ka_restorer
siginitset(&new_ka.sa.sa_mask,
sizeof(*oact))
&oact->sa_handler)
&oact->sa_restorer)
__put_user(old_ka.sa.sa_flags,
&oact->sa_flags)
__put_user(old_ka.sa.sa_mask.sig[0],
&oact->sa_mask))
compat_old_sigaction
CONFIG_SGETMASK_SYSCALL
Functionality
superseded
__ARCH_WANT_SYS_SIGNAL
*set)
sigsuspend(&newset);
old_sigset_t,
siginitset(&blocked,
sigsuspend(&blocked);
unused2,
exposing
kdb_send_sig_info(struct
kdb_prev_t
kdb_printf("Process
"the
DEFAULT_SYS_FILTER_MESSAGE
filter_op_ids
OP_OR,
OP_AND,
OP_GLOB,
OP_NE,
OP_LT,
OP_LE,
OP_GT,
OP_GE,
OP_BAND,
OP_NOT,
OP_NONE,
OP_OPEN_PAREN,
*string;
"!=",
"==",
">=",
"&",
FILT_ERR_TOO_MANY_OPERANDS,
FILT_ERR_OPERAND_TOO_LONG,
FILT_ERR_FIELD_NOT_FOUND,
FILT_ERR_ILLEGAL_FIELD_OP,
FILT_ERR_ILLEGAL_INTVAL,
FILT_ERR_MISSING_FIELD,
FILT_ERR_INVALID_FILTER,
FILT_ERR_IP_FIELD_ONLY,
FILT_ERR_ILLEGAL_NOT_OP,
value",
expression",
"Missing
u##size
pred->regex.match(addr,
0xffff;
searched
NULL-terminated
r->len)
r->len
memcmp(str
r->len,
comparison.
buff
filter_parse_regex(char
**search,
*not
*r
OP_GLOB)
MATCH_FULL:
MATCH_FRONT_ONLY:
MATCH_MIDDLE_ONLY:
MATCH_END_ONLY:
pred->not
*move
MOVE_DOWN;
get_parent;
(move)
MOVE_DOWN:
MOVE_UP_FROM_LEFT:
MOVE_UP_FROM_RIGHT:
circuit
((match
(!match
OP_AND))
(!WARN_ON_ONCE(!pred->fn))
pred->fn(pred,
folded.
folded
d->match
d->rec);
Optimization:
pred->op
.match
.rec
n_preds,
filter->n_preds;
(!root)
*string)
newlen;
append_filter_string(filter,
print_event_filter(struct
filter->filter_string)
print_subsystem_event_filter(struct
n_preds)
stack->index;
__pop_pred_stack(stack);
(!left
leaf,
right->index
FILTER_PRED_FOLD;
(filter->preds)
filter->a_preds
filter->n_preds
__free_preds(filter);
free_event_filter(struct
filter_pred_none;
call->filter
filter->n_preds,
pred);
filter_assign_type(const
strstr(type,
"char"))
'[')
*field)
(field->filter_type
(strcmp(field->name,
"ip"))
(!fn)
ps->infix.cnt--;
opstr[1]
(!strcmp(opstr,
ps->ops[i].string))
ps->ops[i].id;
(filter_opstack_empty(ps))
list_first_entry(&ps->opstack,
opstack_op,
opstack_op->op;
kmalloc(sizeof(*elt),
(!elt)
elt->operand
kfree(elt);
list_add_tail(&elt->list,
&ps->postfix);
in_string
ch))
clear_operand_string(ps);
filter_opstack_push(ps,
OP_OPEN_PAREN)
OP_AND
&pred;
operand2);
n_logical_preds
terminate.
.max
(*count)++;
(move
count_leafs(preds,
dry_run)
n_preds);
elt->operand;
operand2
&stack);
__pop_pred_stack(&stack);
filter->root
replace_preds(file->event_call,
filter_item->filter
__alloc_filter();
replace_filter_string(filter,
event_set_filtered_flag(file);
event_set_filter(file,
filters.
__free_filter(filter_item->filter);
err)
kfree(ps);
@filter_str
@filterp:
(always
@filter_str.
@set_str
*@filterp
create_filter_start(filter_str,
&ps,
create_filter_finish(ps);
create_event_filter(struct
@system:
Identical
apply_event_filter(struct
(!strcmp(strstrip(filter_string),
"0"))
filter_string,
apply_subsystem_event_filter(struct
dir->tr;
event->filter
**re;
kstrndup(buf,
white
re,
reset);
verify:
Checking
ftrace_function_check_pred(pred,
ftrace_function_set_filter(struct
filter->root,
*filter_str)
free_filter;
free_filter:
va,
vb,
vc,
vd,
ve,
vf,
vg,
.filter
.c
YES
test_filter_data_t
"a
"e
1"
"bcdefgh"),
"bd"),
"((((((((a
1))"
"bdfh"),
test_pred_visited
'%s',
tests,
Mode
user-defined
Berkeley
Packet
Socket
get/put
(other
usage;
18)
args[6];
sd->nr
(by
(pc
bounds.
BPF_LEN:
BPF_IMM;
ftest->k
seccomp_data);
BPF_RET
BPF_IMM:
BPF_MISC
BPF_MEM:
@syscall:
response
sane.
TIF_SECCOMP
TIF_SECCOMP);
ancestor.
BUG_ON(!mutex_is_locked(&current->signal->cred_guard_mutex));
for_each_thread(caller,
caller)
SECCOMP_MODE_FILTER
synchronized.
SECCOMP_MODE_DISABLED)
no_new_privs
ERR_PTR
privileged
current_user_ns(),
(!fp)
fprog
checker
fprog->len);
bpf_convert_filter(fp,
fprog->len,
&new_len);
kfree(fp);
free_prog:
user-supplied
(is_compat_task())
user_filter,
falls
total_insns
current->seccomp.filter;
penalty
*orig
tsk->seccomp.filter;
Reference
tsk->seccomp.filter
orig
in-process
@reason:
Forces
reason)
sizeof(info));
Secure
__secure_computing_strict(this_syscall);
(likely(phase1_result
seen.
SECCOMP_RET_DATA;
SECCOMP_RET_ACTION;
syscall_set_return_value(current,
task_pt_regs(current));
SECCOMP_PHASE1_OK;
do_exit(SIGSYS);
seccomp_phase1()
SECCOMP_MODE_STRICT:
SECCOMP_MODE_FILTER:
task_pt_regs(current);
seccomp_mode
(!seccomp_may_assign_mode(seccomp_mode))
seccomp_assign_mode(current,
seccomp_mode);
seccomp_set_mode_filter(unsigned
nnp
uargs)
do_seccomp(op,
syscall_enter_register(struct
syscall_exit_register(struct
prefixed
trace_get_syscall_nr(struct
syscall)
**stop;
(typeof(trace))ent;
trace->nr;
syscall_nr_to_meta(syscall);
ent->type)
entry->args[i],
entry->nb_args
offsetof(typeof(trace),
name),
len=0,
"\"");
*print_fmt;
(entry->enter_event
call->print_fmt
First:
__set_enter_print_fmt(entry,
(!print_fmt)
Second:
@print_fmt
print_fmt,
print_fmt;
SYSCALL_FIELD(int,
nr),
*ftrace_file;
handler's
rcu_read_lock_sched
ftrace_file
(!ftrace_file)
sys_data->enter_event->event.type,
syscall_get_arguments(current,
sys_data->nb_args,
sys_data->exit_event->event.type,
entry->ret
syscall_get_return_value(current,
(!tr->sys_refcount_enter)
(!tr->sys_refcount_exit)
"syscalls",
.raw_init
init_syscall_trace,
.fields
init_ftrace_syscalls(void)
NR_syscalls);
(!test_bit(syscall_nr,
ALIGN(size
*)perf_trace_buf_prepare(size,
rec->nr
perf_trace_buf_submit(rec,
(!sys_perf_refcount_enter)
activate"
"syscall
point");
set_bit(num,
enabled_perf_enter_syscalls);
clear_bit(num,
(!sys_perf_refcount_exit)
enabled_perf_exit_syscalls);
cond_syscall(sys_madvise);
Triplett
also:
<linux/srcu.h>
(jiffies)");
disable");
"Duration
fqs_holdoff,
fqs_stutter,
gp_cond,
gp_exp,
irqreader,
n_barrier_cbs,
testing");
nfakewriters,
onoff_holdoff,
"Shutdown
stall_cpu,
run/halt
test_boost,
"Test
test_boost_duration,
seconds.");
test_boost_interval,
test_no_idle_hz,
*torture_type
module_param(torture_type,
charp,
MODULE_PARM_DESC(torture_type,
"Type
...)");
**reader_tasks;
*stats_task;
rcu_torture_current_version;
DEFINE_PER_CPU(long
[RCU_TORTURE_PIPE_LEN
RTWS_SYNC
defined(MODULE)
RCUTORTURE_RUNNABLE_INIT
module_param(torture_runnable,
MODULE_PARM_DESC(torture_runnable,
rcu_can_boost()
rcu_trace_clock_local(void)
0ULL;
boost_starttime;
Coordinate
rcu_tortures
spin_lock_bh(&rcu_torture_lock);
list_del_init(p);
rcu_torture,
&rcu_torture_freelist);
ttype;
(*init)(void);
(*readlock)(void);
(*read_delay)(struct
oldstate);
*cur_ops;
*rrsp)
50;
(!(torture_random(rrsp)
shortdelay_us)))
atomic_inc(&rcu_torture_wcount[i]);
rp->rtort_mbtest
*rp;
(old_rp)
(rcu_torture_pipe_update_one(rp))
rcu_torture_free(rp);
rcu_torture_read_lock,
rcu_torture_read_unlock,
rcu_read_lock_bh();
rcu_read_unlock_bh();
it!
emits
synchronize_rcu_busted,
srcu
delay;
longdelay
(!delay)
call_srcu(&srcu_ctl,
*head))
c0,
(long)per_cpu_ptr(srcu_ctl.per_cpu_ref,
synchronize_rcu_tasks,
endtime;
boost-test
oldstarttime
jiffies);
checkwait;
endtime
failed");
delays,
Besides,
rare.
*rhp;
VERBOSE_TOROUT_STRING("rcu_torture_cbflood
fqs_burst_remaining
gp_cond1
gp_exp1
gp_normal1
gp_sync1
synctype[]
boot/sysfs
%s,\n",
cur_ops->name);
RTWS_COND_GET;
RTWS_EXP_SYNC;
RTWS_DEF_FREE;
(gp_normal
RTWS_SYNC;
RTWS_STOPPING;
torture_kthread_stopping("rcu_torture_writer");
old_rp
rp);
torture_random(&rand)
(can_expedite
0xff
rcu_gp_is_expedited());
(expediting
rcu_expedite_gp();
-expediting;
(torture_random(&rand)
pipeline
unused)
pipe_count;
cur_ops->readlock();
cur_ops->started();
rcu_trace_clock_local();
rcu_read_lock_sched_held()
srcu_read_lock_held(&srcu_ctl));
rcu_torture_writer
underway
(p->rtort_mbtest
atomic_inc(&n_rcu_torture_mberror);
cur_ops->read_delay(&rand);
p->rtort_pipe_count;
do_trace_rcu_torture_read(cur_ops->name,
&p->rtort_rcu,
completed);
__this_cpu_inc(rcu_torture_count[pipe_count]);
completed++;
__this_cpu_inc(rcu_torture_batch[completed]);
time!!!
accomplished
rcu_torture_stats
init/cleanup
rtcv_snap
per_cpu(rcu_torture_count,
per_cpu(rcu_torture_batch,
n_rcu_torture_barrier_error
n_rcu_torture_boost_ktrerror
n_rcu_torture_boost_rterror
n_rcu_torture_boost_failure
pr_cont("Reader
%ld",
rcu_torture_current_version
Writer
stat_interval
schedule_timeout_interruptible(stat_interval
rcu_torture_stats_print();
*cur_ops,
verbose=%d
stutter=%d
shutdown_secs=%d
onoff_holdoff=%d\n",
onoff_holdoff);
(boost_tasks[cpu]
mutex_lock(&boost_mutex);
cpu_to_node(cpu),
CPU-stall
pr_alert("rcu_torture_stall
stall_task);
newphase;
torture_must_stop());
atomic_set(&barrier_cbs_invoked,
atomic_set(&barrier_cbs_count,
n_barrier_cbs);
Implies
wait_event().
barrier_cbs_tasks
kzalloc(n_barrier_cbs
barrier_cbs_wq
(barrier_cbs_tasks
barrier_cbs_tasks[i]);
barrier_task);
rcutorture_record_test_transition();
(torture_cleanup_begin())
(reader_tasks)
kfree(reader_tasks);
nfakewriters;
fakewriter_tasks[i]);
fakewriter_tasks
fqs_task);
cbflood_task[i]);
((test_boost
cur_ops->can_boost)
test_boost
-After-
stopped!
FAILURE");
(torture_onoff_failures())
SUCCESS");
torture_cleanup_end();
conditions,
debug-objects
WARN:
rcu_torture_leak_cb);
harder
call_rcu(&rh2,
*torture_ops[]
(!torture_init_begin(torture_type,
&torture_runnable))
torturer
torture_ops[i];
(strcmp(torture_type,
ARRAY_SIZE(torture_ops))
pr_alert("rcu-torture:
types:");
pr_alert("
torture_ops[i]->name);
pr_alert("\n");
"goto
unwind"
point!!!
cpu)[i]
sizeof(reader_tasks[0]),
(reader_tasks
(stat_interval
torture_shutdown_init(shutdown_secs,
torture_onoff_init(onoff_holdoff
unwind:
firsterr;
Maintainer:
2000-2001
VERITAS
Amit
Kale
<amitkale@linsyssoft.com>
Rini
<trini@kernel.crashing.org>
LinSysSoft
Pvt.
Ltd.
2005-2009
Software,
Contributors
jason.wessel@windriver.com
<george@mvista.com>
Anurekh
Saxena
(anurekh.saxena@timesys.com)
Lake
Stevens
Instrument
(Glenn
Engel)
Kingdon,
Cygnus
Support.
stub:
<dave@gcom.com>,
Tigran
Aivazian
<tigran@sco.com>
"KGDB:
<linux/serial_core.h>
<linux/vmacache.h>
"debug_core.h"
Action
dbg_is_early
dbg_switch_cpu;
dbg_kdb_mode
Holds
ATOMIC_INIT(-1);
kgdb_do_roundup
management,
*bpt)
probe_kernel_write((char
flush_icache_range(addr,
pr_info("BP
kgdb_flush_swbreak_addr(kgdb_break[i].bpt_addr);
dbg_set_sw_break(unsigned
(breakno
BP_SET;
BP_BREAKPOINT;
BP_ACTIVE)
kgdb_arch_remove_breakpoint(&kgdb_break[i]);
dbg_remove_sw_break(unsigned
(!dbg_io_ops)
(kgdb_connected)
(atomic_read(&kgdb_setting_breakpoint))
$3#33
KDB\n");
pr_crit("Waiting
kgdb_arch_pc(ks->ex_vector,
occurred,
exception_level
kgdb_skipexception(ks->ex_vector,
pr_crit("re-enter
(exception_level
panic("Recursive
debugger");
sstep_tries
trace_on
exception_state;
DCPU_WANT_MASTER)
'trap
stepping.
kgdb_info[cpu].debuggerinfo
kgdb_info[cpu].task
kgdb_info[cpu].irq_depth
(raw_spin_trylock(&dbg_master_lock))
atomic_xchg(&kgdb_active,
cpu_master_loop;
(arch_kgdb_ops.correct_hw_break)
arch_kgdb_ops.correct_hw_break();
~(DCPU_WANT_MASTER
kgdb_info[cpu].enter_kgdb--;
deadlock,
(atomic_read(&kgdb_cpu_doing_single_step)
atomic_set(&kgdb_active,
raw_spin_unlock(&dbg_master_lock);
kgdb_restore;
ks->linux_regs))
(!kgdb_single_step)
slaves
(kgdb_do_roundup
atomic_read(&slaves_in_kgdb))
kgdb_single_step
(dbg_kdb_mode)
kgdb_sstep_pid
hierarchy:
arch_kgdb_ops.enable_nmi(0);
signo;
ks->err_code
Ouch,
(kgdb_info[ks->cpu].enter_kgdb
DCPU_WANT_MASTER);
arch_kgdb_ops.enable_nmi(1);
err_code;
print.
atomic_read(&kgdb_active)
dbg_kdb_mode)
CON_PRINTBUFFER
(!kgdb_connected)
KGDB\n");
sysrq_key_op
.help_msg
.action_msg
INT_MAX,
kgdb_arch_late();
board
kgdb_io_module_registered
&kgdb_panic_event_nb);
&sysrq_dbg_op);
kgdb_con_registered
kgdb_break_asap
spin_lock(&kgdb_registration_lock);
dbg_io_ops
kgdb_initial_breakpoint();
NO_POLL_CHAR)
wmb();
Sync
<linux/miscdevice.h>
<linux/swapops.h>
memset(&data->handle,
swap_type_of(swsusp_resume_device,
data->mode
O_RDONLY;
pm_notifier_call_chain(PM_HIBERNATION_PREPARE);
pm_notifier_call_chain(PM_POST_HIBERNATION);
wait_for_device_probe();
O_WRONLY;
pm_notifier_call_chain(PM_RESTORE_PREPARE);
pm_notifier_call_chain(PM_POST_RESTORE);
data->platform_support
free_all_swap_pages(data->swap);
(data->frozen)
O_RDONLY
*offp)
pg_offp
(!data->ready)
(!pg_offp)
pg_offp;
&pg_offp,
(!mutex_trylock(&pm_mutex))
data->ready)
SNAPSHOT_CREATE_IMAGE:
(data->mode
!data->frozen
thawed,
SNAPSHOT_GET_IMAGE_SIZE:
put_user(size,
SNAPSHOT_AVAIL_SWAP_SIZE:
SNAPSHOT_ALLOC_SWAP_PAGE:
MAX_SWAPFILES)
put_user(offset,
suspend_devices_and_enter(PM_SUSPEND_MEM);
hibernation_platform_enter();
SNAPSHOT_SET_SWAP_AREA:
resume_swap_area
swap_area;
*)arg,
compat_resume_swap_area
compat_loff_t
compat_ptr(arg);
snapshot_fops
miscdevice
<linux/anon_inodes.h>
<linux/license.h>
list_for_each_entry(tl,
list_node)
(tl->type
tl->ops;
list_add(&tl->list_node,
map->ops->map_free(map);
max_entries
O_RDWR
O_CLOEXEC);
f.file->private_data;
*uvalue
u64_to_ptr(attr->value);
kmalloc(map->value_size,
map->ops->map_lookup_elem(map,
map->value_size);
free_value:
kfree(value);
map->ops->map_update_elem(map,
map->ops->map_delete_elem(map,
next_key
free_next_key;
bpf_call
instructions:
prog->len;
free_used_maps(prog->aux);
bpf_prog_free(prog);
(IS_ERR(prog))
is_gpl;
GPL-ed
plain
prog->len
free_used_maps;
JITed
temporarily.
sizeof(attr);
decaying
x_i(t)
x_i(t_0)
definition,
loose
calc_load_tasks;
estimates
calc_load_fold_active(struct
nr_active;
solved
sample,
LOAD_FREQ
calc_load_idx;
calc_load_fold_active(this_rq);
this_rq->calc_load_update))
calc_load_fold_idle(void)
@x
relation
x^n
encoding
vector.
(frac_bits
(a0
e))
e^2
a3
geometric
folds
weights
calc_global_nohz(void)
atomic_long_read(&calc_load_tasks);
FIXED_1
EXP_1,
avenrun[1]
EXP_5,
avenrun[2]
EXP_15,
Fold
intervals,
(2^idx
cur_load
misses
busy,
have:
2^idx)^(n-1)
2^idx)
approximated
scale.
row
degradation
Example:
128};
12,
15,
112,
45,
cpu_load
backlog
(missed_updates
this_load,
load:
this_load;
CPU_LOAD_IDX_MAX;
scale)
old_load
new_load
example.
get_rq_runnable_load(struct
rq->cfs.runnable_load_avg;
seriously
update_idle_cpu_load(struct
get_rq_runnable_load(this_rq);
pending_updates;
up-to-date.
this_rq->last_load_update_tick)
this_rq->last_load_update_tick;
curr_jiffies;
pending_updates);
raw_spin_lock(&this_rq->lock);
update_cpu_load_active(struct
"tree.h"
rcu_qs_ctr);
*)file->private_data;
*r_start(struct
*r_next(struct
r_stop(struct
atomic_read(&rsp->barrier_cpu_count),
(!rdp->beenonline)
rdp->cpu),
rdp->qlen_lazy;
per_cpu(rcu_cpu_has_work,
r_open(inode,
(int)(jiffies
0xffff));
ulong2long(gpnum),
rsp->n_force_qs_ngp,
rnp++)
%d:%d
NULL],
gpage
rsp->gp_start;
"rcutorture
sequence:
rcutorture_testseq
*rcudir;
*retval;
rcudir
debugfs_create_dir("rcu",
(!rcudir)
debugfs_create_file("rcudata",
rcudir,
free_out:
McKenney");
MODULE_DESCRIPTION("Read-Copy
implementation");
CONFIG_GENERIC_SMP_IDLE_THREAD
idle_thread_set_boot_cpu(void)
idle_threads_init(void)
setup,
unpark
kfree(td);
BUG_ON(td->cpu
HP_THREAD_ACTIVE;
PTR_ERR(tsk);
smpboot_create_threads(unsigned
smpboot_unpark_threads(unsigned
smpboot_park_threads(unsigned
@plug_thread:
Hotplug
*plug_thread)
smpboot_destroy_threads(plug_thread);
CPU_UP_PREPARE
entirely,
CPU_UP_PREPARE);
Timeout
death,
proceed,
virtualized
jf_left
oldstate;
sleep_jf
update_state;
atomic_read()
died,
(atomic_cmpxchg(&per_cpu(cpu_hotplug_state,
CPU_BROKEN)
oldstate)
CPU_DEAD;
newstate)
2010-2012
nhit;
tp.args)
probe_arg)
(n)))
register_uprobe_event(struct
*tu);
unregister_uprobe_event(struct
bp_addr;
uprobe_dispatcher(struct
uretprobe_dispatcher(struct
adjust_stack_addr(unsigned
sizeof(long));
user_stack_pointer(regs);
DEFINE_FETCH_stack(type)
FETCH_FUNC_NAME(stack,
DEFINE_BASIC_FETCH_FUNCS(stack)
fetch_stack_string
fetch_stack_string_size
DEFINE_FETCH_memory(type)
DEFINE_BASIC_FETCH_FUNCS(memory)
null-terminated
string)(struct
get_rloc_data(dest);
*src
(!maxlen)
maxlen);
((u8
*)get_rloc_data(dest))[0]
make_data_rloc(0,
get_rloc_offs(rloc));
MAX_STRING_SIZE);
base_addr
*dest)\
[FETCH_TYPE_STRING]
__ASSIGN_FETCH_TYPE("string",
"__data_loc
char[]"),
[FETCH_TYPE_STRSIZE]
__ASSIGN_FETCH_TYPE("string_size",
string_size,
"u32"),
ASSIGN_FETCH_TYPE(u8,
ASSIGN_FETCH_TYPE(u16,
ASSIGN_FETCH_TYPE(u32,
ASSIGN_FETCH_TYPE(u64,
ASSIGN_FETCH_TYPE(s8,
ASSIGN_FETCH_TYPE(s16,
ASSIGN_FETCH_TYPE(s32,
ASSIGN_FETCH_TYPE(s64,
tu->consumer.ret_handler
(!event
!is_good_name(event))
!is_good_name(group))
(!tu)
kstrdup(event,
kstrdup(group,
kfree(tu->tp.call.name);
kfree(tu);
&uprobe_list,
probe_event:
free_trace_uprobe(tu);
probe_event
event(%d)\n",
uprobe:
buf[MAX_EVENT_NAME_LEN];
'r')
'-'.\n");
&argv[0][2];
'/');
event[-1]
(strlen(group)
pr_info("Group
(strlen(event)
(is_delete)
pr_info("Delete
name.\n");
%s/%s
exist.\n",
unregister_trace_uprobe(tu);
specified.\n");
(isdigit(argv[1][0]))
filename.\n");
(!arg)
*tail;
MAX_TRACE_ARGS;
strchr(argv[i],
kstrdup(argv[i],
argv[i];
omitted,
"argN"
"arg%d",
(!parg->name)
name.\n",
(!is_good_name(parg->name))
pr_info("Invalid
parg->name);
(traceprobe_conflict_field_name(parg->name,
pr_info("Argument[%d]
"another
field.\n",
argv[i]);
traceprobe_parse_probe_arg(arg,
parg,
pr_info("Parse
argument[%d].
iput(inode);
*probes_seq_start(struct
*probes_seq_next(struct
probes_seq_stop(struct
probes_seq_show(struct
'p';
tu->filename,
probes_seq_op
probes_seq_show
probes_open(struct
&probes_seq_op);
probes_write(struct
traceprobe_probes_write(file,
probes_open,
probes_write,
probes_profile_seq_show(struct
%-44s
%15lu\n",
profile_seq_op
probes_profile_seq_show
profile_open(struct
&profile_seq_op);
profile_open,
err_cpu;
per_cpu_ptr(uprobe_cpu_buffer,
long)per_cpu_ptr(uprobe_cpu_buffer,
cpu)->buf);
free_percpu(uprobe_cpu_buffer);
BUG_ON(!mutex_is_locked(&event_mutex));
access,
entry->vaddr[1]
&tu->tp.files,
__uprobe_trace_func(tu,
printers
entry->vaddr[0]);
*mm);
kmalloc(sizeof(*link),
link->file
list_add_tail_rcu(&link->list,
TP_FLAG_TRACE;
TP_FLAG_PROFILE;
WARN_ON(!uprobe_filter_is_empty(&tu->filter));
&tu->consumer);
uprobe_buffer_disable();
~TP_FLAG_TRACE;
list_del_rcu(&link->list);
vaddr[0],
__uprobe_perf_filter(&tu->filter,
write_lock(&tu->filter.rwlock);
(event->hw.target)
tu->filter.nr_systemwide
uprobe_filter_event(tu,
tu->filter.nr_systemwide;
write_unlock(&tu->filter.rwlock);
uprobe_apply(tu->inode,
&tu->consumer,
copy_process(),
current->mm
dup_mmap()
event->attr.enable_on_exec
uprobe_perf_close(tu,
sizeof(u64))
(WARN_ONCE(size
PERF_MAX_TRACE_SIZE,
enough"))
UPROBE_HANDLER_REMOVE;
__uprobe_perf_func(tu,
probe_event_enable(tu,
probe_event_disable(tu,
udd;
container_of(con,
udd.tu
udd.bp_addr
current->utask->vaddr
&udd;
(WARN_ON_ONCE(!uprobe_cpu_buffer))
__get_data_size(&tu->tp,
uprobe_buffer_get();
store_trace_args(esize,
&tu->tp,
uprobe_buffer_put(ucb);
INIT_LIST_HEAD(&call->class->fields);
call->class->reg
call->data
trace_add_event_call(call);
unregister_ftrace_event(&call->event);
Different
bpf_map_lookup_elem(u64
bpf_map_lookup_elem_proto
r3;
bpf_map_update_elem_proto
.arg3_type
ARG_ANYTHING,
bpf_map_delete_elem_proto
prandom_u32();
bpf_get_prandom_u32_proto
bpf_get_smp_processor_id_proto
<linux/prefetch.h>
__tracepoint_string
RCU_STATE_NAME(sname)
sabbr,
.level
300UL,
example)
EXPORT_SYMBOL_GPL(rcu_scheduler_active);
rcu_init_new_rnp(struct
*rnp_leaf);
invoke_rcu_callbacks(struct
delays.
rcutorture_vernum
correlating
permit
->lock,
rcu_sched_qs(void)
rcu_bh_qs(void)
DYNTICK_TASK_EXIT_IDLE,
->dynticks
rcu_sched_qs_mask
switch"));
(unlikely(raw_cpu_read(rcu_sched_qs_mask)))
rcu_momentary_dyntick_idle();
lightweight
blimit.
jiffies_till_sched_qs
rcu_start_gp_advanced(struct
force_qs_rnp(struct
(*f)(struct
*maxj),
force_quiescent_state(struct
BH
%#lx\n",
correlated
rcu_state_p;
*flags
*gpnum
*completed
(ACCESS_ONCE(rnp->completed)
*fp
not-yet-started
No,
offline)
(!user
idle_task(smp_processor_id());
trace_rcu_dyntick(TPS("Error
task"),
"Current
idle->pid,
idle->comm);
task!
do_nocb_deferred_wakeup(rdp);
sojourn.
WARN_ON_ONCE(atomic_read(&rdtp->dynticks)
adaptive-tickless
DYNTICK_TASK_NEST_VALUE)
rcu_eqs_enter_common(oldval,
user);
crowbar
rcu_idle_enter(void)
EXPORT_SYMBOL_GPL(rcu_idle_enter);
unbalanced
deserve,
hard.
infrequently
irreproducibly.
limitation.
warned.
rcu_irq_exit(void)
WARN_ON_ONCE(rdtp->dynticks_nesting
rcu_eqs_exit_common(oldval,
starting.
rcu_idle_exit(void)
EXPORT_SYMBOL_GPL(rcu_idle_exit);
mode!
rcu_irq_enter(void)
(oldval)
viewpoint,
rdtp->dynticks
permits
incby
WARN_ON_ONCE(rdtp->dynticks_nmi_nesting
0x1))
paying
__rcu_is_watching(void)
defined(CONFIG_PROVE_RCU)
defined(CONFIG_HOTPLUG_CPU)
preempted,
nature
CPU_DYING
CPU_DEAD
rcu_rnp_online_cpus(rnp))
&rdp->dynticks->dynticks);
rcu_sysidle_check_cpu(rdp,
TPS("dti"));
ACCESS_ONCE(rdp->gpwrap)
virtue
forth
merely
ULONG_CMP_GE(jiffies,
rdp->rsp->jiffies_resched))
ACCESS_ONCE(*rcrmp)
resched_cpu(rdp->cpu);
rdp->rsp->jiffies_resched
beating.
beat
Re-enable
j1;
j1
gpa;
ACCESS_ONCE(rsp->gp_activity);
starved
stalled
ACCESS_ONCE(rsp->jiffies_stall);
!rcu_gp_in_progress(rsp))
rat
Documentation/RCU/stallwarn.txt
print_cpu_stall_info_begin();
print_cpu_stall_info(rsp,
rnp->grplo
ndetected++;
print_cpu_stall_info_end();
cpu)->qlen;
q=%lu)\n",
(long)rsp->gpnum,
(long)rsp->completed,
totqlen);
rcu_dump_cpu_stacks(rsp);
start\n");
seen,
gpa,
rcu_check_gp_kthread_starvation(rsp);
force_quiescent_state(rsp);
CPU",
(t=%lu
rsp->gp_start,
machinery
unreasonable
check_cpu_stall(struct
gps;
js;
Lots
rsp->gpnum,
rsp->jiffies_stall,
opposite
(rcu_gp_in_progress(rsp)
ULONG_CMP_GE(j,
stall-warning
appearing
no-callbacks
RCU_NEXT_SIZE;
&rdp->nxtlist;
init_default_callback_list(rdp);
tag
fashion
->need_future_gp
->lock.
*rnp_root
rcu_cbs_completed(rdp->rsp,
0x1])
believe
rnp_root's
rnp->need_future_gp[c
0x1]++;
needed).
rnp_root)
raw_spin_lock(&rnp_root->lock);
rdp->nxtcompleted[i]
raw_spin_unlock(&rnp_root->lock);
just-ended
needmore;
0x1]
needmore
rsp->gp_kthread
proven
idempotent,
repeatedly.
saying
(!rdp->nxttail[RCU_NEXT_TAIL]
!*rdp->nxttail[RCU_DONE_TAIL])
c))
"i"
rcu_start_future_gp(rnp,
sublists
sublist.
rdp->nxttail[i];
rdp->nxttail[j]
rdp->nxttail[RCU_NEXT_TAIL])
!unlikely(ACCESS_ONCE(rdp->gpwrap)))
rnp->gpnum;
!!(rnp->qsmask
rdp->grpmask);
ACCESS_ONCE(rnp->gpnum)
__note_gp_changes(rsp,
Spurious
rsp->gpnum
(rnp->qsmaskinit
rnp->qsmaskinit;
rnp->wait_blkd_tasks
rcu_cleanup_dead_rnp(rnp);
excluded
rsp->completed;
rdp->mynode)
force_qs_rnp(rsp,
fqs_state;
gp_duration;
WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
needgp;
RCU_GP_FLAG_INIT;
TPS("newreq"));
rsp->gp_state
(!ACCESS_ONCE(rnp->qsmask)
!rcu_preempt_blocked_readers_cgp(rnp))
Deal
stray
warranted,
preparation
node's
endless
satisfied
period!
raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock,
snapshot,
gps,
!!rnp->gp_tasks);
rcu_report_qs_rsp(rsp,
(rcu_state_p
&rcu_sched_state
->gpnum.
ended,
beginnings.
no,
that).
orphanage.
->orphan_lock.
No-CBs
operations,
(rdp->nxtlist
orphanage,
partway
*rsp->orphan_donetail
init_callback_list(rdp);
Adopt
rsp->qlen_lazy;
rcu_idle_count_callbacks_posted();
(rsp->orphan_donelist
rdp->nxttail[RCU_DONE_TAIL])
rsp->orphan_donelist
&rsp->orphan_donelist;
(rsp->orphan_nxtlist
*rdp->nxttail[RCU_NEXT_TAIL]
rsp->orphan_nxtlist
&rsp->orphan_nxtlist;
rcu_cleanup_dying_cpu(struct
RCU_TRACE(struct
prematurely.
said,
needless
rnp_leaf;
rcu_cleanup_dying_idle_cpu(int
rnp.
rcu_cleanup_dead_cpu(int
count_lazy;
trace_rcu_batch_start(rsp->name,
trace_rcu_batch_end(rsp->name,
rcu_is_callbacks_kthread());
WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
bl
prefetch(next);
!!list,
Reinstate
worked
qhimark)
rcu_check_callbacks(int
scheduler-tick"));
modify,
(!in_softirq())
rcu_note_voluntary_context_switch(current);
(!rcu_gp_in_progress(rsp))
rcu_initiate_boost(rnp,
rcu_report_unblock_qs_rnp()
rnp->grphi;
cpu++,
*isidle
raw_spin_unlock(&rnp_old->fqslock);
rsp->n_force_qs_lh++;
rnp_old
raw_spin_unlock_irqrestore(&rnp_old->lock,
whom
belongs.
(cpu_needs_another_gp(rsp,
rcu_start_gp(rsp);
core"));
idleness.
LONG_MAX;
versa,
(__is_kfree_rcu_offset((unsigned
call_rcu_sched(struct
EXPORT_SYMBOL_GPL(call_rcu_sched);
call_rcu_bh(struct
EXPORT_SYMBOL_GPL(call_rcu_bh);
tagging
synchronize_rcu_bh().
overhead:
preempt_disable
differ
synchronize_sched(void)
(rcu_blocking_is_gp())
EXPORT_SYMBOL_GPL(synchronize_sched);
rcu_read_lock_bh()
get_state_synchronize_rcu()
Conditionally
wraps,
try_stop_cpus()
met
documentation
unfriendly
workloads,
common-case
restructure
updates,
thought
sync_sched_expedited_done
word.
cm;
cma
wrap,
(cpumask_weight(cm)
(ulong)firstsnap))
joy,
(trycount++
udelay(trycount
chance.
NOCB
implementation;
(If
al
hc
(!rdp->nxtlist)
done)
(atomic_dec_and_test(&rsp->barrier_cpu_count))
complete(&rsp->barrier_completion);
atomic_inc(&rsp->barrier_cpu_count);
references,
comment.
snap_done);
snap.
mutex_unlock(&rsp->barrier_mutex);
ACCESS_ONCE()
ACCESS_ONCE(rsp->n_barrier_done)
rsp->n_barrier_done
WARN_ON_ONCE((rsp->n_barrier_done
task).
Exclude
rcu_barrier_callback()
rdp->cpu
rdp->rsp
rcu_spawn_all_nocb_kthreads(cpu);
CPU_DYING_FROZEN:
PM_SUSPEND_PREPARE:
Expediting
PM_POST_SUSPEND:
99)
system).
prohibited
rcu_scheduler_starting(void)
WARN_ON(nr_context_switches()
(IS_ENABLED(CONFIG_RCU_FANOUT_EXACT))
rcu_fanout_leaf;
rsp->levelspread[i]
CONFIG_RCU_FANOUT;
cprv
ccur
rsp->levelcnt[i];
buf[]
fl_mask
init!
num_rcu_lvl[i];
rsp->level[i
j++,
rnp->grpnum
rnp->grpmask
rnp->parent
rsp->levelspread[i
geometry
accurate,
nr_cpu_ids);
rcu_capacity[0]
rcu_capacity[i
leaf-level
num_rcu_lvl[j]
rcu_init(void)
rcu_early_boot_tests();
open_softirq(RCU_SOFTIRQ,
rcu_process_callbacks);
rcu_cpu_notify(NULL,
*)(long)cpu);
<linux/lglock.h>
lock_acquire_shared(&lg->lock_dep_map,
this_cpu_ptr(lg->lock);
watchdog:
detector.
'watchdog_enabled'.
sysctl_softlockup_all_cpu_backtrace;
sysctl_softlockup_all_cpu_backtrace
watchdog_running;
soft-lockup
~NMI_WATCHDOG_ENABLED;
"panic",
softlockup_panic
!!simple_strtol(str,
~=
generates
((u64)NSEC_PER_SEC
__this_cpu_write(watchdog_touch_ts,
__this_cpu_write(softlockup_touch_sync,
touch_ts
event->hw.interrupts
LOCKUP
__this_cpu_write(hard_watchdog_warn,
*hrtimer)
duration;
hrtimer_forward_now(hrtimer,
paused
vm
warns
quiet
reported.
(softlockup_all_cpu_backtrace)
CPU#%d
tasks");
hrtimer_start(hrtimer,
ns_to_ktime(sample_period),
(4
non-obvious
(!(watchdog_enabled
NMI_WATCHDOG_ENABLED))
cpu0_err
_any_
periodically.
enabled\n",
oprofile
watchdog_nmi_enable_all(void)
watchdog_nmi_disable_all(void)
watchdog,
watchdog_enable,
unqueued
Updating
'on
fly'.
Watchdog
watchdog_enable_all_cpus();
table->data
*watchdog_param
mutex_lock(&watchdog_proc_mutex);
which;
proc_watchdog_update();
mutex_unlock(&watchdog_proc_mutex);
set_sample_period();
1998,2001-2005
<linux/bio.h>
BIO
@rw:
WRITE.
@bio_chain:
bio.
reading,
sector;
lock_page(page);
submit_bio(bio_rw,
bio);
wait_on_page_locked(page);
READ)
get_page(page);
*bio_chain;
*bio_chain
hib_bio_read_page(pgoff_t
hib_resume_bdev,
page_off
9),
virt_to_page(addr),
hib_bio_write_page(pgoff_t
hib_wait_on_bio_chain(struct
ipi
Jens
Axboe
CSD_FLAG_LOCK
CSD_FLAG_SYNCHRONOUS
flush_smp_call_function_queue(bool
free_cpumask_var(cfd->cpumask);
sent.
*cpu
*)(long)smp_processor_id();
observing
CSD_FLAG_LOCK;
->flags
*csd,
((unsigned)cpu
csd->func
csd->info
architecture,
WRT
&per_cpu(call_single_queue,
arch_send_call_function_single_ipi(cpu);
'false'.
llist)
smp_call_function_single
smp_call_function_single(int
processor,
WARN_ON_ONCE(cpu_online(this_cpu)
irqs_disabled()
csd_lock(csd);
generic_exec_single(cpu,
csd,
EXPORT_SYMBOL(smp_call_function_single);
smp_call_function_single(),
pre-allocated
smp_call_function_single_async(int
disabled!
(cpu_online(cpu))
subset).
fastpath.
Ignoring
cfd->cpumask)
per_cpu_ptr(cfd->csd,
setup_max_cpus
activation
Command-line
arch_disable_smp_support();
--RR
cpu_up(cpu);
processors.
early_boot_irqs_disabled
EXPORT_SYMBOL(on_each_cpu);
cpumask,
on_each_cpu_mask(const
EXPORT_SYMBOL(on_each_cpu_mask);
cond_func
on_each_cpu_cond(bool
(*cond_func)(int
gfp_flags)
(cond_func(cpu,
EXPORT_SYMBOL(on_each_cpu_cond);
pm_idle
wake_up_all_idle_cpus
sleepers
deficit.
sooner,
buddies
contention,
Enabled
<linux/percpu-rwsem.h>
*brw,
brw->fast_read_ctr
inc/dec
down_read()
percpu_up_read()
(likely(update_fast_ctr(brw,
per_cpu(*brw->fast_read_ctr,
writers,
(jea->key
jeb->key)
long)start)
sort(start,
jump_label_update(struct
(!jump_label_get_branch_default(key))
atomic_inc(&key->enabled);
work.work);
__static_key_slow_dec(&key->key,
(entry->code
entry->code
iter++;
jump_label_type
arch_jump_label_transform(entry,
(jump_label_t)(unsigned
long)key);
entry++)
invalidates
kernel_text_address()
__stop___jump_table;
jump_label_sort_entries(iter_start,
iter_stop);
*iterk;
iterk
arch_jump_label_transform_static(iter,
(iterk
iterk;
key->entries
JUMP_LABEL_TRUE_BRANCH.
*((unsigned
*)&key->entries)
long)iter;
__module_text_address((unsigned
long)start);
optimal
(iter_start
(__module_address(iter->key)
jlm->mod
jlm->next;
jump_label_del_module(mod);
overlap,
<linux/keyctl.h>
kdebug(FMT,
"FMT"\n",
current->pid
,##__VA_ARGS__)
.usage
ATOMIC_INIT(2),
.magic
.uid
.user_ns
cred);
(cred->magic
panic("CRED:
put_cred_rcu()
cred->magic,
key_put(cred->request_key_auth);
put_user_ns(cred->user_ns);
BUG_ON(cred
tsk->real_cred,
tsk->cred,
atomic_read(&tsk->cred->usage),
read_cred_subscribers(tsk->cred));
tsk->real_cred
validate_creds(cred);
alter_cred_subscribers(cred,
permitted.
cred;
credentials,
modification.
kmem_cache_alloc(cred_jar,
%p",
set_cred_subscribers(new,
get_group_info(new->group_info);
get_uid(new->user);
get_user_ns(new->user_ns);
new->security
(security_prepare_creds(new,
key_put(new->thread_keyring);
key_put(new->process_keyring);
circumstances
p->real_cred
get_cred(p->cred);
CLONE_NEWUSER)
inherit.
CLONE_THREAD))
atomic_inc(&new->user->processes);
subsets
subset_ns->parent)
say,
BUG_ON(atomic_read(&new->usage)
subj
(task->mm)
(new->user
old->user)
(!uid_eq(new->uid,
old->suid)
proc_id_connector(task,
old->sgid)
obj
Discard
current->cred;
rcu_assign_pointer(current->cred,
atomic_read(&old->usage),
read_cred_subscribers(old));
discarding
SLAB_HWCACHE_ALIGN|SLAB_PANIC,
@daemon
that;
(daemon)
@secid:
interpreted
inode.
inode->i_uid;
inode->i_gid;
0xffffff00)
(POISON_FREE
%d,%d,%d,%d
cred->uid),
cred->euid),
cred->suid),
cred->gid),
cred->egid),
cred->sgid),
->security
credentials\n");
%s:%u\n",
(tsk->cred
tsk->real_cred)
creds_are_invalid(tsk->cred)))
invalid_creds;
Real
__FILE__,
__LINE__);
Wrapper
nicely
<linux/prctl.h>
egid)
euid)
suid)
ruidp,
euidp,
suidp)
suid;
ruid
(!(retval
put_user(ruid,
!(retval
put_user(euid,
put_user(suid,
suidp);
sgid)
rgidp,
egidp,
sgidp)
sgid;
rgid
put_user(rgid,
put_user(egid,
put_user(sgid,
sgidp);
*grouplist,
*grouplist)
make_kgid(user_ns,
cred->group_info->ngroups;
(gidsetsize)
cred->group_info))
(!may_setgroups())
((unsigned)gidsetsize
NGROUPS_MAX)
groups_alloc(gidsetsize);
grouplist);
set_current_groups(group_info);
high2lowuid(from_kuid_munged(current_user_ns(),
high2lowgid(from_kgid_munged(current_user_ns(),
debug_mutex_wake_waiter(struct
debug_mutex_free_waiter(struct
debug_mutex_add_waiter(struct
*ti);
mutex_remove_waiter(struct
debug_mutex_unlock(struct
debug_mutex_init(struct
*key);
spin_lock_mutex(lock,
l);
spin_unlock_mutex(lock,
preempt_check_resched();
Oracle
~WQ_FLAG_EXCLUSIVE;
WQ_FLAG_EXCLUSIVE;
(nr_exclusive
TASK_RUNNING.
WQ_FLAG_EXCLUSIVE)
__wake_up
@nr_exclusive:
wake-one
wake-many
ie.
WF_SYNC
wait-queue
SMP,
wait->private
emptiness
list).
Wakes
aborts
WQ_FLAG_WOKEN)
woken_wake_function()
WQ_FLAG_WOKEN
LOCK
set_mb()
*wait_bit
wait_bit_queue,
(wait_bit->key.flags
key->flags
wait_bit->key.bit_nr
key->bit_nr
nonblocking)
prepare_to_wait(wq,
(test_bit(q->key.bit_nr,
q->key.flags))
__wait_on_bit(wq,
waitqueue_active()
internally,
*zone
atomic_read(val)
(*action)(atomic_t
*val;
.key
Abuse
(time_after_eq(now,
word->timeout))
interrupts:
IRQ_BITMAP_BITS);
irqs_resend);
check_irq_resend(struct
confused.
(irq_settings_is_level(desc))
trace_init_global_iter(&iter);
old_userobj
~TRACE_ITER_SYM_USEROBJ;
buffer:\n");
memset(&iter.seq,
iter.iter_flags
iter.pos
iter.buffer_iter[cpu]
ring_buffer_read_prepare(iter.trace_buffer->buffer,
tracing_iter_reset(&iter,
kdb_printf("---------------------------------\n");
print_trace_line(&iter);
trace_printk_seq(&iter.seq);
(ftrace
empty)\n");
atomic_dec(&per_cpu_ptr(iter.trace_buffer->data,
simple_strtol(argv[2],
"rwsem.h"
__down_read_trylock,
__down_read);
rwsem_release(&sem->dep_map,
__up_read(sem);
rwsem_clear_owner(sem);
*nest)
nest,
Serge
Hallyn
<serue@us.ibm.com>
<net/net_namespace.h>
&init_pid_ns,
CONFIG_NET
*nsproxy;
(nsproxy)
*new_fs)
new_fs);
kmem_cache_free(nsproxy_cachep,
*new_ns;
(CLONE_NEWNS
CLONE_NEWUTS
undolist:
unreachable.
CLONE_SYSVSEM
(CLONE_NEWIPC
CLONE_SYSVSEM))
new_ns
tsk->fs);
new_ns;
unshare.
*user_ns;
task_lock(p);
task_unlock(p);
new_nsproxy);
fput(file);
Tests
USECS
distributed,
terms.
udelay_test_usecs;
udelay_test_iterations
DEFAULT_ITERATIONS;
uint32_t
udelay
do_div(avg,
iters);
avg,
ts.tv_sec,
Alan
Stern
flexible;
(bp->attr.bp_type
*tsk_pinned
type)->tsk_pinned;
*slots,
*cpumask
cpumask_of_bp(bp);
(!bp->hw.target)
task_bp_pinned(cpu,
slots->pinned
(per_cpu(info->flexible,
roughly
((per_cpu(info->flexible,
find_slot_idx(bp);
hw_breakpoint_weight(bp);
toggle_bp_slot(bp,
mutex_lock(&nr_bp_mutex);
__reserve_bp_slot(bp);
mutex_unlock(&nr_bp_mutex);
__release_bp_slot(bp);
slots.
(mutex_is_locked(&nr_bp_mutex))
validate_hw_breakpoint(bp);
'task_struct'
perf_event_create_kernel_counter(attr,
modify_user_hw_breakpoint
@bp:
targeting
bp->attr.bp_addr
bp->attr.bp_len
perf_event_enable(bp);
@return
0x7fffffff
devm_free_irq()
*dr;
devres_alloc(devm_irq_release,
irq_devres),
request_threaded_irq(irq,
devname,
dr->irq
dr->dev_id
free_irq().
match_data
WARN_ON(devres_destroy(dev,
&match_data));
(EDF)
Constant
Bandwidth
Server
deadlines.
Juri
Lelli
def_dl_bandwidth;
&dl_se->rb_node;
init_dl_bandwidth(struct
init_dl_bw(struct
(global_rt_runtime()
init_dl_rq(struct
rq->rd->dlo_mask);
Matched
inc_dl_migration(struct
update_dl_migration(dl_rq);
dec_dl_migration(struct
sched_rt.c,
enqueue_pushable_dl_task(struct
pushable_dl_tasks);
(dl_entity_preempt(&p->dl,
dl_rq->pushable_dl_tasks_leftmost
&dl_rq->pushable_dl_tasks_root);
dequeue_pushable_dl_task(struct
RB_CLEAR_NODE(&p->pushable_dl_tasks);
push_dl_task(struct
need_pull_dl_task(struct
*find_lock_later_rq(struct
*later_rq
(!later_rq)
later_rq->cpu);
activate_task(later_rq,
resched_curr(later_rq);
enqueue_task_dl(struct
__dequeue_task_dl(struct
check_preempt_curr_dl(struct
deadline;
to!)
reconcile
entities.
strategy
deals
overcome
entity.
rq->clock.
Anyway,
rq_clock(rq)))
printk_deferred_once("sched:
DL
enqueued)
exceeding
CBS
bandwidth,
(deadline
dl_runtime
dl_period
recycle
Notice
dl_deadline
*pi_se,
sides
shuffling
multiplications
DL_SCALE)
DL_SCALE);
left);
past,
arrival
pi_se,
enforcement
instant
hard;
(boosted)
e.g.,
ktime_to_ns(ktime_sub(hard,
soft));
reservation
(through
deactivate_task()
prev->on_rq
Replenish
replenish_dl_entity(dl_se,
enqueue_task_dl(rq,
(dl_task(rq->curr))
check_preempt_curr_dl(rq,
init_dl_task_timer(struct
sched_rt_bandwidth_account(struct
(provided
solution,
__dequeue_task_dl(rq,
&rq->dl))
quota.
servers
*pick_next_earliest_dl_task(struct
inc_dl_deadline(struct
dl_time_before(deadline,
next_deadline(rq);
dec_dl_deadline(struct
dl_rq->rb_leftmost;
dl_task_of(dl_se)->prio;
WARN_ON(!dl_prio(prio));
&dl_rq->rb_root);
*pi_task
rt_mutex_get_top_task(p);
(pi_task
clearly
reclaiming
planned
(thanks
p->dl.runtime
microscopic
cost.
find_later_rq(struct
ACCESS_ONCE(rq->curr);
(curr->nr_cpus_allowed
!dl_entity_preempt(&p->dl,
(target
cpudl_find(&rq->rd->cpudl,
migratable,
rq->curr->dl.deadline)
!test_tsk_need_resched(rq->curr))
start_hrtick_dl(struct
hrtick_start(rq,
!CONFIG_SCHED_HRTICK
rb_entry(left,
*dl_se;
prev))
pull_dl_task(rq);
pull_rt_task()
re-acquire)
rq->lock;
task_on_rq_queued(rq->stop))
put_prev_task().
dl_se
start_hrtick_dl(rq,
set_post_schedule(rq);
resulted
achieved
sched_fork()
&p->dl.dl_timer;
*dl_b
dl_bw_of(task_cpu(p));
task_cpu(task);
(task->nr_cpus_allowed
later_mask);
later_mask))
cheaper
cpumask_any(later_mask);
tries;
(tries
tries++)
(double_lock_balance(rq,
(unlikely(task_rq(task)
!task_on_rq_queued(task)))
BUG_ON(rq->cpu
BUG_ON(task_current(rq,
p));
BUG_ON(p->nr_cpus_allowed
BUG_ON(!task_on_rq_queued(p));
BUG_ON(!dl_task(p));
*next_task;
(!rq->dl.overloaded)
pick_next_pushable_dl_task(rq);
(!next_task)
(unlikely(next_task
rq->curr))
(dl_task(rq->curr)
get_task_struct(next_task);
(task_cpu(next_task)
next_task)
set_task_cpu(next_task,
this_rq->cpu,
dmin
double_lock_balance(this_rq,
dl_time_before(p->dl.deadline,
src_rq->curr);
WARN_ON(!task_on_rq_queued(p));
activate_task(this_rq,
double_unlock_balance(this_rq,
push_dl_tasks(rq);
!test_tsk_need_resched(rq->curr)
Migrating
((p->nr_cpus_allowed
(rq->dl.overloaded)
i),
while.
pi_lock
lockdep_assert_held(&p->pi_lock);
__dl_clear_params(p);
task_rq(p))
(maybe
.set_cpus_allowed
.post_schedule
.task_woken
.task_fork
print_dl_rq(struct
*dl_rq);
print_dl_stats(struct
Fair
Dmitry
Scaled
2007,
Adaptive
<linux/latencytop.h>
<linux/migrate.h>
sysctl_sched_latency
6000000ULL;
configurable
SCHED_TUNABLESCALING_LOG
linear,
sysctl_sched_min_granularity
750000ULL;
sched_nr_latency
fork,
decoupled
1000000UL;
sysctl_sched_migration_cost
slice)
msec,
inc;
factor;
get_update_sysctl_factor();
WMULT_CONST
WMULT_SHIFT
*lw)
lw
\e
delta_exec,
(fact
32x32->64
fair_sched_class;
*rq_of(struct
cfs_rq->rq;
entity_is_task(se)
*task_of(struct
container_of(se,
*task_cfs_rq(struct
*cfs_rq_of(struct
se->cfs_rq;
*group_cfs_rq(struct
force_update);
list_add_leaf_cfs_rq(struct
&rq_of(cfs_rq)->leaf_cfs_rq_list);
cfs_rq->on_list
list_del_leaf_cfs_rq(struct
thr'
list_for_each_entry_rcu(cfs_rq,
*parent_entity(struct
find_matching_se(struct
**se,
**pse)
i.e
se_depth
pse_depth
parent_entity(*se);
parent_entity(*pse);
vruntime)
(s64)(vruntime
vruntime;
min_vruntime,
(cfs_rq->rb_leftmost)
se->vruntime);
backwards.
cfs_rq->min_vruntime
vruntime);
cfs_rq->min_vruntime_copy
rbtree:
frequently
cfs_rq->rb_leftmost
&cfs_rq->tasks_timeline);
rb_next(&se->run_node);
*__pick_first_entity(struct
*__pick_last_entity(struct
nl)
sysctl_sched_latency;
sched_nr_latency;
wall-time
load);
select_idle_sibling(struct
__update_task_entity_contrib(struct
__update_task_entity_utilization(struct
curr->sum_exec_runtime
update_min_vruntime(cfs_rq);
schedstat_set(se->statistics.wait_start,
update_stats_wait_start(cfs_rq,
se->statistics.wait_start);
update_stats_wait_end(cfs_rq,
se->exec_start
Approximate
sysctl_numa_balancing_scan_period_min
sysctl_numa_balancing_scan_period_max
RSS
PTE
scanner
windows
task_nr_scan_windows(p);
max_t(unsigned
Watch
account_numa_enqueue(struct
rq->nr_numa_running
rq->nr_preferred_running
account_numa_dequeue(struct
NR_NUMA_HINT_FAULT_BUCKETS
private,
numa_faults_stats
nr_node_ids
p->numa_faults[task_faults_idx(NUMA_MEM,
(!p->numa_group)
p->numa_group->faults[task_faults_idx(NUMA_MEM,
group->faults_cpu[task_faults_idx(NUMA_MEM,
connected.
NUMA_DIRECT)
introducing
8.
furthest
(dist
sched_max_numa_distance
NUMA_BACKPLANE
nearby
NUMA_GLUELESS_MESH)
(sched_max_numa_distance
score;
evenly
(!total_faults)
score_nearby_nodes(p,
dist,
dst_cpu)
temporal
task<->page
relation.
~
sufficiently
pattern.
ng->active_nodes))
ones,
1/4
hysteresis
weighted_cpuload(const
source_load(int
target_load(int
capacity_of(int
update_sg_lb_stats
rq->nr_running;
capacity_of(cpu);
cpus++;
smt
imp;
env->dst_cpu;
src_load,
dst_capacity;
corrected
load_a
dst_load;
load_b
src_load;
threshold.
brings
orig_src_load
swap,
overshoot
balance?
src.
groupimp)
env->p)
differential
incurred
weights.
tiny
differences.
(cur->numa_group)
group_weight(cur,
(!cur)
Balance
move.
assign:
.imbalance_pct
(sd->imbalance_pct
Cpusets
boundaries.
env.dst_nid
env.dist
node_distance(env.src_nid,
taskweight
groupweight
env.dst_nid,
taskweight;
update_numa_stats(&env.dst_stats,
nid.
task_numa_find_cpu(&env,
(nid
workload's
p->numa_group->active_nodes))
sched_setnuma(p,
placed.
env.best_cpu);
trace_sched_stick_numa(p,
env.src_cpu,
located.
smarter
group_faults_cpu(numa_group,
numa_group->active_nodes);
NUMA_PERIOD_SLOTS
p->numa_faults_locality[1];
p->mm->numa_next_scan
msecs_to_jiffies(p->numa_scan_period);
(scan
ratio)
speaking
intent
diff,
memset(p->numa_faults_locality,
sizeof(p->numa_faults_locality));
calculations.
p->se.exec_start;
*period
p->last_sum_exec_runtime
p->last_task_numa_placement
max_score
max_node
Finding
untangle
tricks
max_group
nodes)
this_group;
round,
max_group_nid
max_group_faults
p->numa_scan_seq
p->numa_scan_period_max
task_scan_max(p);
group_faults
mem_idx
p->numa_faults[membuf_idx]
p->numa_faults[mem_idx]
Normalize
div64_u64(runtime
p->numa_faults[cpu_idx]
f_diff;
p->total_numa_faults
numa_migrate_preferred(p);
p->pid;
Second
grp->nr_tasks++;
rcu_assign_pointer(p->numa_group,
my_grp
(my_grp->nr_tasks
current->mm)
collisions
BUG_ON(irqs_disabled());
PROT_NONE
mem_node,
faulting
basis
(!priv
local.
priv)]
migrate,
nr_pte_updates
msecs_to_jiffies(sysctl_numa_balancing_scan_delay);
win
p->node_stamp
(!pages)
(!vma)
reset_ptenuma_scan(p);
max(start,
vma->vm_start);
min(end,
vma->vm_end);
VMA
task_tick_numa(struct
curr->node_stamp
(!parent_entity(se))
task_of(se));
cfs_rq->tg_load_contrib;
cfs_rq->load.weight;
calc_cfs_shares(struct
calc_tg_weight(tg,
cfs_rq);
(tg->shares
(shares
MIN_SHARES)
MIN_SHARES;
account_entity_dequeue(cfs_rq,
update_load_set(&se->load,
account_entity_enqueue(cfs_rq,
update_cfs_shares(struct
tg);
LOAD_AVG_PERIOD
Precomputed
0xffffffff,
y^32
0.5
y^PERIOD
LOAD_AVG_PERIOD))
1024*y^n
runnable_avg_yN_sum[n];
->|<-
ago)
u_0
multiplying
update:
u_0`
arch_scale_freq_capacity(NULL,
backwards,
sa->last_runnable_update
boundary,
accrue
runnable_contrib
entity's
decays;
__update_cfs_rq_tg_load_contrib(struct
tg_contrib;
tg_contrib
Aggregate
contributions.
__update_tg_runnable_avg(struct
cfs_rq->tg_runnable_contrib
__update_group_entity_contrib(struct
<1
expensive,
disjoint
overlap.
larger,
converge
(se->avg.avg_period
scale_load(contrib);
old_contrib
old_contrib;
update_entity_load_avg(struct
update_cfs_rq)
utilization_delta;
cfs_rq_clock_task()
se))
removed_load;
enqueue_entity_load_avg(struct
Newly
se->avg.last_runnable_update
clock_task
readable.
on-rq
__synchronize_entity_decay(se);
(wakeup)
se->avg.load_avg_contrib);
dequeue_entity_load_avg(struct
statistic.
update_rq_runnable_avg(this_rq,
se->statistics.sleep_start
se->statistics.sum_sleep_runtime
account_scheduler_latency(tsk,
se->statistics.block_start
thresh
check_spread(cfs_rq,
__enqueue_entity(cfs_rq,
se->on_rq
(tsk->state
TASK_INTERRUPTIBLE)
rq_clock(rq_of(cfs_rq));
__dequeue_entity(cfs_rq,
sched_slice(cfs_rq,
ideal_runtime)
narrow
slice.
se->sum_exec_runtime
wakeup_preempt_entity(struct
"next"
buddy,
second;
(prev->on_rq)
cfs_bandwidth_used(void)
cfs_bandwidth_usage_inc(void)
cfs_bandwidth_usage_dec(void)
cfs
__refill_cfs_bandwidth_runtime(struct
cfs_b->runtime_expires
*tg_cfs_bandwidth(struct
cfs_rq->throttled_clock_task;
sched_cfs_bandwidth_slice()
(!cfs_b->timer_active)
(cfs_b->runtime
amount;
cfs_rq->runtime_expires)
cfs_rq_throttled(struct
cfs_bandwidth_used()
throttled_lb_pair(struct
tg->cfs_rq[cpu_of(rq)];
(!cfs_rq->throttle_count)
walk_tg_tree_from(cfs_rq->tg,
*)rq);
task_delta
cfs_rq->h_nr_running;
task_delta);
cfs_rq->throttled
unthrottle_cfs_rq(struct
cfs_rq->tg->se[cpu_of(rq)];
(enqueue)
enqueue_entity(cfs_rq,
remaining,
starting_runtime
overrun)
throttled;
out_deactivate;
!list_empty(&cfs_b->throttled_cfs_rq);
amounts
cfs_b->runtime;
distribute_cfs_runtime(cfs_b,
min(runtime,
cfs_b->runtime);
cfs_bandwidth_slack_period
min_expire)
call-back
(runtime_refresh_within(cfs_b,
slack_runtime;
(!cfs_rq->runtime_enabled
rq->locks
respective
cfs_rqs.
throttle_cfs_rq(cfs_rq);
conditionally
put_prev_entity()
set_curr_task),
cfs_bandwidth,
cfs_b->period);
(!overrun)
HRTIMER_NORESTART
cfs_b->period
init_cfs_rq_runtime(struct
__start_cfs_bandwidth(struct
cfs_b->timer_active)
destroy_cfs_bandwidth(struct
update_runtime_enabled(struct
unthrottle_offline_cfs_rqs(struct
hrtick_start_fair(struct
hrtick_update(struct
evaluation
h_nr_running
cfs_rq->h_nr_running++;
rq->nr_running);
hrtick_update(rq);
set_next_buddy(struct
decreased.
DEQUEUE_SLEEP;
cfs_rq->h_nr_running--;
besides
pick_next
parent_entity(se);
conservatively.
!sched_feat(LB_BIAS))
current->wakee_flip_decay_ts
min_vruntime_copy;
perfectly
shares.
@wg
s_i
rw_j
Suppose
s'_i
dw_i
(s'_i
wg)
(!tg->parent)
non-cgroup
W;
wg
(w
(wl
wakee
alone.
prev_eff_load;
balanced;
sd->wake_idx;
(sync)
-weight);
task_group(p);
situations,
this_cpu.
ULONG_MAX,
local_group;
local_group
sched_group_cpus(group));
Tally
source_load(i,
target_load(i,
(avg_load
min_exit_latency
least_loaded_cpu
Traverse
(idle_cpu(i))
idle->exit_latency
min_exit_latency)
irrespective
cache.
this_cpu))
sched_domain.
for_each_lower_domain(sd)
cpumask_first_and(sched_group_cpus(sg),
cfs.utilization_load_avg
capping
(usage
capacity)
SD_BALANCE_WAKE,
SD_BALANCE_FORK,
SD_WAKE_AFFINE
want_affine
*group;
sd->span_weight;
task_cpu(p)
units.
light
penalizing
(vdiff
gran)
(entity_is_task(se)
unlikely(task_of(se)->policy
task_cfs_rq(curr);
next_buddy_marked
set_next_buddy(pse);
catches
(test_tsk_need_resched(curr))
preempt;
(unlikely(p->policy
rq->idle))
(!cfs_rq->nr_running)
simple;
forget
pick_next_entity(cfs_rq,
(cfs_rq);
&prev->se;
pse);
pse
put_prev_entity(cfs_rq,
(new_tasks
descheduled
&curr->se;
preempt)
fairness
W_i,n
instantaneous
w_i,j
j-th
weight:
2^n
factors
avg(W/C),
continuous
discrete
fun
infeasible
log_2
balancer.
2^(k+1)
you'll
convergence
(8)
S_k
w_i,j,k
(10)
property.
max_load_balance_interval
LBF_DST_PINNED
0x04
dst_cpu;
imbalance;
tasks;
(p->sched_class
(sysctl_sched_migration_cost
migrate_improves_locality(struct
*numa_group
rcu_dereference(p->numa_group);
!(env->sd->flags
SD_NUMA))
src_nid
cpu_to_node(env->src_cpu);
cpu_to_node(env->dst_cpu);
(numa_group)
(node_isset(src_nid,
(node_isset(dst_nid,
migrate_degrades_locality(struct
revisit
pulling
src_cpu.
re-select
env's
for_each_cpu_and(cpu,
schedstat_inc(env->sd,
detach_task()
(!can_migrate_task(p,
detach_task(p,
detach_tasks()
busiest_rq,
*tasks
(!list_empty(tasks))
list_first_entry(tasks,
se.group_node);
(env->loop
latency,
tg->cfs_rq[cpu];
out-of-order
update_blocked_averages(int
continually
extending
top-down
(cfs_rq->last_h_load_update
cfs_rq->h_load_next
cfs_rq->h_load
cfs_rq->last_h_load_update
group_weight;
nr_numa_running;
nr_preferred_running;
obtained.
@idle:
index.
load_idx;
SD_SHARE_CPUCAPACITY)
sd->smt_gain
default_scale_cpu_capacity(sd,
age_stamp;
SCHED_CAPACITY_SCALE
*sdg
sdg->sgc->capacity
update_group_capacity(struct
clamp(interval,
1UL,
max_load_balance_interval);
(!child)
(child->flags
group->next;
imbalance_pct
Imagine
difficulty
detected;
find_busiest_group()
group_has_capacity
meaningful
sgs->group_weight)
((sgs->group_capacity
(sgs->group_usage
env->sd->imbalance_pct))
group_overloaded;
@group:
@sgs:
local_group,
sgs->sum_weighted_load
sgs->group_capacity;
sgs->group_no_capacity
sgs);
sgs->group_type
@sg
*sg,
*busiest
(sgs->group_type
busiest->group_type)
busiest->avg_load)
(!(env->sd->flags
SD_ASYM_PACKING))
ASYM_PACKING
group_first_cpu(sg))
(!sds->busiest)
fbq_classify_group(struct
remote;
fbq_classify_rq(struct
env->sd->groups;
prefer_sibling
SD_PREFER_SIBLING)
sched_group_cpus(sg));
sds->local
CPU_NEWLY_IDLE
sg,
group_has_capacity(env,
SD_NUMA)
overload;
primarily
busiest_cpu
sched_domain,
imbn
scaled_busy_load_per_task
busiest->group_capacity;
busiest->load_per_task;
justify
min(local->load_per_task,
busiest->avg_load
busiest->load_per_task
busiest->group_capacity)
local->group_capacity;
group_imbalanced)
fix_small_imbalance(env,
sds);
group_overloaded)
busiest->sum_nr_running
power-savings
policies
max_pull
local->avg_load)
opted
balance,
&sds);
((env->idle
sds.busiest;
force_balance;
(local->avg_load
CPU_NEWLY_IDLE,
Looks
out_balanced:
rt;
classification
weighted_cpuload()
capacity.
maximum.
backoff
Pretty
Working
load_balance
load_balance_mask);
sd))
active_load_balance_cpu_stop(void
sched_group_cpus(sg);
!idle_cpu(cpu))
active_balance
.sd
.dst_rq
this_rq,
.idle
.tasks
LBF_ALL_PINNED;
raw_spin_lock_irqsave(&busiest->lock,
cur_ld_moved
cumulative
family
raw_spin_unlock(&busiest->lock);
more_balance;
given_cpu
ilb_cpu
acting
_same_
((env.flags
env.imbalance
env.loop
env.loop_break
(sd_parent)
&sd_parent->groups->sgc->imbalance;
newidle
active_load_balance_cpu_stop,
raw_spin_unlock_irqrestore(&busiest->lock,
begun
(sd->balance_interval
this_rq->cpu;
this_rq->idle_stamp
(this_rq->avg_idle
for_each_domain(this_cpu,
continue_balancing
t0,
sd->max_newidle_lb_cost)
t0
sched_clock_cpu(this_cpu);
sd->max_newidle_lb_cost
(time_after(this_rq->next_balance,
this_rq->next_balance
meantime
target_cpu,
one).
nohz.idle_cpus_mask);
(!sd
sd->nohz_idle
nohz_flags(cpu)))
balanced,
update_next_balance
need_decay
max_cost
(need_decay)
SD_SERIALIZE;
(need_serialize)
CPU_NOT_IDLE;
rq->max_idle_balance_cost
rq->next_balance
nohz_idle_balance(struct
nohz.next_balance
clear_bit(NOHZ_BALANCE_KICK,
kicking
nr_busy
sched_domain_span(sd))
nohz_idle_balance
Else
trigger_load_balance(struct
__set_task_cpu(p,
thing.
!queued,
sched_fair
se->parent->depth
init_cfs_rq(struct
->vruntime
sleeper
base.
(!queued)
set_task_rq(p,
tg->shares
err_free_rq;
err_free_rq:
init_tg_cfs_entry(struct
se->cfs_rq
parent->my_q;
shares)
rr_interval
print_cfs_stats(struct
stands
task_group_is_autogroup(struct
task_wants_autogroup(struct
autogroup_task_group(struct
buflen);
*init_task)
Emelyanov
<xemul@openvz.org>,
Sukadev
Bhattiprolu
<sukadev@us.ibm.com>,
Oleg
Nesterov
pid_cache
nr_ids;
name[16];
*cachep;
nr_ids)
mutex_unlock(&pid_caches_mutex);
MAX_PID_NS_LEVEL
limiting
out_free_map;
ns->parent
ns->user_ns
get_user_ns(user_ns);
PIDMAP_ENTRIES;
kmem_cache_free(pid_ns_cachep,
put_user_ns(ns->user_ns);
CLONE_NEWPID))
old_ns);
ns->parent;
*pid_ns)
*me
next_pidmap(pid_ns,
!__fatal_signal_pending(task))
send_sig_info(SIGKILL,
reparent
pid_ns,
volatile
pid_max;
&pid_max,
pid_ns->reboot
(ns)
get_pid_ns(ns);
&ns->ns
!ns_capable(current_user_ns(),
active->level)
read/writes
irq_to_desc((long)m->private);
*mask
desc->irq_data.affinity;
free_cpumask;
(!is_affinity_mask_valid(new_value))
targeted.
(!cpumask_intersects(new_value,
irq_to_desc((long)
m->private);
%u\n"
desc->irq_count,
((action
action->dir
proc_mkdir(name,
(!root_irq_dir
sprintf(name,
root_irq_dir);
/proc/irq
register_irq_proc(irq,
ACTUAL_NR_IRQS
any_count
*(loff_t
ACTUAL_NR_IRQS)
prec
seq_putc(p,
irq_lock_sparse();
irq_to_desc(i);
kstat_irqs_cpu(i,
prec,
desc->irq_data.chip->name);
"-");
irq_unlock_sparse();
defined(TRACE_HEADER_MULTI_READ)
_TRACE_BENCHMARK_H
BENCHMARK_EVENT_STRLEN
TP_STRUCT__entry(
TP_fast_assign(
__entry->str),
<trace/define_trace.h>
getname().
names_list.
audit_context,
audit_tree;
audit_chunk;
effective;
Further,
name_len;
osid;
fcap;
fcap_ver;
audit_proctitle
dummy;
major;
return_valid;
preallocated_names
arch;
qbytes;
msg_len;
msg_prio;
oflag;
argc;
audit_ever_enabled;
audit_copy_inode(struct
audit_log_cap(struct
audit_log_name(struct
record_num,
audit_inode_hash[AUDIT_INODE_BUCKETS];
ino)
audit_match_class(int
audit_uid_comparator(kuid_t
audit_gid_comparator(kgid_t
parent_len(const
audit_compare_dname_path(const
*audit_make_reply(__u32
audit_panic(const
*net;
audit_send_list(void
audit_free_rule_rcu(struct
*audit_dupe_rule(struct
audit_log_d_path_exe(struct
CONFIG_AUDIT_WATCH
audit_put_watch(struct
audit_get_watch(struct
audit_to_watch(struct
audit_add_watch(struct
audit_remove_watch_rule(struct
*audit_watch_path(struct
audit_watch_compare(struct
o)
*audit_tree_lookup(const
audit_put_chunk(struct
audit_tree_match(struct
audit_make_tree(struct
u32);
audit_add_tree_rule(struct
audit_remove_tree_rule(struct
audit_tag_tree(char
*new);
*audit_tree_path(struct
audit_put_tree(struct
audit_kill_trees(struct
(void)0
audit_tag_tree(old,
*audit_unpack_string(void
**,
audit_sig_pid;
__audit_signal_info(int
*t);
t->tgid
audit_filter_inodes(struct
<net/netlink.h>
register/stack
insn.
depth-first-search
rejects
unreachable
jumps
BPF_MOV64_REG(BPF_REG_1,
R10
BPF_REG_10),
BPF_ALU64_IMM(BPF_ADD,
FRAME_PTR)
PTR_TO_MAP_VALUE,
PTR_TO_CTX,
check_mem_access()
'pointer
[key,
Corresponding
FRAME_PTR
CONST_PTR_TO_MAP,
far,
key_size)
BPF_CALL
branch.
frame_pointer
spilled
STACK_MISC
BPF_REG_SIZE];
prev_insn_idx;
pruning
log,
log_len,
"?",
verbose("
PTR_TO_STACK)
PTR_TO_MAP_VALUE_OR_NULL)
MAX_BPF_STACK;
"jmp",
BPF_CLASS(insn->code);
%sr%d
bpf_alu_string[BPF_OP(insn->code)
%s%d\n",
BPF_STX)
r%d\n",
BPF_XADD)
BPF_ST)
BPF_LDX)
BPF_LD)
r0
BPF_IMM)
bpf_jmp_string[BPF_OP(insn->code)
*elem;
env->head
&env->cur_state,
verbose("BPF
(pop_stack(env,
BPF_REG_0,
BPF_REG_3,
BPF_REG_4,
PTR_TO_CTX;
reg_arg_type
(regs[regno].type
!read_ok\n",
BPF_W)
state->regs[value_regno].type
PTR_TO_CTX))
spill\n");
BPF_REG_SIZE]
BPF_REG_SIZE;
state->stack_slot_type[MAX_BPF_STACK
(slot_type[i]
STACK_MISC)
%d+%d
value_regno==-1,
BPF_READ
atomic_add
BPF_READ,
access_size
access_size);
(reg->type
ARG_PTR_TO_MAP_VALUE)
PTR_TO_STACK;
ARG_CONST_STACK_SIZE)
ARG_CONST_MAP_PTR)
CONST_PTR_TO_MAP;
bpf_map_xxx(...,
...,
(!*mapp)
func_id)
*fn
*reg;
proprietary
CALLER_SAVED_REGS;
caller_saved[i];
reg->type
reg->imm
BPF_NEG)
64))
verbose("BPF_MOV
regs[insn->dst_reg]
regs[insn->dst_reg].map_ptr
opcode);
ops:
stack_relative
verbose("BPF_ALU
BPF_SRC(insn->code)
BPF_K)
*insn,
verbose("BPF_JMP
BPF_JNE)
(imm
pc+off;
*insn_idx
PTR_TO_MAP_VALUE;
targer
other_branch->regs[insn->dst_reg].imm
(R
print_verifier_state(env);
insn[1].imm)
(BPF_SIZE(insn->code)
scratch
Implicit
R6
Output:
8/16/32-bit
BPF_MODE(insn->code);
(register
readable,
non-recursive
S.pop()
tree-edge
18
back-edge
22
forward-
cross-edge
23
0x10
FALLTHROUGH
BRANCH
(DISCOVERED
env->prog->len)
w);
STATE_LIST_MARK;
(insn_state[w]
DISCOVERED;
(cur_stack
EXPLORED)
*insns
kcalloc(insn_cnt,
kfree(insn_state);
BPF_JA)
insns[t].off
insn_cnt)
exploring
further,
(slot1=INV
(slot1=MISC
(old->regs[i].type
(old->stack_slot_type[i]
Ex:
equivalent,
BPF_REG_SIZE],
{.type
.imm
stored:
env->explored_states[insn_idx];
marked,
(sl
STATE_LIST_MARK)
sl->next;
technically
bpf_exit
loops,
(insn_idx
verbose("\nfrom
safe\n",
prev_insn_idx,
verbose("%d:
src_reg_type;
(src_reg
(src_reg_type
Reject
(dst_reg
writeable
(BPF_LD
BPF_DW))
next_insn;
insn[0].imm
verifier,
free_bpf_prog_info()
*prog,
BPF_OP(insn->code)
insn_buf,
sizeof(*insn)
substitute
env->prog
*sl,
(!env->explored_states)
log_ubuf
free_env;
log_buf
free_log_buf;
(!env->prog->aux->used_maps)
@id:
declaring
laid
helpers,
use:
@print:
address:
ftrace_entry,
perf_ftrace_event_register
ftrace_graph_ent_entry,
graph_ent,
__entry->depth),
ftrace_graph_ret_entry,
calltime)
rettime
prev_pid
F_printk("%u:%u:%u
%u:%u:%u
[%03u]",
__entry->prev_pid,
__entry->prev_prio,
__entry->prev_state,
__entry->next_pid,
__entry->next_prio,
__entry->next_state,
__entry->next_cpu),
FTRACE_STACK_ENTRIES
"%08lx"
"%016lx"
stack_entry,
F_printk("\t=>
")\n",
__entry->caller[0],
__entry->caller[1],
__entry->caller[2],
__entry->caller[3],
__entry->caller[4],
__entry->caller[5],
__entry->caller[6],
__entry->caller[7]),
userstack_entry,
bprint_entry,
__dynamic_array(
print_entry,
bputs_entry,
trace_mmiotrace_rw,
map_id
F_printk("%lx
long)__entry->phys,
__entry->map_id,
trace_mmiotrace_map,
trace_branch,
Balbir
(!parent_css)
(!ca)
kernel_cpustat);
free_percpu(ca->cpuusage);
kfree(ca);
platforms.
raw_spin_lock_irq(&cpu_rq(cpu)->lock);
raw_spin_unlock_irq(&cpu_rq(cpu)->lock);
totalcpuusage
cpuacct_cpuusage_read(ca,
(reset)
"user",
*kcpustat
per_cpu_ptr(ca->cpustat,
cputime64_to_clock_t(val);
"stat",
parent_ca(ca);
spin_lock(lock);
(void)(flags);
spin_unlock(lock);
ti)
debug_mutex_wake_waiter(lock,
waiter)
debug_mutex_add_waiter(lock,
debug_mutex_init(lock,
linux/kernel/time/clocksource.c
*tc,
cyclecounter
*cc,
tc->cycle_last
tc->nsec
*tc)
cycle_delta;
ns_offset;
tc->cycle_last)
cyclecounter_cyc2ns(tc->cc,
frac)
cycle_tstamp)
kernel/stop_machine.c
Products
Tejun
Heo
<tj@kernel.org>
stop_machine_initialized
stoppers
*done,
(done)
per_cpu(cpu_stopper_task,
cpu_stop_signal_done(work->done,
monopolizing
middle,
@fn.
&done
msdata->state
curstate
re-read
hard_irq_disable();
work1
work2
cpu1,
cpu2,
.num_threads
cpu_stop_work){
&msdata,
stop_one_cpu()
untouched
stop_cpus
cpu_stop_work,
&per_cpu(stop_cpus_work,
&done);
stop_cpus()
__stop_cpus(cpumask,
(!mutex_trylock(&stop_cpus_mutex))
run;
spin_lock_irq(&stopper->lock);
spin_unlock_irq(&stopper->lock);
*done
long)fn,
sched_set_stop_task(int
stopper->enabled
cpu_stop_unpark,
CONFIG_STOP_MACHINE
@fn()
busy-wait
Temporarily
+1
kernel/power/wakelock.c
CONFIG_PM_WAKELOCKS_GC
pm_show_wakelocks(char
show_active)
wakelock,
(str
wakelocks_limit_exceeded(void)
increment_wakelocks_number(void)
decrement_wakelocks_number(void)
wakelocks_lru_add(struct
&wakelocks_lru_list);
wakelocks_lru_most_recent(struct
wakelocks_gc(void)
lru)
&wakelocks_tree);
kfree(wl);
!CONFIG_PM_WAKELOCKS_GC
kstrndup(name,
pm_wake_lock(const
timeout_ns
(!capable(CAP_BLOCK_SUSPEND))
wakelock_lookup_add(buf,
(IS_ERR(wl))
PTR_ERR(wl);
wakelocks_lru_most_recent(wl);
pm_wake_unlock(const
cpu_hardirq_time);
cpu_softirq_time);
irq_time_seq);
latest_ns;
latest_ns
(nsecs_to_cputime64(latest_ns)
account_group_user_time(p,
task_group_account_field(p,
acct_account_cputime(p);
p->gtime
cputime_scaled,
p->stimescaled
((p->flags
(irq_count()
account_guest_time(p,
involuntary
steal_ct;
(eg:
(sum
*times)
nextseq;
round.
nextseq
times->utime
times->stime
times->sum_exec_runtime
@user_tick:
demultiplexing
irqtime_account_process_tick(struct
cputime_to_scaled(cputime_one_jiffy);
(steal_account_process_tick())
scaled,
(user_tick)
account_user_time(p,
scaled);
irqtime_account_idle_ticks(int
CONFIG_VIRT_CPU_ACCOUNTING
__ARCH_HAS_VTIME_ACCOUNT
entry/exit.
(context_tracking_in_user())
task_cputime_adjusted(struct
thread_group_cputime_adjusted(struct
cputime.utime;
cputime.stime;
!CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
user_tick)
(vtime_accounting_enabled())
(sched_clock_irqtime)
cputime_one_jiffy,
one_jiffy_scaled);
scaled;
(total
drop_precision;
stime)
bits?
multiply,
inaccuracies
atomicity.
exported.
rtime;
utime);
.sum_exec_runtime
cputime_adjust(&cputime,
ut,
st);
(clock
delta_cpu,
cputime_to_scaled(delta_cpu));
VTIME_USER;
VTIME_SLEEPING;
gtime;
read_seqbegin(&t->vtime_seqlock);
gtime
vtime_delta(t);
(read_seqretry(&t->vtime_seqlock,
*udelta
*sdelta
sleeping,
udelta,
fetch_task_cputime(t,
&udelta,
&sdelta);
utimescaled,
stimescaled,
Jay
<asm/unaligned.h>
FP
ARG1
regs[BPF_REG_CTX]
Exported
k,
gfp_extra_flags)
gfp_extra_flags;
round_up(size,
__vmalloc(size,
gfp_flags,
(fp
vfree(fp);
fp->pages
fp->aux
fp_old->pages
CONFIG_BPF_JIT
alignment,
128,
hdr
module_alloc(size);
.text
non-static
Decode
BPF_NEG]
BPF_ARSH
BPF_JNE
BPF_JSGT
BPF_JSGE
BPF_XADD
select_insn;
-DST;
(*(s64
&DST)
div64_u64_rem(DST,
div64_u64(DST,
(IMM)
BPF_R1-BPF_R5
BPF_R6-BPF_R9,
BPF_R0.
SRC))
IMM))
SIZE)
insn->off)
u8)
xadd
*)(dst_reg
off16)
insn->off));
ntohl(*(u32
'ctx'
BPF_R6,
ctx.
ntohs(*(u16
imm32)
somewhere.
%02x\n",
JIT
Rewritten
backs
IBM.
tables.
long)_sinittext
long)_einittext)
long)_etext)
(is_module_text_address(addr))
tokens
rtmutex.c.
debug_rt_mutex_reset_waiter(w)
rt_mutex_print_deadlock(struct
debug_rt_mutex_detect_deadlock(struct
walk)
<linux/fsnotify_backend.h>
rules;
mark;
Rules
audit_tree.
rlist
refcounted
logics
free_chunk(chunk);
audit_put_chunk(chunk);
chunk->count
DEFINE_SPINLOCK(hash_lock);
HASH_SIZE;
rcu_read_lock
inode)
*chunk,
tree)
spin_lock(&entry->lock);
list_del_rcu(&chunk->hash);
Fallback;
fsnotify_put_mark(&new->mark);
i--;
get_tree(s);
list_for_each_entry(owner,
same_root)
(1U
(!tree->root)
list_add(&tree->same_root,
tagged
old_entry
old->count;
old_entry->lock,
spin_unlock(&chunk_entry->lock);
list_add(&p->list,
"remove_rule");
rule->filterkey);
res=1",
list_del_init(&rule->rlist);
half-baked
list_del_rcu(&entry->list);
call_rcu(&entry->rcu,
uncommitted
!tree->goner)
list_del_init(&tree->list);
&prune_list);
audit_schedule_prune();
(cursor.next
container_of(cursor.next,
&tree->list);
skip_it;
collect_mounts(&path);
evict_chunk()
*victim;
list_del_init(&victim->list);
prune_one(victim);
prune_thread
wake_up_process(prune_thread);
list_add(&rule->rlist,
&tree->rules);
(IS_ERR(mnt))
iterate_mounts(tag_mount,
mnt);
path_put(&path2);
drop_collected_mounts(tagged);
&barrier);
good_one
list_del(&tree->list);
synchronously.
(!list_empty(list))
need_prune
list_move(&owner->list,
*to_tell,
*inode_mark,
*vfsmount_mark,
data_type,
fsnotify_ops
.handle_event
*buckets;
htab
hash_node;
__aligned(8);
mandatory
htab->map.key_size
attr->key_size;
htab->map.value_size
attr->value_size;
attr->max_entries;
htab->n_buckets
htab->buckets
(!htab->buckets)
hlist_head));
kfree(htab);
hash_node)
rcu_read_lock.
*next_key)
next_l
htab_elem,
hash_node);
(next_l)
memcpy(next_key,
next_l->key,
map_flags)
spin_lock_irqsave(&htab->lock,
(!l_old
map_flags
BPF_NOEXIST)
hlist_del_rcu(&l->hash_node);
htab->count--;
bpf_prog->aux->refcnt
disconnected
executed.
bpf_map_ops
.map_alloc
.map_free
.map_get_next_key
.map_lookup_elem
.map_update_elem
.map_delete_elem
out)
(same
ARG_PTR_TO_STACK,
ARG_CONST_STACK_SIZE,
specifiers
fmt_size
'l')
fmt_cnt++;
'd'
mod[0]
r3
mod[1]
r4
mod[2]
r5
bpf_trace_printk,
misaligned
asm
Include
trace_branch
Ugly
Strip
f->file
trace_likely_condition(struct
(!branch_tracing_enabled)
expect);
enable_branch_tracing(struct
mutex_lock(&branch_tracing_mutex);
rmb()
mutex_unlock(&branch_tracing_mutex);
enable_branch_tracing(tr);
disable_branch_tracing();
field->func,
"branch",
Line\n"
p->incorrect
p->file
%-20.20s
++p;
(percent_a
percent_b)
(a->incorrect
b->incorrect)
(correct)
(a->correct
b->correct)
.stat_cmp
branch_stat_show
stats\n");
CONFIG_PROFILE_ALL_BRANCHES
<laijs@cn.fujitsu.com>
list_for_each_entry(pos,
&trace_bprintk_fmt_list,
mutex_lock(&btrace_mutex);
(tb_fmt)
mutex_unlock(&btrace_mutex);
module_trace_bprintk_format_notify(struct
bprintk
decode
find_next_mod_format(int
start_index,
**fmt,
mod_fmt
container_of(v,
typeof(*mod_fmt),
format_mod_start(void)
format_mod_stop(void)
(unlikely(!fmt))
trace_vbprintk(ip,
trace_vprintk(ip,
**fmt
start_index;
start_index)
__tracepoint_str
last_index
converted.
semaphores:
Alex
mutexes.
unlocked,
WAITING_BIAS)
X-1
((X-1)*ACTIVE_BIAS
ACTIVE_WRITE_BIAS)
Waiters
Writers
lock).
__init_rwsem(struct
semaphore:
*)sem,
sizeof(*sem));
lockdep_init_map(&sem->dep_map,
raw_spin_lock_init(&sem->wait_lock);
INIT_LIST_HEAD(&sem->wait_list);
EXPORT_SYMBOL(__init_rwsem);
RWSEM_WAITING_FOR_WRITE,
RWSEM_WAITING_FOR_READ
rwsem_wake_type
then:
changed)
'waiting
so)
__rwsem_do_wake(struct
(wake_type
rwsem_atomic_update(adjustment,
sem)
RWSEM_WAITING_BIAS))
RWSEM_ACTIVE_MASK)
woken++;
&sem->wait_list)
RWSEM_WAITING_FOR_WRITE);
RWSEM_WAITING_BIAS;
waiter->list.next;
waiter->task;
nil
rwsem_down_read_failed().
RWSEM_WAITING_FOR_READ;
(list_empty(&sem->wait_list))
RWSEM_WAKE_ANY);
(!waiter.task)
cmpxchg(&sem->count,
RWSEM_WAITING_BIAS)
rwsem_atomic_update(RWSEM_WAITING_BIAS,
READ_ONCE(sem->owner);
owner->on_cpu;
owner->on_cpu,
(!owner->on_cpu
need_resched())
rwsem_optimistic_spin(struct
live-lock
re-loaded.
spins.
sem.
RWSEM_WAITING_FOR_WRITE;
system_freezing_cnt
Temporary
unnecessary.
(test_thread_flag(TIF_MEMDIE))
was_frozen
refrigerator\n",
spin_lock_irq(&freezer_lock);
spin_unlock_irq(&freezer_lock);
freezer_count()
spin_lock_irqsave(&freezer_lock,
wake_up_state(p,
refrigerator
Modify
func_stack
klp_mutex
data).
old_addr)
klp_func,
stack_node);
vmlinux
(!klp_is_module(obj))
finishes.
(mod
&klp_patches,
ambiguity
*args
((mod
(!mod
mod->name))
.addr
(args.count
func->old_addr
*pmod,
find_symbol(name,
(sym)
(WARN_ON(!klp_is_object_loaded(obj)))
reloc->val
&reloc->val);
klp_find_ops(func->old_addr);
list_del(&ops->node);
kfree(ops);
kzalloc(sizeof(*ops),
FTRACE_OPS_FL_DYNAMIC
list_add_rcu(&func->stack_node,
&ops->func_stack);
ftrace_set_filter_ip(&ops->fops,
pr_err("failed
stacking:
list)->state
ftrace.
__klp_disable_patch(patch);
klp_enable_object(obj);
unregister;
lookups
__klp_enable_patch(patch);
Sysfs
Interface
container_of(kobj,
klp_patch,
kobj);
consistency
model
klp_register_patch()
&kobj_sysfs_ops,
.default_attrs
*limit)
klp_free_funcs_limited(obj,
kobject_put(obj->kobj);
klp_free_objects_limited(patch,
kobject_put(&patch->kobj);
klp_init_object_loaded(patch,
*pmod
patch->mod;
pr_warn("failed
mod->klp_alive
Profiling
counterparts.
gcc/gcov-io.h.
gcov_type;
Opaque
gcov_event(enum
kernel/power/suspend_test.c
standby
suspend_test_start_time;
suspend_test_start(void)
"jiffies",
*label)
Warning
have)
WARN_ON
RTCs
rtc_read_time(rtc,
&alm.time);
dev_name(&rtc->dev),
alm.enabled
rtc_set_alarm(rtc,
&alm);
PM_SUSPEND_STANDBY;
to_rtc_device(dev);
They're
strsep(&value,
pm_labels[i];
printk(warn_bad_state,
*rtc
(test_state
(!rtc)
gcc/gcov-io.h
*gcov_info_head;
@num:
@values:
gcov_type
*values;
@ident:
ident;
@version:
@stamp:
@filename:
@merge:
@n_functions:
@functions:
stamp;
n_functions;
gcov_info_filename
info->filename;
gcov_info_version
gcov_info_next
gcov_info_link
link/add
gcov_info_unlink
unlink/remove
Symbolic
gcov_link[]
"gcno"
$(objtree).
NULL},
counter_active(struct
num_counter_active(struct
(counter_active(info,
gcov_info_reset
(!counter_active(info,
sizeof(gcov_type)
ci_ptr++;
gcov_info_is_compatible
@info1:
@info2:
*info2)
(info1->stamp
info2->stamp);
gcov_info_add
@source:
@source
@dest.
*dci_ptr;
*sci_ptr;
dci_ptr
sci_ptr
dci_ptr++;
sci_ptr++;
gcov_info_dup
*dup;
dup
(!dup)
kstrdup(info->filename,
(!dup->filename)
(!dup->functions)
fi_size
gcov_ctr_info)
dup;
gcov_info_free(dup);
gcov_info_free
kfree(info->functions);
kfree(info->filename);
@off:
data[0]
0xffffffffUL);
data[1]
sizeof(*data)
GCOV_DATA_MAGIC);
GCOV_TAG_FUNCTION);
gcov_iter_new
iter->info
iter->size
iter->buffer
iter->info;
gcov_iter_start
gcov_iter_next
ITER_STRIDE;
gcov_iter_write
*seq)
<linux/posix-clock.h>
delete_clock(struct
fp->private_data;
down_read(&clk->rwsem);
clk;
*clk)
fp,
(clk->ops.ioctl)
clk->ops.ioctl(clk,
posix_clock,
fp->private_data
kref_put(&clk->kref,
delete_clock);
.fasync
*cd)
cd->clk
((cd.fp->f_mode
*kit)
kit);
*kit,
kit,
.clock_adj
_CONSOLE_CMDLINE_H
*options;
CONFIG_A11Y_BRAILLE_CONSOLE
assist
audit_free_parent(parent);
(likely(parent))
watch->parent
(watch->ino
long)-1)
*watch;
watch->dev
watch->ino
watch;
audit_init_watch(path);
undefined.
(IS_ERR(new))
ses=%u
*nextw;
*nextr;
*nentry;
nextw,
AUDIT_NAME_FULL))
nextr,
nentry
(IS_ERR(nentry))
nentry->rule.watch
&nentry->rule.list);
audit_watch_log_rule_change(r,
&parent->watches);
list_del(&r->rlist);
list_del(&r->list);
list_del_rcu(&e->list);
call_rcu(&e->rcu,
fsnotify_destroy_mark(&parent->mark,
Associate
watch_found
audit_get_parent(parent);
**list)
audit_put_parent(parent);
*krule)
audit_update_watch(parent,
audit_watch_group
attr->max_entries
array->map.max_entries
<linux/mmiotrace.h>
__field_struct(type,
item[];
F_STRUCT
F_STRUCT(args...)
struct_name
tstruct
FTRACE_ENTRY_DUP(name,
FTRACE_ENTRY_REG(name,
args[];
trace_flag_type
enumeration
TRACE_FLAG_IRQS_OFF
TRACE_FLAG_IRQS_NOSUPPORT
0x04,
TRACE_FLAG_HARDIRQ
0x08,
TRACE_FLAG_SOFTIRQ
TRACE_FLAG_PREEMPT_RESCHED
well:
trace_array_get(struct
trace_array_put(struct
enum.
*opts;
tracing_enabled)
@stop:
tracing_thresh
setting,
calls...
"global"
FTRACE
did.
trace_recursion.
(current)->trace_recursion
TRACE_CONTEXT_BITS))
TRACE_LIST_MAX
current->trace_recursion;
((val
(unlikely(val
bit)))
current->trace_recursion
tracer_init(struct
tracing_reset(struct
tracing_reset_online_cpus(struct
tracing_open_generic(struct
*trace_create_file(const
trace_buffer_lock_reserve(struct
*trace_find_next_entry(struct
__buffer_unlock_commit(struct
*trace_find_next_entry_inc(struct
trace_init_global_iter(struct
tracing_iter_reset(struct
trace_function(struct
trace_graph_function(struct
trace_latency_header(struct
trace_default_header(struct
print_trace_header(struct
trace_graph_return(struct
trace_graph_entry(struct
set_graph_array(struct
register_tracer(struct
*type);
tracing_lseek(struct
whence);
tracing_buffer_mask;
nsecs_to_usecs(unsigned
tracing_thresh;
update_max_tr(struct
update_max_tr_single(struct
ftrace_now(int
trace_find_cmdline(int
ring_buffer_expanded;
tracing_selftest_disabled;
ftrace_cpu_disabled);
trace_selftest_startup_function(struct
trace_selftest_startup_function_graph(struct
trace_selftest_startup_irqsoff(struct
trace_selftest_startup_preemptoff(struct
trace_selftest_startup_preemptirqsoff(struct
trace_selftest_startup_wakeup(struct
trace_selftest_startup_nop(struct
trace_selftest_startup_sched_switch(struct
trace_selftest_startup_branch(struct
Tracer
ns2usecs(cycle_t
trace_vbprintk(unsigned
trace_vprintk(unsigned
trace_array_vprintk(struct
trace_array_printk(struct
...);
trace_array_printk_buf(struct
trace_printk_seq(struct
print_trace_line(struct
trace_find_mark(unsigned
duration);
TRACE_GRAPH_PRINT_CPU
TRACE_GRAPH_PRINT_OVERHEAD
TRACE_GRAPH_PRINT_PROC
0x8
TRACE_GRAPH_PRINT_DURATION
print_graph_headers_flags(struct
trace_print_graph_duration(unsigned
graph_trace_open(struct
graph_trace_close(struct
__trace_graph_entry(struct
__trace_graph_return(struct
ftrace_graph_addr(unsigned
ftrace_graph_notrace_addr(unsigned
ftrace_trace_task(struct
ftrace_init_array_ops(struct
ftrace_is_dead(void)
defined(CONFIG_FUNCTION_TRACER)
ftrace_create_filter_files(struct
ftrace_destroy_filter_files(struct
*ops);
ftrace_event_is_function(struct
*call);
@idx:
parser->idx
trace_parser_get_init(struct
trace_parser_put(struct
trace_get_user(struct
positions
TRACE_ITER_PRINT_PARENT
TRACE_ITER_ANNOTATE
TRACE_ITER_CONTEXT_INFO
TRACE_ITER_SLEEP_TIME
TRACE_ITER_GRAPH_TIME
TRACE_ITER_RECORD_CMD
TRACE_ITER_IRQ_INFO
TRACE_ITER_MARKERS
TRACE_ITER_SYM_MASK
symbols.
trace_branch_enable(struct
trace_branch_disable(void)
link;
filter_type;
ref_count;
reserved.
16384
regex;
*filter_string);
trace_find_event_field(struct
trace_event_enable_cmd_record(bool
event_trace_add_tracer(struct
event_trace_del_tracer(struct
clear_event_triggers(struct
*filter_str;
per-event
@init()
implemented.
@reg()
de-initialization
(*print)(struct
purposes,
deferring
below).
discarded.
trace_event_enable_disable(struct
soft_disable);
trace_keep_overwrite(struct
set_tracer_flag(struct
warnings,
perf_ftrace_event_register(struct
trace_event_init(void)
*tracepoint_print_iter;
sysctl_sched_autogroup_enabled
*ag;
(!lock_task_sighand(p,
autogroup_kref_get(&autogroup_default);
(IS_ERR(tg))
fly.
__sched_setscheduler()
free_rt_sched_group(tg);
tg->rt_se
tg->rt_rq
sched_online_group(tg,
&root_task_group)
autogroup_move_group(p,
INITIAL_JIFFIES;
(nice
security_task_setnice(current,
!can_nice(current,
nice))
autogroup_task_get(p);
O(1)
tuning
Kolivas.
Haskins,
<asm/tlb.h>
<asm/mutex.h>
start_bandwidth_timer(struct
*period_timer,
HRTIMER_MODE_ABS_PINNED,
runqueues);
update_rq_clock_task(struct
update_rq_clock(struct
rq->clock;
__SCHED_FEAT_NR;
sched_feat_disable(int
sched_feat_enable(int
(strncmp(cmp,
*cmp;
63)
mutex_lock(&inode->i_mutex);
mutex_unlock(&inode->i_mutex);
MSEC_PER_SEC;
sysctl_sched_rt_period
scheduler_running;
hrtick_clear(struct
High-resolution
&rq->hrtick_timer;
__hrtick_restart(rq);
10000ns,
delay,
10000LL);
hrtimer_set_expires(timer,
this_rq())
fairness.
init_rq_hrtick(struct
__old;
TIF_POLLING_NRFLAG,
thereby
set_nr_and_not_polling(struct
*ti
task_thread_info(p);
_TIF_NEED_RESCHED)
set_nr_if_polling(struct
resched_curr(struct
semi
enqueues
(tick_nohz_full_cpu(cpu))
tick_nohz_tick_stopped())
got_nohz_idle_kick(void)
!need_resched())
(current->policy
Round-robin
priority?
Inline
rq->age_stamp
rq->rt_avg
defined(CONFIG_RT_GROUP_SCHED)
@down
@up
rcu_lock
walk_tg_tree_from(struct
from;
list_for_each_entry_rcu(child,
siblings)
tg_nop(struct
*load
minimal
load->weight
load->inv_weight
sched_info_dequeued(rq,
activate_task(struct
(task_contributes_to_load(p))
rq->nr_uninterruptible--;
deactivate_task(struct
it...
defined(CONFIG_IRQ_TIME_ACCOUNTING)
defined(CONFIG_PARAVIRT_TIME_ACCOUNTING)
{soft,}irq
->clock_task
irq_delta;
CONFIG_PARAVIRT_TIME_ACCOUNTING
modifiers.
(task_has_dl_policy(p))
__normal_prio(p);
p->normal_prio;
*prev_class,
dl_task(p))
check_preempt_curr(struct
rq->curr->sched_class)
set_task_cpu()
ttwu()
TASK_WAKING
sched_move_task()
pins
task_rq_lock().
*src_rq,
dst_rq
p->wake_cpu
dst_rq);
migrate_swap(struct
arg.src_cpu,
migration_cpu_stop(void
count).
remained
sometime
match_state)
ncsw;
out!
(task_running(rq,
ncsw
again..
now),
enter/exit
*nodemask
cpuset;
cpu_to_node()
for_each_cpu(dest_cpu,
(!cpu_online(dest_cpu))
(!cpu_active(dest_cpu))
Nice
(both
(p->mm
printk_ratelimit())
(%s)
sd_flags,
p->sched_class->select_task_rq(p,
*avg
(wake_flags
cpu_of(rq));
(p->sched_class->task_woken)
p->sched_class->task_woken(rq,
rq->avg_idle
ttwu_activate(rq,
ENQUEUE_WAKEUP
TASK_RUNNING,
->on_rq.
check_preempt_curr()
__task_rq_unlock(rq);
sched_ttwu_pending(void)
ttwu_do_activate(rq,
irq_enter();
irq_exit();
try_to_wake_up
set_current_state()
does.
smp_mb__before_spinlock();
(!(p->state
select_task_rq(p,
ttwu_stat(p,
raw_spin_lock(&p->pi_lock);
raw_spin_unlock(&p->pi_lock);
try_to_wake_up(p,
__dl_clear_params(struct
dl_se->dl_runtime
dl_se->dl_bw
memset(&p->se.statistics,
sizeof(p->se.statistics));
p->mm->numa_scan_seq
set_numabalancing_state(bool
proc_dointvec_minmax(&t,
task_has_rt_policy(p))
NICE_TO_PRIO(0);
fork.
(dl_prio(p->prio))
(rt_prio(p->prio))
sched_fork().
defined(CONFIG_SCHEDSTATS)
defined(CONFIG_TASK_DELAY_ACCT)
plist_node_init(&p->pushable_tasks,
MAX_PRIO);
to_ratio(u64
1ULL
paths,
rcu_lockdep_assert(rcu_read_lock_sched_held(),
held");
dl_bw_cpus(int
attr->sched_deadline;
attr->sched_runtime;
(new_bw
!__dl_overflow(dl_b,
new_bw))
new_bw);
__dl_clear(dl_b,
Fork
fire_sched_in_preempt_notifiers(struct
*notifier;
hlist_for_each_entry(notifier,
&curr->preempt_notifiers,
fire_sched_out_preempt_notifiers(struct
sched_info_switch(rq,
actions.
prev_state;
rq->prev_mm
prev->state;
finish_arch_post_lock_switch();
TASK_DEAD))
post_schedule(struct
freshly
preemtion
finish_task_switch(prev);
post_schedule(rq);
*oldmm;
oldmm
backend
spin_release(&rq->lock.dep_map,
bootup.
atomic_read(&this->nr_iowait);
defined(CONFIG_64BIT)
indistinguishable
rq_last_tick_reset(rq);
(in_lock_functions(addr))
defined(CONFIG_PREEMPT)
Underflow?
PREEMPT_MASK)
get_parent_ip(CALLER_ADDR1);
current->preempt_disable_ip
trace_preempt_off(CALLER_ADDR0,
preempt_count()))
!(preempt_count()
get_parent_ip(CALLER_ADDR1));
bug:
(oops_in_progress)
atomic:
(irqs_disabled())
pr_err("Preemption
at:");
print_ip_sym(current->preempt_disable_ip);
atomic.
prev->state
rcu_sleep_check();
__builtin_return_address(0));
RETRY_TASK))
idle_sched_class
waitqueue,
interrupt-handler
cond_resched()
need_resched()
ACT
pick_next_task(rq,
solution.
NB:
Ideally
exception_exit(prev_state);
(likely(!preemptible()))
preempt_enable_notrace
site
oldprio,
*prev_class;
prev_class
&dl_sched_class;
(dl_prio(oldprio))
SCHED_DEADLINE,
NICE_TO_PRIO(nice);
old_prio
/proc.
find_task_by_vpid(pid)
dl_se->dl_deadline;
SHOULD
amount.
attr->sched_policy;
SETPARAM_POLICY)
(dl_policy(policy))
(fair_policy(policy))
attr->sched_priority;
change:
__setscheduler_params(p,
prio.
attr->sched_runtime
(attr->sched_deadline
DL_SCALE
wrap-around
__task_cred(p);
recheck:
reset_on_fork
SCHED_BATCH
(!p->mm
((dl_policy(policy)
(rt_policy(policy)
(attr->sched_priority
task_nice(p)
!can_nice(p,
(!check_same_owner(p))
(p->sched_reset_on_fork
security_task_setscheduler(p);
p->policy))
attr->sched_nice
attr))
(rt_bandwidth_enabled()
&p->cpus_allowed)
recheck;
__setscheduler(rq,
ENQUEUE_HEAD
SCHED_RESET_ON_FORK
attr.sched_flags
SCHED_FLAG_RESET_ON_FORK;
attr.sched_policy
__sched_setscheduler(p,
&attr,
sched_setscheduler
_sched_setscheduler(p,
(!param
memset(attr,
sizeof(*attr));
get_user(size,
copy_from_user(attr,
values?
err_size:
put_user(sizeof(*attr),
do_sched_setscheduler(pid,
@uattr:
extension.
(!uattr
class)
*)attr
usize;
PF_NO_SETAFFINITY)
CAP_SYS_NICE))
cpuset_cpus_allowed(p,
basis,
out_put_task:
*user_mask_ptr,
cpumask_size();
user_mask_ptr,
user_mask_ptr
@user_mask_ptr:
sched_setaffinity(pid,
&p->cpus_allowed,
(sizeof(unsigned
sched_getaffinity(pid,
yld_count);
(should_resched())
schedule,
strange
99%
yield();
Never
something,
absolutely
p_rq)
double_rq_unlock(rq,
(curr->sched_class
preempt);
current->in_iowait
rt_priority
(policy)
SCHED_FIFO:
SCHED_DEADLINE:
SCHED_NORMAL:
SCHED_BATCH:
SCHED_IDLE:
@interval:
time_slice
TASK_STATE_TO_CHAR_STR;
__ffs(state)
%08lx
thread_saved_pc(p));
CONFIG_DEBUG_STACK_USAGE
ppid
0x%08lx\n",
free,
father\n");
sched_show_task(p);
idle->sched_class
&idle_sched_class;
*idle,
Alternatively
trial_cpus
__dl_overflow(dl_b,
(overflow)
root_domain,
CPU)
exit()
thread:
move_queued_task(p,
migrate_task_to(struct
target_cpu)
curr_cpu
(curr_cpu
target_cpu);
sched_setnuma(struct
enforcing
wakeups,
rq->stop
put_prev_task()
@next,
dead_cpu,
defined(CONFIG_SCHED_DEBUG)
defined(CONFIG_SYSCTL)
**tablep)
*tablep
maxlen,
maxlen;
(table
"name",
CORENAME_MAX_SIZE,
0555;
entry->child
entry++;
ctl_table_header
register_sched_domain_sysctl(void)
num_possible_cpus();
sd_sysctl_header
unregister_sched_domain_sysctl(void)
rq->rd->online);
rq->calc_load_update
BUG_ON(!cpumask_test_cpu(cpu,
rq->rd->span));
set_rq_online(rq);
set_rq_offline(rq);
__cpuinit
set_cpu_rq_start_time();
set_cpu_active((long)hcpu,
migration_call(&migration_notifier,
BUG_ON(err
CPU_PRI_SCHED_ACTIVE);
sched_debug_enabled;
sched_debug(void)
"%*s
sd->name);
sched_group_cpus(group)))
NULL\n");
CPUs\n");
(!cpumask_equal(sched_domain_span(sd),
domain->span\n");
superset
"CPU%d
sched_domain_debug(sd,
SD_BALANCE_EXEC
SD_PREFER_SIBLING
free_cpumask_var(rd->dlo_mask);
free_cpumask_var(rd->rto_mask);
free_cpumask_var(rd->online);
free_cpumask_var(rd->span);
kfree(rd);
(cpumask_test_cpu(rq->cpu,
rq->rd
free_dlo_mask;
def_root_domain;
free_sched_groups(sd->groups,
sd_llc);
sd_llc_size);
sd_llc_id);
sd_numa);
sd_busy);
sd_asym);
highest_flag_domain(cpu,
cpumask_weight(sched_domain_span(sd));
tmp->parent
sd->child
rq_attach_root(rq,
cpulist_parse(str,
sched_domain_span(sibling)))
cpumask_set_cpu(i,
sched_group_mask(sg));
group_balance_cpu(struct
cpumask_clear(covered);
(cpumask_test_cpu(i,
covered))
sched_group)
sg_span);
cpumask_set_cpu(j,
cpu_capacity.
Typically
__sdt_free(const
*cpu_map);
__sdt_alloc(const
sa_sd_storage;
cpu))->ref))
sched_numa_topology_type;
topologies
SD_ASYM_PACKING
*tl,
behaviour.
sd->imbalance_pct
sd->idle_idx
(tl
sched_domain_topology
tl;
find_numa_distance(int
node_distance(0,
controllers.
b?
sched_init_numa(void)
distances
distance;
(sched_debug()
next_distance;
members.
(node_distance(j,
sched_domains_numa_distance[i])
sched_domain_topology[i].mask;
tl[i]
sched_domains_numa_masks[i][j]);
sched_domains_numa_masks_update(struct
sdd->sd
sdd->sg
sdd->sgc
*cpu_map,
pr_err("
domain\n",
cpu_map);
(array
maps.
ndoms,
doms;
arch_update_cpu_topology();
ndoms_cur
&fallback_doms;
register_sched_domain_sysctl();
idx_new)
'doms_new'
destroying
mutex_lock(&sched_domains_mutex);
!new_topology;
cpu_active_mask,
ndoms_new;
dattr_new
mutex_unlock(&sched_domains_mutex);
cpuset_update_active_cpus()
CPU_DOWN_FAILED_FROZEN:
sched_init_smp(void)
non_isolated_cpus;
cpumask_andnot(non_isolated_cpus,
sched_init_granularity();
free_cpumask_var(non_isolated_cpus);
CONFIG_CPUMASK_OFFSTACK
&task_groups);
task-groups
formed
(of
A0's
NULL).
sysctl_sched_migration_cost;
isolcpus
(nested
preempt_offset);
current->state,
[<%p>]
ratelimiting
prev_jiffy
(task_stack_end_corrupted(current))
!rt_task(p))
set_user_nice(p,
defined(CONFIG_KGDB_KDB)
kdb.
configuration.
VALID
WHEN
THE
WHOLE
SYSTEM
STOPPED!
non-blocking
synchronized,
above)
cpu_curr(cpu)
*sched_create_group(struct
sched_online_group(struct
spin_lock_irqsave(&task_group_lock,
&parent->children);
spin_unlock_irqrestore(&task_group_lock,
sched_destroy_group(struct
sched_offline_group(struct
sched_move_task(struct
d->rt_period;
d->rt_runtime;
.tg
tg,
mutex_lock(&rt_constraints_mutex);
tg->rt_bandwidth.rt_runtime
mutex_unlock(&rt_constraints_mutex);
rt_runtime,
tg_set_rt_bandwidth(tg,
rt_runtime_us;
rt_period_us)
rt_period_us;
sched_rt_global_constraints(void)
!CONFIG_RT_GROUP_SCHED
global_rt_period();
new_bw;
(sysctl_sched_rt_runtime
old_runtime;
DEFINE_MUTEX(mutex);
mutex_lock(&mutex);
mutex_unlock(&mutex);
task))
*old_css,
min_cfs_quota_period
__cfs_schedulable(struct
quota)
toggle
(runtime_enabled
ns_to_ktime(period);
runtime_enabled;
cfs_quota_us)
ktime_to_ns(tg->cfs_bandwidth.period);
tg_set_cfs_bandwidth(tg,
quota_us;
tg->cfs_bandwidth.quota;
cfs_period_us)
cfs_period_us;
parent_quota
d);
.can_attach
wakeup_cpu;
wakeup_lock
wakeup_reset(struct
__wakeup_reset(struct
save_flags;
TRACE_DISPLAY_GRAPH
TRACER_OPT(display-graph,
trace_opts,
is_graph()
(tracer_flags.val
Prologue
atomic_inc_return(&(*data)->disabled);
(unlikely(disabled
atomic_dec(&(*data)->disabled);
(function_enabled
(!set
TRACE_ITER_FUNCTION)))
(!function_enabled)
register_wakeup_function(tr,
is_graph(),
unregister_wakeup_function(tr,
TRACE_ITER_FUNCTION)
trace_keep_overwrite(tracer,
tracing_is_enabled())
wakeup_set_flag(struct
(!(bit
TRACE_DISPLAY_GRAPH))
(!(is_graph()
set))
stop_func_tracer(tr,
!set);
wakeup_trace_open(struct
graph_trace_open(iter);
wakeup_trace_close(struct
(iter->private)
graph_trace_close(iter);
GRAPH_TRACER_FLAGS
wakeup_print_line(struct
__trace_function(struct
trace_graph_function(tr,
__trace_function
trace_function
trace_latency_header(s);
reported/recorded?
report_latency(struct
tracing_thresh)
tr->max_latency)
wakeup_task)
wakeup_current_cpu
entry->prev_pid
entry->prev_prio
prev->prio;
entry->prev_state
entry->next_pid
entry->next_prio
entry->next_state
entry->next_cpu
*wakee,
T0,
T1,
tracing_record_cmdline(prev);
atomic_inc_return(&per_cpu_ptr(wakeup_trace->trace_buffer.data,
per_cpu_ptr(wakeup_trace->trace_buffer.data,
__trace_function(wakeup_trace,
CALLER_ADDR1,
T0
T1-T0;
(likely(!is_tracing_stopped()))
__wakeup_reset(wakeup_trace);
atomic_dec(&per_cpu_ptr(wakeup_trace->trace_buffer.data,
success)
sched_dl
kernel_sched_wakeup\n");
kernel_sched_wakeup_new\n");
fail_deprobe;
pr_info("sched
fail_deprobe_wake_new;
fail_deprobe_wake_new:
unregister_trace_sched_wakeup_new(probe_wakeup,
fail_deprobe:
unregister_trace_sched_wakeup(probe_wakeup,
screws
wakeup_busy
lat_flag
overwrite_flag
TRACE_ITER_OVERWRITE;
lat_flag);
overwrite_flag);
compatibililty
robust-list
**entry,
*pi)
uaddr;
curr->robust_list
(very
carefully,
list!)
list-walking
*next_entry,
ROBUST_LIST_LIMIT,
pi,
pip;
uninitialized_var(next_pi);
futex_offset;
sys_set_robust_list()):
&head->list.next,
&pi))
offset:
(get_user(futex_offset,
&head->futex_offset))
lock-add
&head->list_op_pending,
&pip))
next_entry
&head->list)
handle_futex_death:
&next_pi);
twice:
pi))
uentry
next_entry;
next_pi;
excessively
lists:
(!--limit)
pip);
sizeof(*head)))
head_ptr,
len_ptr)
(!ptrace_may_access(p,
(put_user(sizeof(*head),
len_ptr))
head_ptr);
FUTEX_WAIT
FUTEX_LOCK_PI
FUTEX_WAIT_REQUEUE_PI))
(!timespec_valid(&ts))
timespec_to_ktime(ts);
FUTEX_WAIT)
ktime_add_safe(ktime_get(),
FUTEX_REQUEUE
FUTEX_CMP_REQUEUE
FUTEX_WAKE_OP)
do_futex(uaddr,
Real-Time
do_sched_rt_period_timer(struct
def_rt_bandwidth;
rt_b->rt_period);
init_rt_bandwidth(struct
*rt_b)
(!rt_bandwidth_enabled()
push_irq_work_func(struct
init_rt_rq(struct
rt_rq->push_cpu
rt_entity_is_task(rt_se)
*rt_task_of(struct
container_of(rt_se,
*rq_of_rt_rq(struct
rt_rq->rq;
*rt_rq_of_se(struct
rt_se->rt_rq;
*rq_of_rt_se(struct
(tg->rt_se)
init_tg_rt_entry(struct
rt_se->rt_rq
rt_rq,
rt_se,
need_pull_rt_task(struct
rq->rt.highest_prio.curr
inc_rt_migration(struct
(!rt_entity_is_task(rt_se))
&rq_of_rt_rq(rt_rq)->rt;
update_rt_migration(rt_rq);
dec_rt_migration(struct
enqueue_pushable_task(struct
plist_del(&p->pushable_tasks,
dequeue_pushable_task(struct
(has_pushable_tasks(rq))
plist_first_entry(&rq->rt.pushable_tasks,
pushable_tasks);
enqueue_top_rt_rq(struct
dequeue_top_rt_rq(struct
sched_rt_runtime(struct
sched_rt_period(struct
*rt_rq_iter_t;
(&tg->list
(rt_rq
*group_rt_rq(struct
enqueue_rt_entity(struct
dequeue_rt_entity(struct
sched_rt_rq_enqueue(struct
rt_rq->tg->rt_se[cpu];
enqueue_top_rt_rq(rt_rq);
enqueue_rt_entity(rt_se,
sched_rt_rq_dequeue(struct
dequeue_top_rt_rq(rt_rq);
dequeue_rt_entity(rt_se);
rt_rq_throttled(struct
(rt_rq)
*sched_rt_period_rt_rq(struct
*sched_rt_bandwidth(struct
borrow
neighbours.
rd->span)
raw_spin_lock(&iter->rt_runtime_lock);
rt_period)
lend
(unlikely(!scheduler_running))
reclaim.
can.
pretending
sched_rt_rq_enqueue(rt_rq);
balance_runtime(struct
cpusets,
balance_runtime(rt_rq);
rt_rq->rt_nr_running)
sched_rt_runtime(rt_rq);
&rt_sched_class)
BUG_ON(&rq->rt
(!rt_rq->rt_queued)
rt_rq->rt_nr_running);
inc_rt_prio_smp(struct
(&rq->rt
(rq->online
dec_rt_prio_smp(struct
inc_rt_prio(struct
dec_rt_prio(struct
sched_find_first_bit(array->bitmap);
inc_rt_group(struct
(rt_se_boosted(rt_se))
dec_rt_group(struct
*group_rq
rt_rq->rt_nr_running
rt_se_nr_running(rt_se);
*queue
back;
dequeue_rt_stack(rt_se);
__enqueue_rt_entity(rt_se,
enqueue_top_rt_rq(&rq->rt);
find_lowest_rq(struct
lock?
optimistic,
p->prio))
non-migratable
readily
rq->curr->prio
*queue;
MAX_RT_PRIO);
pull_rt_task(rq);
dl
(!has_pushable_tasks(rq))
consult
(lowest_rq->rt.highest_prio.curr
retrying
BUG_ON(!rt_task(p));
pick_next_pushable_task(rq);
find_lock_lowest_rq
rq->cpu.
(rq->rt.push_flags
RT_PUSH_IPI_EXECUTING)
rq->rt.push_flags
raw_spin_unlock(&rq->rt.push_lock);
Pass
(unlikely(cpu
(low
push_rt_tasks(rq);
(rq->rt.overloaded)
p->prio)
RLIMIT_RTTIME);
sched_rr_timeslice;
print_rt_rq(struct
print_rt_stats(struct
tick_device,
tick_cpu_device);
tick_period;
tick_setup_periodic(struct
tick_handle_periodic(struct
tick_check_new_device(struct
tick_shutdown(unsigned
tick_check_replacement(struct
tick_install_replacement(struct
*tick_get_device(int
clockevents_tick_resume(struct
clockevents_shutdown(struct
clockevents_exchange_device(struct
clockevents_set_state(struct
clockevents_program_event(struct
clockevents_handle_noop(struct
__clockevents_update_freq(struct
sysfs_get_uname(const
tick_suspend_broadcast(void)
tick_resume_broadcast(void)
tick_resume_check_broadcast(void)
tick_broadcast_init(void)
tick_suspend(void)
tick_resume(void)
tick_switch_to_oneshot(void
tick_oneshot_possible(void)
tick_resume_oneshot(void)
tick_oneshot_notify(void)
tick_oneshot_mode_active(void)
tick_clock_notify(void)
allow_nohz)
*bc);
!(BROADCAST
tick_broadcast_switch_to_oneshot(void)
tick_broadcast_oneshot_active(void)
tick_check_oneshot_broadcast_this_cpu(void)
tick_broadcast_oneshot_available(void)
tick_nohz_init(void)
<linux/uts.h>
*uts_ns;
uts_ns
Clone
ERR_PTR(-ENOMEM)
kfree(ns);
tsk's
uts_namespace,
get_uts_ns(ns);
Table
"Data
BP_HARDWARE_BREAKPOINT;
nextarg++;
bp->bph_length,
bp->bp_type);
bp->bp_installed
bp->bp_delayed
installed.
bp->bp_installed);
(bp->bp_delay
bp);
kdb_bp_install(struct
&kdb_breakpoints[i];
bp_enabled
bp->bp_enabled);
KDB_SP_DEFAULT);
kdb_printf("\n
bpno
bpno++,
kdb_printbp(bp,
bpno);
KDB_MAXBPT)
template.bp_type
[bd|bc|be]
cmd;
KDBCMD_BC
highbp++;
criteria
breakpoint).
ss
go.
breakpoints",
_PRINTK_BRAILLE_H
braille_set_options(struct
_braille_console_setup(char
**brl_options)
deferred,
ETT_NONE
defer.
tt;
tt)
(data->cmd_ops->trigger_type
SHOW_AVAILABLE_TRIGGERS
*event_file
SHOW_AVAILABLE_TRIGGERS)
*event_file;
->stop()
->start()
(unlikely(!event_file))
*command,
(strcmp(p->name,
list_add(&cmd->list,
@m:
themselves.
":unlimited");
":count=%ld",
(filter_str)
@init
data->ref++;
@free
trigger_data_free(data);
trace_event_enable_disable(file,
trigger,
sm_ref
registration.
set_cond
@reg
*test;
list_for_each_entry_rcu(test,
(data->ops->init)
data->ops->init(data->ops,
list_add_rcu(&data->list,
&file->triggers);
ret++;
(trace_event_trigger_enable_disable(file,
ret--;
@unreg
*test,
(unregistered
data->ops->free)
instantiation.
*trigger_data;
*trigger_ops;
[if
filter])
trigger_ops
cmd_ops->get_trigger_ops(cmd,
trigger);
trigger_data
kzalloc(sizeof(*trigger_data),
(!trigger_data)
trigger_data->count
trigger_data->ops
trigger_ops;
trigger_data->cmd_ops
cmd_ops;
INIT_LIST_HEAD(&trigger_data->list);
cmd_ops->unreg(glob+1,
(trigger)
&trigger_data->count);
non-empty,
(!cmd_ops->set_filter)
cmd_ops->set_filter(param,
cmd_ops->reg(glob,
(cmd_ops->set_filter)
cmd_ops->set_filter(NULL,
@set_filter
(!filter_str)
data->filter_str
traceon_trigger_print,
traceoff_trigger_print,
(strcmp(cmd,
"traceon")
"traceon",
ETT_TRACE_ONOFF,
onoff_get_trigger_ops,
"traceoff",
snapshot_trigger_print,
register_trigger_snapshot_cmd(void)
STACK_SKIP
stacktrace_trigger_print,
register_trigger_stacktrace_cmd(void)
typos
"enable_event"
DISABLE_EVENT_STR
"disable_event"
&enable_data->file->flags);
"%s:%s:%s",
enable_data->enable
data->count);
event_enable_trigger,
event_enable_count_trigger,
*enable_data;
find_event_file(tr,
enable_data
trace_event_enable_disable(event_enable_file,
out_disable:
test->private_data;
ENABLE_EVENT_STR,
ETT_EVENT_ENABLE,
event_enable_trigger_func,
event_enable_register_trigger,
event_enable_unregister_trigger,
event_enable_get_trigger_ops,
spaces)
(WARN_ON(!trace->entries))
utilized
yet.\n");
Future
info->ctr_mask;
gcov_fn_info)
num_counter_active(info)
int))
ALIGN(size,
__alignof__(struct
in-memory
iter->count
content.
(iter->record)
RECORD_FILE_MAGIC:
RECORD_GCOV_VERSION:
RECORD_FUNCTION_TAG:
RECORD_FUNCTON_TAG_LEN:
RECORD_FUNCTION_IDENT:
RECORD_COUNT_TAG:
RECORD_COUNT:
RECORD_COUNT_LEN:
RECORD_FUNCTION_CHECK:
RECORD_TIME_STAMP:
restricting
July
Oracle,
resolve
KVM
(!prof_on)
cpumask_copy(prof_cpu_mask,
profile_type
PROFILE_TASK_EXIT:
&task_exit_notifier,
PROFILE_MUNMAP:
&munmap_notifier,
context).
opposed
mutex_lock(&profile_flip_mutex);
get_cpu());
on_each_cpu(__profile_flip_buffers,
*hits
NR_PROFILE_HIT;
atomic_add(hits[i].hits,
&prof_buffer[hits[i].pc]);
hits[i].hits
mutex_unlock(&profile_flip_mutex);
do_profile_hits(int
long)_stext)
(NR_PROFILE_GRP
PROFILE_GRPSHIFT;
write-queue
j].pc
j].hits
nr_hits;
atomic_add(nr_hits,
cpu_to_mem(cpu);
(!per_cpu(cpu_profile_hits,
(prof_cpu_mask
prof_cpu_mask);
cpu)[0]);
buf))
frequency,
sizeof(atomic_t));
__GFP_THISNODE,
out_cleanup;
*)page_address(page);
S_IWUSR
proc_set_size(entry,
2006-2007
Distribute
tick_sched,
jiffies_lock.
*tick_get_tick_sched(int
last_jiffies_update);
ktime_add(last_jiffies_update,
prompt.
ticks.
profile_tick(CPU_PROFILING);
"more
running\n");
tick_nohz_restart_sched_tick(struct
Re-evaluate
(ts->tick_stopped
(!can_stop_full_tick())
tick_nohz_restart_sched_tick(ts,
ktime_get());
__tick_nohz_full_check();
(!tick_nohz_full_cpu(smp_processor_id()))
(!tick_nohz_full_cpu(cpu))
(!tick_nohz_full_running)
tick_nohz_full_mask)
nohz_full
(tick_nohz_full_running
NOTIFY_BAD;
"NO_HZ:
cpumask_clear(tick_nohz_full_mask);
pr_warning("NO_HZ:
Clearing
context_tracking_cpu_set(cpu);
%*pbl.\n",
aka
"off"))
"on"))
ktime_add(ts->iowait_sleeptime,
ktime_add(ts->idle_sleeptime,
ts->idle_entrytime
ts->idle_active
@last_update_time:
cummulative
microseconds.
sampling,
ktime_get()
(!tick_nohz_active)
update_ts_time_stats(cpu,
last_update_time);
ts->idle_sleeptime;
ts->iowait_sleeptime;
(!ts->tick_stopped
ts->do_timer_last
expires.tv64
ts->tick_stopped
KTIME_MAX,
(unlikely(expires.tv64
next_jiffies;
ts->sleep_length
is_idle_task(current))
NOHZ_MODE_INACTIVE)
tick_nohz_stop_sched_tick(ts,
ktime_get(),
Boot
ts->tick_stopped)
rcu_idle_enter()
*ts;
WARN_ON_ONCE(irqs_disabled());
tick_nohz_stop_sched_tick()
ts->inidle
__tick_nohz_idle_enter(ts);
fires
hrtimer_start_expires(&ts->sched_timer,
tick_nohz_restart(ts,
randomly
(ticks
tick_nohz_stop_idle(ts,
tick_sched_do_timer(now);
tick_sched_handle(ts,
(unlikely(ts->tick_stopped))
tick_nohz_switch_to_nohz(void)
hrtimer_forward
hrtimer_init(&ts->sched_timer,
tick_nohz_irq_enter(void)
hrtimers:
(per
Async
happened,
cyclic
softirq)
_KERNEL_TIME_TIMEKEEPING_H
ktime_get_update_offsets_tick(ktime_t
*offs_tai);
ktime_get_update_offsets_now(ktime_t
timekeeping_inject_offset(struct
*ts);
timekeeping_set_tai_offset(s32
tai_offset);
do_timer(unsigned
adapt
long)(v
%u",
explanation.
hotplugged
cpumask_*
*offset;
cpumask_next(n
long)(n
(*offset)++;
Morton
Made
alloc_percpu
managed.
backing
"workqueue_internal.h"
behaves
worker_attach_to_pool()
POOL_DISASSOCIATED
WORKER_CPU_INTENSIVE
WORKER_UNBOUND
WORKER_NOT_RUNNING
MAX_IDLE_WORKERS_RATIO
MAYDAY_INTERVAL
100ms
Rescue
HIGHPRI_NICE_LEVEL
pool->attach_mutex
Sched-RCU
reads.
nr_idle;
idle_list,
arbitration
purely
informational
attach/detach
refcnt;
*rescuer;
*dfl_pwq;
*wq_dev;
wq_pool_mutex.
sysrq.
WQ_POWER_EFFICIENT
instantiating
worker_thread(void
copy_workqueue_attrs(struct
rcu_lockdep_assert(rcu_read_lock_sched_held()
held")
&per_cpu(cpu_worker_pools,
assert_rcu_or_pool_mutex();
pool_workqueues
assert_rcu_or_wq_mutex(wq);
cancel_work_sync(work);
debug_object_activate(work,
debug_object_free(work,
debug_work_activate(struct
debug_work_deactivate(struct
WORK_OFFQ_POOL_NONE)
pool->id
(*work_data_bits(work)
(color
WORK_NR_COLORS;
get_work_pool()
extra_flags)
extra_flags);
pool_id)
long)pool_id
WORK_OFFQ_POOL_SHIFT,
WORK_STRUCT_PENDING);
wmb
*)(data
worker?
!list_empty(&pool->worklist)
working?
pool->nr_idle
pool->nr_workers
worker.
first_idle_worker(pool);
spin_lock_irq(rq->lock)
wq_worker_waking_up(struct
(!(worker->flags
Worker
*wq_worker_sleeping(struct
raw_smp_processor_id()
counterpart
@flags
WARN_ON_ONCE(worker->task
NOT_RUNNING,
oflags
worker->flags;
tricky.
Well,
*pool,
(worker->current_work
@head.
pool->lock.
lockdep_assert_held(&pwq->pool->lock);
trace_workqueue_activate_work(work);
pwq->nr_active++;
pwq_activate_delayed_work(work);
pwq's
nr_in_flight
(!list_empty(&pwq->delayed_works))
pwq->max_active)
pwq_activate_first_delayed(pwq);
@work,
@work's
is_dwork,
to_delayed_work(work);
pwq->pool
debug_work_deactivate(work);
pwq->nr_active
list_del_init(&work->entry);
pwq_dec_nr_in_flight(pwq,
(work_is_canceling(work))
lying
@wq.
worker->current_pwq->wq
WORK_CPU_UNBOUND)
last_pool
worker->current_pwq;
spin_lock(&pwq->pool->lock);
determined,
work_flags
insert_work(pwq,
__queue_work(cpu,
__data)
__queue_work(dwork->cpu,
dwork->wq,
&dwork->work;
correctness.
@delay:
__queue_work()
__queue_delayed_work(cpu,
dwork,
modified.
try_to_grab_pending(&dwork->work,
-EAGAIN));
WORKER_IDLE)
worker->last_active
LIFO
mod_timer(&pool->idle_timer,
WARN_ON_ONCE(!(pool->flags
WORKER_IDLE)))
pool->nr_idle--;
list_del_init(&worker->entry);
attached,
It'll
pool->attrs->cpumask);
WORKER_UNBOUND;
leftover
WORKER_REBOUND);
snprintf(id_buf,
sizeof(id_buf),
pool->attrs->nice
pool->attrs->nice);
meddling
worker_enter_idle(worker);
ida_simple_remove(&pool->worker_ida,
kfree(worker);
__pool)
*)__pool;
destroy_worker(worker);
&wq->maydays);
spin_lock(&wq_mayday_lock);
spin_unlock(&wq_mayday_lock);
mod_timer(&pool->mayday_timer,
need_to_create_worker()
may_start_working()
__releases(&pool->lock)
__acquires(&pool->lock)
del_timer_sync(&pool->mayday_timer);
role
works,
Anyone
pool->manager
mutex_unlock(&pool->manager_arb);
interaction
"held
freed"
lockdep_copy_map(&lockdep_map,
find_worker_executing_work(pool,
worker->current_pwq
(unlikely(cpu_intensive))
worker_set_flags(worker,
WORKER_CPU_INTENSIVE);
(need_more_worker(pool))
lock_map_acquire_read(&pwq->wq->lockdep_map);
lock_map_acquire(&lockdep_map);
lock_map_release(&lockdep_map);
lock_map_release(&pwq->wq->lockdep_map);
preempt_count(),
worker->current_func);
worker->desc_valid
process_one_work(worker,
explained
PF_WQ_WORKER;
am
~PF_WQ_WORKER;
process_scheduled_works(worker);
&worker->scheduled,
sleep:
rarely.
@wq->maydays
should_stop
spin_lock_irq(&wq_mayday_lock);
spin_unlock_irq(&wq_mayday_lock);
rescuer->pool
Slurp
queueing.
back-to-back
need_more_worker()
wq_barrier,
LINKED
@target.
executed,
*bits
non-negative,
accordingly,
non-negative
@work_color
atomic_set(&wq->nr_pwqs_to_flush,
pwq->work_color
work_next_color(wq->work_color);
wq->flush_color)
wq->work_color
wq->flush_color,
list_add_tail(&this_flusher.list,
&wq->flusher_queue);
flush_workqueue_prep_pwqs(wq,
Oops,
flusher_queue.
(wq->first_flusher
&this_flusher)
list_del_init(&next->list);
cascading.
draining
flush_cnt
drain_workqueue()
drained;
drained
1000))
already_gone;
@max_active
CANCELING
-ENOENT))
considers
re-arm
cancel_work_sync()
schedule_on_each_cpu()
function!
great
need,
fn);
equality
->no_numa
get_unbound_pool()
@attr
zalloc'd
POOL_DISASSOCIATED;
free_workqueue_attrs(wq->unbound_attrs);
kfree(wq);
worker_pool,
@pool's
@attrs,
nope,
attribute,
(!create_worker(pool))
kmem_cache_free(pwq_cache,
call_rcu_sched(&wq->rcu,
rcu_free_wq);
@pwq->max_active
workqueue's
(!freezable
visible.
pool->node);
(!pwq)
init_pwq(pwq,
wq_attrs
@attrs->cpumask
@attrs->cpumask.
(!wq_numa_enabled
use_dfl;
cpumask_and(cpumask,
cpumask_copy(cpumask,
@wq's
numa_pwq_tbl[]
link_pwq(pwq);
rcu_access_pointer(wq->numa_pwq_tbl[node]);
__WQ_ORDERED)
new_attrs
sanitize
differing
pools.
enomem_pwq;
*cpumask;
wq->unbound_attrs);
wq's,
use_dfl_pwq;
lim);
tbl_size
err_free_wq;
wq_clamp_max_active(max_active,
wq->saved_max_active
proceeding
Directly
refs.
put_pwq_unlocked(pwq);
@max_active.
(WARN_ON(wq->flags
__WQ_ORDERED))
rescuer.
hints
contested
work_func_t
PF_WQ_WORKER))
garbage.
(fn
name[0]
(work->func
has_in_flight
has_pending
pr_cont_pool_info(pool);
hash_for_each(pool->busy_hash,
bkt,
hentry)
(worker->current_pwq
pr_cont_work(comma,
!(*work_data_bits(work)
WORK_STRUCT_LINKED);
pi;
!list_empty(&pwq->delayed_works))
mix
Zap
served
Rebind
WARN_ON_ONCE(set_cpus_allowed_ptr(worker->task,
pool->attrs->cpumask)
~POOL_DISASSOCIATED;
worker_clr_flags()
before,
restored.
wq_update_unbound_numa(wq,
pool->lock's.
workqueue_freezing
WQ_SYSFS
wq_device,
scnprintf(buf
delim,
attrs->no_numa
.dev_groups
kfree(wq_dev);
dev_set_uevent_suppress(&wq_dev->dev,
device_unregister(&wq_dev->dev);
BUG_ON(!(attrs
alloc_workqueue_attrs(GFP_KERNEL)));
std_nice[i];
WQ_FREEZABLE,
WQ_POWER_EFFICIENT,
(n->priority
(*nl)->priority)
*nl;
next_nb;
NOTIFY_STOP_MASK)
Atomic
Registration
spinlock,
call_chain
spin_lock_irqsave(&nh->lock,
spin_unlock_irqrestore(&nh->lock,
down_write().
Raw
CONFIG_SRCU
mutex_lock().
mutex_lock(&nh->mutex);
mutex_unlock(&nh->mutex);
sorts
STACK_TRACE_ENTRIES
CC_USING_FENTRY
ip.
.max_entries
.entries
Location
entries)\n"
--------\n",
max_stack_trace.nr_entries;
(i+1
stack_dump_trace[i+1]
stack_dump_index[i];
stack_dump_index[i+1];
stack_dump_index[i],
(this_size
max_stack_size)
tracer_frame
max_stack_size
max_stack_trace.skip
are.
stack;
(top
MCOUNT_INSN_SIZE;
trace_ops
"%ld\n",
sizeof(buf);
cpu)++;
__next(m,
"#\n"
ftrace_filter_write,
!write
last_stack_tracer_enabled
(stack_tracer_enabled)
unregister_ftrace_function(&trace_ops);
COMMAND_LINE_SIZE);
2011,
gc->lock
*ct->mask_cache,
Unmask
Ack
EOI
gc->wake_active
*reg_base,
@num_ct:
defaults
sizeof(*gc)
irq_chip_type);
kzalloc(sz,
irq_init_generic_chip(gc,
handler);
*mskptr
mskreg
gc->num_ct;
irqs_per_chip,
sizeof(*dgc)
sizeof(gc);
(!dgc)
numchips;
list_add_tail(&gc->list,
&gc_list);
hw_irq)
*dgc
d->gc;
dgc->num_chips)
dgc->gc[idx];
(test_bit(idx,
(!gc->installed)
irq_gc_init_mask_cache(gc,
IRQ_GC_INIT_NESTED_LOCK)
&irq_nested_lock_class);
(chip->irq_calc_mask)
irq_set_chip_and_handler(virq,
ct->handler);
gc);
.xlate
irq_domain_xlate_onetwocell,
@gc:
@msk:
gc->irq_base.
msk,
raw_spin_lock(&gc_lock);
raw_spin_unlock(&gc_lock);
msk;
msk
(!(msk
0x01))
irq_set_chip_data(i,
irq_modify_status(i,
CONFIG_PM
<linux/irqdesc.h>
irq_domain_alloc_descs(int
of_node_to_nid(of_node));
domain->of_node
domain->revmap_size
domain->name);
@first_irq:
virqs
first_irq.
hwirqs
__irq_domain_add(of_node,
(!domain)
translation.
'0',
map()
first_hwirq,
first_hwirq
*found
though...
&irq_domain_list,
(h->of_node
irq_default_domain
irq))
PIC
radix_tree_delete(&domain->revmap_tree,
virq%i
telling
domain->name,
assigned,
chip's
(!domain->name
domain->name
radix_tree_insert(&domain->revmap_tree,
irq_clear_status_flags(virq,
of_node_full_name(domain->of_node),
simplest
(!virq)
pr_debug("create_direct
domain->revmap_direct_max_irq)
domain->revmap_direct_max_irq);
(irq_domain_associate(domain,
nececssary
(virq)
establish
CONFIG_IRQ_DOMAIN_DEBUG
%-10s
%-*s
"chip
name",
intspec[1]
Optional
data->domain;
data->hwirq;
*irq_data,
irq_get_irq_data(virq
(!irq_data)
irq_domain_get_irq_data
*irq_domain_get_irq_data(struct
@chip_data:
irq_data->chip_data
irq_domain_set_hwirq_and_chip(domain,
__irq_set_handler(virq,
irq_set_handler_data(virq,
(irq_domain_is_auto_recursive(domain))
irq_domain_free_irqs_recursive(domain->parent,
pr_debug("cannot
IRQ%d\n",
out_free_desc;
irq_domain_alloc_irqs_recursive(domain,
out_free_desc:
irq_free_descs(virq,
"NULL
alloc()
domain_ops->activate
irq_data->domain)
(irq_data->parent_data)
domain_ops->deactivate
cpu_nr
#,
xchg(&node->next,
this_cpu_ptr(&osq_node);
node->next
OSQ_UNLOCKED_VAL)
@prev->next
unlock(),
@next
osq_wait_next(lock,
uncontended
WRITE_ONCE(next->locked,
2008-2014
tp_probes,
**funcs,
*tp_func)
*funcs;
old[nr_probes].func;
nr_probes++)
(old[nr_probes].func
old[nr_probes].data
probe,
allocate_probes(nr_probes
new[nr_probes
M
*tp_funcs;
tp_funcs
rcu_dereference_protected(tp->funcs,
lockdep_is_held(&tracepoints_mutex));
rcu_assign_pointer(tp->funcs,
tp_funcs);
release_probes(old);
@probe:
*probe,
tp_func;
mutex_lock(&tracepoints_mutex);
tp_func.func
probe;
tp_func.data
&tp_func);
mutex_unlock(&tracepoints_mutex);
mod->taints
tp_module"
leaks
*begin,
(!begin)
(!mod->num_tracepoints)
blocking_notifier_call_chain(&tracepoint_notify_list,
mod->tracepoints_ptrs
"tainted"
taints
*end,
(*fct)(struct
(!sys_tracepoint_refcount)
for_each_process_thread(p,
TIF_SYSCALL_TRACEPOINT);
gidsetsize;
group_info;
long)group_info->blocks[i]);
kfree(group_info);
from_kgid_munged(user_ns,
gidsetsize
base++)
mid)))
@group_info:
impose
task_lock
(!gid_eq(grp,
groups_search(cred->group_info,
cred->egid))
rp;
*symbol;
tk->rp.handler
"unknown";
register_kprobe_event(struct
*tk);
unregister_kprobe_event(struct
kprobe_dispatcher(struct
kretprobe_dispatcher(struct
sc->addr
(sc->addr)
kfree(sc);
*alloc_symbol_cache(const
int)((unsigned
retval))
get_rloc_offs(*(u32
*)dest));
string_size));
(!tk)
(symbol)
(!tk->symbol)
tk->rp.kp.addr
kfree(tk->tp.call.name);
kfree(tk->symbol);
kfree(tk);
"perf"
"trace"
Ignored
k*probes
(trace_probe_is_registered(&tk->tp))
tk->rp.kp.flags
trace_kprobe_symbol(tk),
tk->rp.kp.addr);
__unregister_trace_kprobe(tk);
__register_trace_kprobe(tk);
ftrace_event_name(&tk->tp.call),
kprobe:
Nth
kernel)
+|-
Type
TYPE
unregister_trace_kprobe(tk);
pr_info("Return
symbol.\n");
0x%p",
release_all_trace_kprobes();
event_trigger_unlock_commit_regs(ftrace_file,
&tk->tp.files,
Kretprobe
entry->ret_ip
long)ri->ret_addr;
trace_probe,
call.event);
ftrace_event_name(&tp->call));
TRACE_ITER_SYM_OFFSET))
*)&field[1];
(!tp->args[i].type->print(s,
tp->args[i].offset,
field))
*)event_call->data;
sizeof(field)
call->prog;
__size,
(prog
!trace_call_bpf(prog,
__size
ALIGN(__size
lockless,
tk->nhit++;
tweek
a1,
a2,
a3,
kprobe_trace_selftest_target
find_trace_kprobe("testprobe",
find_trace_kprobe("testprobe2",
(warn)
kernel/power/autosleep.c
autosleep_state;
mutex_lock_interruptible()
mutex_lock(&autosleep_lock);
queue_up_suspend_work();
pm_autosleep_state(void)
pm_autosleep_lock(void)
pm_autosleep_unlock(void)
pm_autosleep_set_state(suspend_state_t
pm_autosleep_init(void)
optional.
Iteration
register_stat_tracer(struct
unregister_stat_tracer(struct
written?
contents.
trace_seq_printf
formating
(@s).
seq_buf_vprintf(&s->seq,
wasting
(@s)
seq_buf_putmem(&s->seq,
@c:
(TRACE_SEQ_BUF_LEFT(s)
@mem:
Copies
@list:
subdirectory
debugfs.
**loaded_info;
restrictive
etc.),
accumulating
gcov_iter_free(iter);
gcov_info_free(info);
&all_head,
all)
remove_node(struct
write()
get_node_by_name(gcov_info_filename(info));
reset_node(node);
*ext)
*copy;
(!copy)
(dir)
kasprintf(GFP_KERNEL,
ext);
"%s.%s",
Construct
rel,
sizeof(SKEW_PREFIX)
(!node->links)
(!target)
kfree(target);
(i--
debugfs_remove(node->links[i]);
kfree(node->links);
strlen(name)
err_nomem;
node->dentry
parent->dentry);
file\n");
gcov_info_free(node->unloaded_info);
read()
(strcmp(curr,
new_node(parent,
err_remove:
(out
memory)\n",
node->unloaded_info
(!node->unloaded_info)
found)\n",
Shrink
CONFIG_LATENCYTOP
stringified
times)
latencytop_enabled;
(!latencytop_enabled)
*lat)
firstnonnull
MAXLR;
record)
backtrace:
(record
(same)
lat->time;
latency_record));
interruptible,
specifically,
Long
5000)
&lat);
lat.time;
2002-2003
MAX_SCHEDULE_TIMEOUT
utp,
compat_timex))
&utp->modes)
&utp->offset)
&utp->freq)
&utp->maxerror)
&utp->esterror)
&utp->status)
&utp->constant)
&utp->precision)
&utp->tolerance)
&utp->time.tv_sec)
&utp->time.tv_usec)
&utp->tick)
&utp->ppsfreq)
&utp->jitter)
&utp->shift)
&utp->stabil)
&utp->jitcnt)
&utp->calcnt)
&utp->errcnt)
ktv;
do_gettimeofday(&ktv);
tv))
(copy_to_user(tz,
&sys_tz,
sizeof(sys_tz)))
user_tv;
new_ts;
new_tz;
new_ts.tv_sec
user_tv.tv_sec;
new_ts.tv_nsec
user_tv.tv_usec
(copy_from_user(&new_tz,
tz,
sizeof(*tz)))
do_sys_settimeofday(tv
&new_ts
tz
&new_tz
*ctv)
ctv,
sizeof(*ctv))
&ctv->tv_sec)
&ctv->tv_usec))
cts,
sizeof(*cts))
&cts->tv_sec)
&cts->tv_nsec))
*utv)
sizeof(*tv))
utv);
*uts)
sizeof(*ts))
uts);
*kts
uts;
restart->nanosleep.compat_rmtp;
compat_put_timespec(&rmt,
rqtp))
(!timespec_valid(&tu))
hrtimer_nanosleep(&tu,
-ERESTART_RESTARTBLOCK
therefor
nevertheless
*i)
do_getitimer(which,
do_setitimer(which,
compat_tms
tbuf)
(tbuf)
(copy_to_user(tbuf,
&r,
COMPAT_RLIM_OLD_INFINITY)
__put_user(r.rlim_cur,
__put_user(r.rlim_max,
*ru)
set_fs
(stat_addr
(siginfo_t
compat_sigevent
created_timer_id)
(timer_event_spec)
setting)
compat_put_timespec(&ts,
utp)
compat_get_timex(&txc,
utp);
compat_put_timex(utp,
*rmtp
compat_clock_nanosleep_restart;
*umask,
bitmap_size)
um;
nr_compat_longs;
ALIGN(bitmap_size,
umask,
nr_compat_longs
BITS_TO_COMPAT_LONGS(bitmap_size);
BITS_TO_LONGS(bitmap_size);
sizeof(m)/sizeof(um);
(nr_compat_longs--
umask))
um
umask++;
4*sizeof(um);
compat->sig[6]
compat->sig[4]
compat->sig[2]
compat->sig[0]
(copy_siginfo_to_user32(uinfo,
__ARCH_WANT_COMPAT_SYS_TIME
tloc)
(tloc)
(put_user(i,tloc))
tptr)
(get_user(tv.tv_sec,
tptr))
tv.tv_nsec
security_settime(&tv,
do_settimeofday(&tv);
do_adjtimex(&txc);
old_nodes,
nr_bits
(compat_get_bitmap(nodes_addr(tmp_mask),
nr_bits))
(new_nodes)
nodes_addr(tmp_mask),
Arnaldo
Carvalho
Melo
<acme@redhat.com>
<generated/utsrelease.h>
<linux/splice.h>
insertions
ftrace_dump_mode
trace_enum_map_head
trace_enum_map_tail
beginning,
tracing_set_tracer(struct
allocate_snapshot;
((strcmp(str,
"=0")
strcmp(str,
"=off")
tracepoint_printk
linking
(tr
(!buf->buffer)
mirror
multi
trace_access_lock(int
Secondly
trace_access_unlock(int
trace_access_lock_init(void)
(void)cpu;
irqsoff),
tracers,
tr->buffer_disabled
tracing_off.
\n
entry->buf[size]
tracing_snapshot(void)
SNAPSHOT
update_max_tr(tr,
EXPORT_SYMBOL_GPL(tracing_snapshot);
resize_buffer_duplicate_size(struct
*trace_buf,
*size_buf,
set_buffer_entries(struct
set_buffer_entries(&tr->max_buffer,
tracing_alloc_snapshot(void)
EXPORT_SYMBOL_GPL(tracing_alloc_snapshot);
tracing_snapshot_alloc(void)
EXPORT_SYMBOL_GPL(tracing_snapshot_alloc);
"Snapshot
used");
buf_size
memparse(str,
&str);
nr_entries
isspace(ch))
trace_parser'
trace_parser_clear(parser);
parser->buffer[parser->idx++]
parser->buffer[parser->idx]
*trace_buf
current_uid(),
max_data->uid
tsk->policy;
(tr->stop_count)
WARN_ON_ONCE(tr->current_trace
&nop_trace);
__update_max_tr(tr,
Iterators
static,
(trace_buffer_iter(iter,
iter->cpu_file))
run_tracer_selftest(struct
(type->use_max_tr)
@type
plugin
MAX_TRACER_SIZE)
tracing_selftest_running
t->next)
registered\n",
buf->buffer;
buf->cpu);
kmalloc(val
kfree(s->map_cmdline_to_pid);
NO_CMDLINE_MAP,
raw_spin_lock_irqsave(&global_trace.start_lock,
screwed
arch_spin_lock(&global_trace.max_lock);
global_trace.max_buffer.buffer;
arch_spin_unlock(&global_trace.max_lock);
raw_spin_unlock_irqrestore(&global_trace.start_lock,
raw_spin_lock_irqsave(&tr->start_lock,
raw_spin_unlock_irqrestore(&tr->start_lock,
tsk->comm);
comm[])
PID_MAX_DEFAULT)
"<...>");
__this_cpu_write(trace_cmdline_save,
0xff;
((pc
ring_buffer_lock_reserve(buffer,
ring_buffer_unlock_commit(buffer,
ftrace_trace_userstack(buffer,
__trace_buffer_unlock_commit(buffer,
**current_rb,
*ftrace_file,
temp_buffer
(!entry
&event_function;
stack_entry
variables,
save_stack_trace_regs(regs,
trace.nr_entries;
memset(&entry->caller,
entry->caller;
TRACE_ITER_STACKTRACE))
__ftrace_trace_stack(buffer,
skip)
(tracing_disabled
tracing_selftest_running)
more,
does)
current->tgid;
UNUSED
(in_softirq())
pr_warning("**********************************************************\n");
Allocating
Expand
global_trace.buffer
(!buffers_allocated)
*tbuffer;
trace_vprintk
tbuffer
get_trace_buf();
(!tbuffer)
sizeof(u32)
tbuffer,
*buf_iter
ring_buffer_read(buf_iter,
ring_buffer_iter_peek(buf_iter,
ring_buffer_peek(iter->trace_buffer->buffer,
*ent_ts)
iter->trace_buffer->buffer;
next_lost
next_size
(ring_buffer_empty_cpu(buffer,
peek_next_entry(iter,
(ent_cpu)
*ent_cpu
(ent
lost_events;
iter->ent_size;
next_ts;
__find_next_entry(iter,
ring_buffer_consume(iter->trace_buffer->buffer,
*s_next(struct
trace_find_next_entry_inc(iter);
*buf_iter;
per_cpu_ptr(iter->trace_buffer->data,
cpu)->skipped_entries
iter->tr;
current_trace,
iter->trace->name
*iter->trace
*tr->current_trace;
iter->trace->use_max_tr)
(iter->leftover)
s_stop(struct
*total,
*entries
stamp.
irqs-off
need-resched
|||
preempt-depth
|||||
\\
\n");
get_total_entries(buf,
&total,
&entries);
print_event_info(buf,
FUNCTION\n"
*type
latency:
")\n");
-----------------\n");
seq_print_ip_sym(&iter->seq,
event->funcs->trace(iter,
sym_flags,
"Unknown
entry->pid);
(!ring_buffer_iter_empty(buf_iter))
(!ring_buffer_empty_cpu(iter->trace_buffer->buffer,
trace_event_read_lock()
(trace_seq_has_overflowed(&iter->seq))
TRACE_BPUTS
TRACE_BPRINT
trace_print_bprintk_msg_only(iter);
TRACE_PRINT
trace_print_printk_msg_only(iter);
print_trace_header(m,
print_lat_help_header(m);
TRACE_ITER_IRQ_INFO)
TRACING
MAY
MISSING
EVENTS\n");
allocated.\n"
(Doesn't
that\n"
'1')\n");
print_snapshot_help(struct
"#\n#
*\n#\n");
s_show(struct
tracer:
s_stop,
snapshot)
mutex_init(&iter->mutex);
iter->trace->open)
iter->trace->open(iter);
Annotate
(ring_buffer_overruns(iter->trace_buffer->buffer))
TRACE_FILE_ANNOTATE;
iter->buffer_iter[cpu]
ring_buffer_read_prepare(iter->trace_buffer->buffer,
ring_buffer_read_prepare_sync();
ring_buffer_read_start(iter->buffer_iter[cpu]);
kfree(iter->buffer_iter);
seq_release_private(inode,
(!(file->f_mode
mutex_destroy(&iter->mutex);
free_cpumask_var(iter->started);
__tracing_open(inode,
(IS_ERR(iter))
PTR_ERR(iter);
tr))
t->next;
get_tracer_for_array(tr,
t->name);
whence)
file_inode(filp)->i_private;
mutex_lock(&tracing_cpumask_update_lock);
mutex_unlock(&tracing_cpumask_update_lock);
tracing_cpumask_new);
tr->tracing_cpumask)
!cpumask_test_cpu(cpu,
tracing_cpumask_new))
cpumask_copy(tr->tracing_cpumask,
free_cpumask_var(tracing_cpumask_new);
trace_opts
trace_options[i]);
"no%s\n",
trace_opts[i].name);
*trace
!neg);
tracer_flags->val
TRACE_ITER_OVERWRITE)
TRACE_ITER_RECORD_CMD)
"no",
tracing_single_release_tr,
file:
trace\n"
local:
global:
TSC
accepts:
func_full_name,
*func_end,
func_begin*,
*func_middle*\n"
modules:
trigger:
traceon,
traceoff\n"
enable_event:<system>:<event>\n"
disable_event:<system>:<event>\n"
stacktrace\n"
snapshot\n"
decrement.
(function_graph)\n"
(0
filter\t\t-
block_unplug
hit.\n"
<system>/<event>/trigger\n"
ptr++)
*v;
ptr->tail.next;
update_enum_map(ptr);
(WARN_ON_ONCE(!ptr))
trace_insert_enum_map_file(struct
map_array;
map_array++;
trace_create_enum_file(struct
ring_buffer_resize(trace_buf->buffer,
cpu)->entries;
expanding
(!(tr->flags
does,
set_buffer_entries(&tr->trace_buffer,
(!cpumask_test_cpu(cpu_id,
__tracing_resize_ring_buffer(tr,
create_trace_option_files(struct
destroy_trace_option_files(struct
(tr->current_trace
&nop_trace)
tr->current_trace->enabled--;
(tr->current_trace->reset)
tr->current_trace->reset(tr);
*topts;
topts
(strcmp(t->name,
current_trace
sufficient.
free_snapshot(tr);
tr->current_trace->ref++;
trace_poll(iter,
O_NONBLOCK))
EOF
trace_seq_to_user(&iter->seq,
tracing_wait_pipe(filp);
iter->seq.seq.len;
TRACE_TYPE_PARTIAL_LINE)
trace_consume(iter);
iter->ent->type);
generic_pipe_buf_get,
overflowed.
*pages_def[PIPE_DEF_BUFFERS];
partial_def[PIPE_DEF_BUFFERS];
pages_def,
partial_def,
.nr_pages
pipe,
spd.nr_pages_max
spd.pages[i]
alloc_page(GFP_KERNEL);
spd.partial[i].offset
spd.partial[i].len
buf_size_same
"%lu
(expanded:
(!val)
tracing_resize_ring_buffer(tr,
expanded_size
tracer_tracing_off(tr);
*fpos)
PAGE_MASK))
Ring
map_page[0]
entry->buf[cnt]
*fpos
ARRAY_SIZE(trace_clocks);
"["
trace_clocks[i].func);
(tr->allocated_snapshot)
update_max_tr_single(tr,
kfree(m->private);
tracing_buffers_open(struct
tracing_buffers_read(struct
tracing_buffers_release(struct
tracing_buffers_splice_read(struct
info->iter.trace_buffer
tracing_buffers_read,
tracing_buffers_release,
tracing_buffers_splice_read,
int)-1;
iter->tr->current_trace->use_max_tr)
(!info->spare)
*)buf->private;
(--ref->ref)
ring_buffer_entries_cpu(iter->trace_buffer->buffer,
land.
spd.nr_pages++;
(!spd.nr_pages)
((file->f_flags
O_NONBLOCK)
usec_rem;
trace_clock
"oldest
%5llu.%06lu\n",
"read
defined(CONFIG_TRACER_SNAPSHOT)
(!*count)
(*count
(*count)--;
"%ps:",
ftrace_snapshot_print,
*)-1;
*)&count);
register_snapshot_cmd(void)
tr->percpu_dir;
tracing_get_dentry(tr);
*fops)
&tracing_pipe_fops);
&tracing_fops);
&tracing_entries_fops);
&snapshot_fops);
*topt
topt->opt->bit)
"1\n";
"0\n";
(long)filp->private_data;
tr->options;
t_options,
opts[cnt].name;
(!topts)
init_tracer_tracefs(struct
ring_buffer_flags
buf->buffer
ring_buffer_free(buf->buffer);
allocate_trace_buffer(tr,
(tr->name
strcmp(tr->name,
kstrdup(name,
trace_buf_size)
tr->dir
&ftrace_trace_arrays);
free_trace_buffers(tr);
kfree(tr->name);
kfree(tr);
(!mod->num_trace_enums)
trace_module_remove_enums(struct
trace_module_notify(struct
trace_module_nb
trace_module_notify,
register_module_notifier(&trace_module_nb);
ftrace_dump(ftrace_dump_on_oops);
INT_MAX
TRACE_MAX_PRINT
KERN_TRACE
at.
atomic_dec(&dump_running);
"Bad
"Dumping
printk("#
CPUS
ring_buf_size
buffer!\n");
trace_boot_clock);
(tracepoint_printk)
2005-2006
Kihon
Esben
Nielsen
"rtmutex_common.h"
bit0
(fast
taskpointer
(*)
RT_MUTEX_HAS_WAITERS;
rt_mutex_cmpxchg(l,c,n)
mark_rt_mutex_waiters(struct
unlock_rt_mutex_safe(struct
__releases(lock->wait_lock)
cmpxchg(p,
acquire(lock);
dl_prio(),
(rt_mutex_waiter_less(waiter,
&lock->waiters);
task->pi_waiters_leftmost
&task->pi_waiters);
(likely(!task_has_pi_waiters(task)))
(!task_has_pi_waiters(task))
(task->prio
(Note:
Deadlock
conducted
cond
chain:
p->pi_blocked_on
@orig_lock
deboosting
scope
lock(task->pi_lock);
max_lock_depth)
@task.
(!detect_deadlock)
(waiter->prio
detection,
conditionals
check_exit_conditions_3
(!rt_mutex_owner(lock))
task_blocked_on_lock(task);
waiter->prio
task->prio;
rt_mutex_enqueue(lock,
(prerequeue_top_waiter
(highest
priority)
rt_mutex_dequeue_pi(task,
dereferenced
rt-mutex
callsite
@lock,
(rt_mutex_owner(lock))
(kernel
task->pi_blocked_on
raw_spin_lock_irqsave(&owner->pi_lock,
rt_mutex_dequeue_pi(owner,
rt_mutex_enqueue_pi(owner,
__rt_mutex_adjust_prio(owner);
task_blocked_on_lock(owner);
raw_spin_unlock_irqrestore(&owner->pi_lock,
wait_lock.
rt_mutex_adjust_prio_chain(owner,
raw_spin_lock_irqsave(&current->pi_lock,
raw_spin_unlock_irqrestore(&current->pi_lock,
TASK_INTERRUPTIBLE
!timeout->task)
debug_rt_mutex_print_deadlock(waiter);
-EDEADLOCK
(unlikely(timeout))
hrtimer_start_expires(&timeout->timer,
(!hrtimer_active(&timeout->timer))
timeout->task
task_blocks_on_rt_mutex(lock,
__rt_mutex_slowlock(lock,
rt_mutex_deadlock_account_unlock(current);
suffer
slowfn(lock,
*lock))
slowfn(lock);
rt_mutex_fastlock(lock,
rt_mutex_timed_fastlock(lock,
uninitialized,
forbidden.
*proxy_owner)
complement
subtraction
suspended).
Care
occupies
source)
updates.
Duration
INITIAL_JIFFIES);
jiffy_sched_clock_read,
mult,
sched_clock(void)
raw_read_seqcount(&cd.seq);
reverts
backup
raw_write_seqcount_latch(&cd.seq);
epoch.
cyc;
cd.read_data[0];
cyc_to_ns((cyc
rd.epoch_cyc)
rd.sched_clock_mask,
rd.mult,
rd.shift);
rd.epoch_cyc
update_clock_read_data(&rd);
rate)
new_mask,
new_mult,
new_shift;
new_mask
(irqtime
hrtimer_start(&sched_clock_timer,
cd.wrap_kt,
&cd.read_data[0];
rd->read_sched_clock
__TRACE_EVENTS_H
trace_print_bputs_msg_only(struct
trace_print_bprintk_msg_only(struct
trace_print_printk_msg_only(struct
seq_print_ip_sym(struct
seq_print_userip_objs(const
seq_print_user_ip(struct
trace_print_context(struct
trace_print_lat_context(struct
*ftrace_find_event(int
trace_nop_print(struct
trace_print_lat_fmt(struct
__unregister_ftrace_event(struct
*__unused)
rcu_idle_enter();
(cpu_idle_force_poll
rcu_idle_exit();
*drv
rescheduled.
Possibly
(entered_state
(current_clr_polling_and_test())
idle_set_state(this_rq(),
__current_set_polling();
*)(long)smp_processor_id());
fell
promise
die,
2006,2007
lock),
"lockdep_internals.h"
prove_locking
lock_stat
current->lockdep_recursion++;
current->lockdep_recursion--;
nr_lock_classes;
(points[i]
lt->min
dst->min
lock_class_stats));
&per_cpu(cpu_lock_stats,
cpu)[class
lock_release_holdtime(struct
holdtime
holdtime);
CLASSHASH_SIZE
CLASSHASH_BITS)
hash_long((unsigned
long)key,
CHAINHASH_SIZE
CHAINHASH_BITS)
VERY_VERBOSE
Stack-trace:
tightly
"turning
"Please
trace->max_entries
maxes
friggin
nr_hardirq_chains;
nr_softirq_chains;
nr_process_chains;
max_lockdep_depth;
lockdep_stats,
lockdep_stats);
printouts:
__USAGE(__STATE)
__get_key_name(struct
lock_flag(bit
get_usage_chars(struct
usage[i++]
get_usage_char(class,
class->name;
usage[LOCK_USAGE_CHARS];
get_usage_chars(class,
(");
MAX_LOCKDEP_KEYS)
print_tainted());
object:
__KERNEL__
var?
unique,
class->name_version
*hash_head;
subclass);
BUILD_BUG_ON(sizeof(struct
lock->key->subkeys
hash_head
classhashentry(key);
lockdep_free_key_range().
(class->key
trample
lock->name);
DEBUG_LOCKS_WARN_ON(!irqs_disabled());
MAX_LOCKDEP_KEYS
class->key
hash_head);
printk("\nnew
(subclass
NR_LOCKDEP_CACHING_CLASSES)
smoke
entry->class
entry->distance
max_bfs_queue_depth;
cq->front
cq->rear
(cq->front
*cq,
(cq->rear
list_entries;
WARN_ON(nr
Out-of-bounds,
lock->class->dep_gen_id
*target_entry
(forward)
cq_depth;
*src_entry,
__bfs(src_entry,
(middle_class)
printk("Chain
of:\n
__print_lock_name(source);
__print_lock_name(parent);
printk("\n\n");
CPU1\n");
first:
*check_src,
*check_tgt)
printk("======================================================\n");
bfs
*)&count,
noop_count,
__bfs_forwards(root,
target,
sub-graph
@root->class
@bit.
subgraph,
*@target_entry.
*@target_entry
*)bit,
usage_match,
backwards-direction
class->ops);
root))
printk("lockdep:%s
graph\n",
get_lock_parent(entry);
(middle_class
__print_lock_name(middle_class);
__print_lock_name(unsafe_class);
<Interrupt>\n");
bit1,
bit2,
%s-safe
%s-unsafe
]\n",
[HC%u[%lu]:SC%u[%lu]:HE%u:SE%u]
HARDIRQ_SHIFT,
softirq_count()
SOFTIRQ_SHIFT,
print_lock(next);
holding:\n");
print_lock(prev);
print_lock_name(hlock_class(prev));
print_lock_name(hlock_class(next));
printk("\n...
lock",
hlock_class(next);
2];
(dir
<prev>,
<next>:
(!check_usage(curr,
exclusive_bit(bit),
state_name(bit)))
check_prev_add_irq(struct
inc_chains(void)
nr_process_chains++;
CPU0\n");
hlock_class(next))
<prev>)
<next>)?
contexts]
softirq-safe
softirq-unsafe
(unlikely(!ret))
write-lock
L3
&hlock_class(prev)->locks_after,
next->acquire_ip,
trylock_loop
out_bug;
(curr->held_locks[depth].irq_context
curr->held_locks[depth-1].irq_context)
(!hlock->trylock)
nr_lock_chains;
*lock_chain_get_class(struct
*chain,
i];
hashed,
lock..
(chain->chain_key
cached,
key:
lock_chains
chain->depth;
j]
lock_classes;
validate_chain(struct
O(N^2)
(!hlock->trylock
hlock->check
chain_key))
(just
hlock->read
double-check
(chain_key
state?
[%u],
%016Lx\n",
again?
(DEBUG_LOCKS_WARN_ON(id
MAX_LOCKDEP_KEYS))
(prev_hlock->irq_context
iterate_chain_key(chain_key,
smoking
damn
bet
prev_bit,
printk("=================================\n");
printk("---------------------------------\n");
{%s}
usage_str[new_bit]);
print_irqtrace_events(curr);
(unlikely(hlock_class(this)->usage_mask
mark_lock(struct
(forwards)
past:\n",
middle->class
<this>
<mask>:
root.parent
root.class
hlock_class(this);
print_irq_inversion_bug(curr,
&root,
print_irqtrace_events(struct
printk("irq
printk("hardirqs
printk("softirqs
mark_lock_irq(struct
(!valid_state(curr,
!usage(curr,
state_name(new_bit
mark_type
bit:
usage_bit
Hardirqs
curr->hardirqs_enabled
(!mark_held_locks(curr,
trace_hardirqs_on_caller(unsigned
Neither
etc..
EXPORT_SYMBOL(trace_hardirqs_on_caller);
trace_hardirqs_on(void)
EXPORT_SYMBOL(trace_hardirqs_on);
trace_hardirqs_off_caller(unsigned
IRQs,
(curr->hardirqs_enabled)
EXPORT_SYMBOL(trace_hardirqs_off_caller);
trace_hardirqs_off(void)
EXPORT_SYMBOL(trace_hardirqs_off);
Softirqs
trace_softirqs_on(unsigned
curr->softirqs_enabled
mark_held_locks(curr,
trace_softirqs_off(unsigned
Whoops,
__GFP_FS
__GFP_FS))
check_flags(unsigned
lockdep_trace_alloc(gfp_t
mark_irqflags(struct
contexts:
(curr->hardirq_context)
(curr->softirq_context)
separate_irq_context(struct
hlock->irq_context
(depth)
Impossible
dirty
lock-class
info:
NR_LOCKDEP_CACHING_CLASSES;
lock->cpu
print_lock(hlock);
printk("%s\n",
__lock_is_held(struct
class_idx;
already?
MAX_LOCK_DEPTH))
(hlock->class_idx
hlock->references
hlock->acquire_ip
hlock->waittime_stamp
hlock->holdtime_stamp
(check
used:
hlock->prev_chain_key
check_chain_key(curr);
(unlikely(curr->lockdep_depth
lockdep_print_held_locks(current);
printk("=====================================\n");
printk("-------------------------------------\n");
print_lockdep_cache(lock);
printk(")
non-nested
(!__lock_acquire(hlock->instance,
hlock_class(hlock)->subclass,
hlock->trylock,
hlock->read,
hlock->check,
hlock->hardirqs_off,
hlock->nest_lock,
hlock->acquire_ip,
hlock->references))
(DEBUG_LOCKS_WARN_ON(curr->lockdep_depth
lock_release_holdtime(hlock);
way:
bottles
beer
mutex_unlock()/spin_unlock*()
mutex_lock_interruptible()).
perfectly.
lock-stack
unlocked)
somehow
printk("possible
reason:
unannotated
current->lockdep_reclaim_gfp
print_lock_contention_bug(curr,
LOCKSTAT_POINTS)
(lock->cpu
!!hlock->read]++;
waittime
acquire,
happen?
waittime);
(unlikely(!lock_stat))
CHAINHASH_SIZE;
INIT_LIST_HEAD(chainhash_table
in:
Unhash
sync_sched()
classhash_table
allocators
zap
(class)
start_kernel()
MAX_LOCKDEP_KEYS);
MAX_LOCKDEP_ENTRIES);
MAX_LOCKDEP_CHAINS);
kB\n",
list_head)
*mem_from,
void*
held!
it.\n");
Careful:
CONFIG_PROVE_RCU_REPEATEDLY
"RCU
CPU!\n"
!rcu_is_watching()
debug_locks);
RCU-free
printk("%14s
set\n",
#f)
desc->action);

local_inc(&rb->nest);
local_read(&rb->head);
LOAD
->data_head
smp_rmb()
C.
RMB
handle->rb
(unlikely(have_lost))
sizeof(lost_event);
event->id_header_size;
@tail
A,
compiler.
handle->page
perf_output_put_handle(handle);
rb->watermark
rb->overwrite
(B),
driver,
*output_event
aux_head,
ring_buffer_get(output_event);
area,
handle->head
aux_tail,
<->
rb->aux_watermark;
(!handle->size)
local_set(&rb->aux_nest,
handle->rb->aux_priv;
adjusting
local_add(size,
&rb->aux_head);
rb->user_page->aux_head
rb->aux_watermark)
local_add(rb->aux_watermark,
&rb->aux_wakeup);
alloc_pages_node(node,
rb_alloc_aux(struct
(event->pmu->capabilities
ilog2()
ilog2(nr_pages);
buffering
rb->free_aux
rb->aux_nr_pages;
rb->aux_priv
rb->aux_watermark
watermark;
pgoff;
rb_free_aux(struct
__perf_mmap_to_page(struct
*rb_alloc(int
ring_buffer);
rb->user_page
perf_mmap_alloc_page(cpu);
ring_buffer_init(rb,
long)rb->data_pages[i]);
long)rb->user_page);
perf_mmap_free_page((unsigned
all_buf
perf_mmap_to_page(struct
pgoff);
<rjw@sisk.pl>,
__setup_irq()
@action
IRQF_FORCE_RESUME)
desc->nr_actions);
IRQF_COND_SUSPEND)
__free_irq()
IRQD_WAKEUP_ARMED
__disable_irq(desc,
(irq_desc_get_chip(desc)->flags
IRQS_SUSPENDED)
interrupt?
resume:
want_early)
desc->action->flags
irq_pm_syscore_ops
suspend_device_irqs()
coredump_params
sizeof(int)
/proc/dma
channels,
problems,
dma_chan
request_dma
@dmanr:
request_dma(unsigned
dmanr,
(dmanr
MAX_DMA_CHANNELS)
(xchg(&dma_chan_busy[dmanr].lock,
free_dma
free_dma(unsigned
dmanr)
DMA%d\n",
dmanr);
proc_dma_show(struct
bug.
SIGEV
#error
EOPNOTSUPP
ENANOSLEEP_NOTSUP
idr_find()
common_nsleep(const
common_timer_create(struct
common_timer_get(struct
common_timer_set(struct
common_timer_del(struct
posix_timer_fn(struct
*__lock_timer(timer_t
*sig,
&posix_timers_hashtable[hash(sig,
sig->posix_timer_id;
spin_unlock_irqrestore(&timr->it_lock,
current_kernel_time();
clock_monotonic
posix_get_coarse_res,
*timr)
timr->it.real.interval);
timr->it_overrun_last
flagged
timr->it_requeue_pending
timr->it_overrun_last;
(timr)
do_schedule_next_timer().
ALWAYS
si_private
spin_lock_irqsave(&timr->it_lock,
trivial
later,
pipeline.
*rtn
lacks
*tmr;
tmr
tmr;
kmem_cache_free(posix_timers_cache,
tmr);
it_id_set
new_timer
new_timer->it_pid
new_timer->it_id;
report.
*cur_setting)
cur_setting->it_interval
(timr->it_sigev_notify
(timr->it_requeue_pending
SIGEV_NONE))
cur_setting->it_value.tv_nsec
cur_setting->it_value
*kc;
kc
clockid_to_kclock(timr->it_clock);
timer_id)
*new_setting,
*old_setting)
(old_setting)
old_setting);
(hrtimer_try_to_cancel(timer)
~REQUEUE_PENDING;
timespec_to_ktime(new_setting->it_interval);
HRTIMER_MODE_REL)
hrtimer_add_expires(timer,
flag;
retry_delete:
(timer_delete_hook(timer)
retry_delete;
spin_lock(&current->sighand->siglock);
list_del(&timer->list);
timer->it_signal
release_posix_timer(timer,
IT_ID_SET);
copy_to_user(tp,
sizeof(ktx)))
rmtp,
which_clock
restart_block->nanosleep.clockid;
panic_on_oops
tainted_mask;
pause_on_oops;
panic_on_warn
message,
deadlocking
syncing:
oops_in_progress
"crash_kexec_post_notifiers"
crash_kexec(NULL);
kdump
panic_notifiers
unstable,
(panic_timeout
PANIC_TIMER_STEP)
i_next)
panic_blink(state
3600
PANIC_BLINK_SPD;
mdelay(PANIC_TIMER_STEP);
emergency_restart();
__sparc__
Stop-A
(L1-A)
defined(CONFIG_S390)
'G'
TAINT_FORCED_MODULE,
TAINT_CPU_OUT_OF_SPEC,
TAINT_MACHINE_CHECK,
TAINT_CRAP,
TAINT_OOT_MODULE,
TAINT_UNSIGNED_MODULE,
'P'
forcibly
'U'
ACPI
overridden.
Taint
'E'
'L'
&tainted_mask);
lockdep_ok
lockdep_ok)
spin_unlock(&pause_on_oops_lock);
spin_lock(&pause_on_oops_lock);
oops,
option.
display,
pause
long:
do_oops_enter_exit();
everything.
print_oops_end_marker();
PID:
WARN()
args.fmt
va_start(args.args,
va_end(args.args);
CONFIG_CC_STACKPROTECTOR
corruption
PCI
<linux/msi.h>
<linux/pci.h>
CONFIG_GENERIC_MSI_IRQ_DOMAIN
checks)
BUG_ON(irq_chip_compose_msi_msg(irq_data,
&msg));
irq_domain_free_irqs_top(domain,
nvec,
(!chip->irq_set_affinity)
(ops->msi_finish)
ops->msi_finish(&arg,
dev_dbg(dev,
MSI\n",
Marco
Wieringen
ideas
from:
BSD-style
Whenever
log.
1995
acct_file
Al
pure
readonly
umount
globals.
do_acct_process(struct
*acct)
sbuf.f_blocks
(sbuf.f_bavail
acct->active
pr_info("Process
acct->needcheck
bsd_acct_struct,
do_acct_process(acct);
acct_put(acct);
acct->file;
comp_t
up?
EXPSIZE;
exponent
shift.
correctly).
(rnd
(++value
*pacct
&current->signal->pacct;
tty_struct
*tty;
ac->ac_etime
elapsed;
flim;
current->signal->rlim[RLIMIT_FSIZE].rlim_cur
whoever
PF_FORKNOEXEC)
AFORK;
PF_SUPERPRIV)
ASU;
PF_DUMPCORE)
ACORE;
AXSIG;
ns->parent)
<linux/dcache.h>
<linux/vmstat.h>
tfc->ret
returns:
.info
.ret
remote_function,
data.ret;
EVENT_OWNER_KERNEL;
PERF_FLAG_FD_CLOEXEC)
EVENT_FLEXIBLE
perf_sched_events
kiB
sysctl_perf_event_mlock
100000
DEFAULT_SAMPLE_PERIOD_NS
DEFAULT_CPU_TIME_MAX_PERCENT
25
sysctl_perf_event_sample_rate
perf_rotate_context(struct
sysctl_perf_event_sample_rate;
ACCESS_ONCE(perf_sample_allowed_ns);
avg_local_sample_len;
local_samples_len;
__this_cpu_read(running_sample_length);
avg_local_sample_len
(%lld
%lld),
"kernel.perf_event_max_sample_rate
avg_local_sample_len,
sysctl_perf_event_sample_rate);
cpu_ctx_sched_out(struct
event_type);
cpu_ctx_sched_in(struct
update_context_time(struct
*ctx);
perf_event_time(struct
perf_cgroup_match(struct
Cgroup
perf_detach_cgroup(struct
is_cgroup_event(struct
perf_cgroup_event_time(struct
per_cpu_ptr(event->cgrp->info,
this_cpu_ptr(cgrp->info);
info->timestamp
update_cgrp_time_from_cpuctx(struct
update_cgrp_time_from_event(struct
(css_get)
(cgrp
perf_cgroup_set_timestamp(struct
(!task
PERF_CGROUP_SWOUT
perf_cgroup_switch(struct
pmu)
event_filter_match()
perf_cgroup_sched_out(struct
*cgrp1;
*cgrp2
perf_cgroup_sched_in(struct
PERF_CGROUP_SWIN);
*group_leader)
(IS_ERR(css))
PTR_ERR(css);
perf_cgroup,
perf_cgroup_set_shadow_time(struct
perf_cgroup_defer_enabled(struct
event->cgrp_defer_enabled
perf_cgroup_mark_enabled(struct
event->total_time_enabled;
sub->tstamp_enabled
sub->total_time_enabled;
rotations
cpuctx->hrtimer_interval);
*hr
&cpuctx->hrtimer;
cpuctx->ctx.pmu;
sane,
pmu->hrtimer_interval_ms
cpuctx->hrtimer_interval
ns_to_ktime(NSEC_PER_MSEC
this_cpu_ptr(pmu->pmu_disable_count);
this_cpu_ptr(&active_ctx_list);
perf_event_context,
perf_pmu_migrate_context()
parent<->child
relation,
vectors
(!atomic_inc_not_zero(&ctx->refcount))
perf_event_ctx_lock_nested(event,
parent_ctx;
event->ns);
perf_lock_task_context(task,
ctx->time
perf_cgroup_event_time(event);
total_time_enabled
PERF_EVENT_STATE_INACTIVE
ctx->time;
event->total_time_enabled
event->tstamp_enabled;
event->total_time_running
event->tstamp_running;
stand
siblings.
(is_software_event(event))
list_add_tail(&event->group_entry,
(event->attr.inherit_stat)
PERF_SAMPLE_ADDR)
PERF_SAMPLE_WEIGHT)
PERF_SAMPLE_READ)
PERF_SAMPLE_DATA_SRC)
PERF_SAMPLE_TRANSACTION)
sizeof(data->id);
event->ctx);
(group_leader->group_flags
WARN_ON_ONCE(event->ctx
exit/hot-unplug
close.
(!(event->attach_state
list_del_init(&event->group_entry);
upgrade
Inherit
orphans_remove_work(struct
perf_wq
mismatch
(event->pending_disable)
(!is_software_event(event))
(event->attr.freq
event->attr.sample_freq)
(event->attr.exclusive
(is_orphaned_child(event))
schedule_orphans_remove(ctx);
*group_event,
event_sched_out(group_event,
any):
.detach_group
__perf_remove_from_context,
perf_event_context_sched_out()
perf_event_for_each_child
perf_event_for_each
__perf_event_disable,
cross-call.
along.
tstamp);
perf_log_throttle(struct
perf_log_itrace_start(struct
pmu->cancel_txn(pmu);
tstamp_stopped
thru
task_ctx_sched_out(struct
ctx_sched_in(struct
EVENT_FLEXIBLE,
cpuctx->task_ctx;
add_event_to_ctx(event,
__perf_install_in_context,
non-leader
event->group_leader;
(!ctx->is_active)
leader->state
leader)
group_sched_in(event,
PERF_EVENT_STATE_ERROR;
__perf_event_enable,
arrived.
PERF_EVENT_STATE_ERROR)
_perf_event_enable(event);
refresh)
_perf_event_refresh(event,
event_type)
ctx->is_active;
(likely(!ctx->nr_events))
((is_active
EVENT_PINNED)
EVENT_PINNED))
EVENT_FLEXIBLE)
EVENT_FLEXIBLE))
ctx2->parent_ctx
ctx2->parent_gen)
(ctx1->parent_ctx
ctx1->parent_gen
next_event);
do_switch
(likely(!ctx))
(!cpuctx->task_ctx)
next_ctx
next_parent
ctx->task
perf_pmu_disable(pmu);
NMI,
disable()
hits,
(__this_cpu_read(perf_sched_cb_usages))
(atomic_read(this_cpu_ptr(&perf_cgroup_events)))
tstamp_enabled
perf_cgroup_mark_enabled(event,
(group_can_go_on(event,
can_add_hw
ERROR
Listen
(!(is_active
cpuctx);
&cpuctx->ctx;
enable()
perf_event_context_sched_in(ctx,
event->attr.sample_freq;
dividend;
nsec_fls
sample_freq
@a
REDUCE_FLS(nsec,
frequency);
frequency;
sec);
(disable)
PERF_EF_RELOAD);
*hwc;
hwc
(hwc->interrupts
MAX_INTERRUPTS)
perf_adjust_period(event,
ctxswin
unclone_ctx(ctx);
vpid)
ERR_PTR(-ESRCH);
Reuse
errout:
ctxn
pmu->task_ctx_nr;
ctx->task_ctx_data
task_ctx_data;
perf_event_exit_task().
kfree(task_ctx_data);
(event->ns)
put_pid_ns(event->ns);
kfree(event);
ring_buffer_attach(struct
(event->attr.comm)
(event->attr.task)
unaccount_event_cpu(event,
"exclusive"
pmus
conflict,
pmu,
PERF_PMU_CAP_EXCLUSIVE)
e2->cpu
(event->attr.sample_type
(event->destroy)
event->destroy(event);
exclusive_event_destroy(event);
perf_mmap_close()
_free_event(event);
owner->perf_event_mutex.
list_del_init(&event->owner_entry);
put_event(event);
*parent_event
list_del_init(&event->child_list);
put_event(parent_event);
*enabled,
*running)
&event->child_list,
child_list)
values[5];
leader->nr_siblings;
primary_event_id(leader);
primary_event_id(sub);
(copy_to_user(buf
values[4];
no_children;
poll_wait(file,
perf_event_set_output()
swizzle
Holding
(copy_from_user(&value,
sysctl_perf_event_sample_rate)
event->attr.sample_period
perf_event_set_output(struct
output;
output_event);
&current->perf_event_list,
*userpg;
userpg
perf_event_mmap_page,
seqlock
total_time_enabled,
calc_timer_values(event,
&now,
++userpg->lock;
userpg->offset
vm_fault
*vmf)
VM_FAULT_SIGBUS;
(vmf->flags
(vmf->pgoff
vmf->page
vmf->pgoff;
event->rcu_pending
rb)
Provide
wake_up_all(&event->waitq);
&rb->event_list,
rb_entry)
*rcu_head)
container_of(rcu_head,
*ring_buffer_get(struct
ring_buffer_put(struct
atomic_inc(&event->mmap_count);
(event->pmu->event_mapped)
event->pmu->event_mapped(event);
perf_event_set_output().
vma->vm_pgoff
&event->mmap_mutex))
&mmap_user->locked_vm);
atomic_dec(&rb->mmap_count);
complicated
mmap_mutex.
free_event()
Aside
mmap_locked;
vm_operations_struct
.fault
perf_mmap_fault,
user_lock_limit;
(vma_size
aux_size
(aux_offset
luck.
user_lock_limit
user_locked
lock_limit
VM_WRITE)
VM_DONTCOPY
vma->vm_ops
perf_event_wakeup(struct
event->pending_wakeup
(rctx
*cbs)
perf_guest_cbs
perf_regs
regs_user->abi
perf_reg_abi(current);
regs_user->regs
perf_user_stack_pointer(regs);
header_size
Case
dump_size);
sp
perf_event_clock(event);
perf_event_header__init_id(struct
__perf_event_header__init_id(header,
data->type;
data->tid_entry);
data->time);
data->stream_id);
data->cpu_entry);
perf_event__output_id_sample(struct
PERF_FORMAT_GROUP
running);
data->callchain->nr;
PERF_SAMPLE_RAW)
(data->raw)
data->raw->size);
(data->br_stack)
data->br_stack->nr
perf_branch_entry);
(PERF_SAMPLE_REGS_ABI_NONE).
abi);
(abi)
event->attr.sample_regs_user;
perf_output_sample_regs(handle,
data->regs_user.regs);
event->attr.sample_regs_intr;
header->misc
ABI
hweight64(mask)
callchain
perf_read_event
task),
.task
*comm;
comm_event->task);
__output_copy(&handle,
comm_event->event_id.header.size
sizeof(comm));
*mmap_event
*vma
mmap_event->vma;
(event->attr.mmap2)
maj
gen
inode->i_sb->s_dev;
inode->i_ino;
VM_DENYWRITE)
VM_LOCKED)
zero'd
.min
perf_event_aux_event(struct
perf_aux_event
.offset
perf_event_header__init_id(&rec.header,
rec.header.size);
throttle,
irq_work_queue(&event->pending);
Recursion
local64_read(&hwc->period_left);
perf_swevent_set_period(event);
(__perf_event_overflow(event,
!event->attr.freq)
data->period
!user_mode(regs))
perf_type_id
*swhash,
__find_swevent_head(hlist,
release.
rcu_dereference_protected(swhash->swevent_hlist,
hlist_for_each_entry_rcu(event,
hlist_entry)
rctx);
event->hw.state
*swhash)
swevent_hlist_release(swhash);
swevent_hlist_put_cpu(event,
rcu_assign_pointer(swhash->swevent_hlist,
event->destroy
perf_swevent_start,
perf_swevent_stop,
perf_swevent_read,
entry_size,
(perf_tp_event_match(event,
perf_tp_register(void)
prog_fd)
event->tp_event->prog
(regs
10000,
freq;
local64_xchg(&event->hw.prev_count,
local64_set(&event->hw.prev_count,
perf_swevent_start_hrtimer(event);
perf_swevent_cancel_hrtimer(event);
cpu_clock_event_update(event);
PERF_EF_START)
perf_swevent_init_hrtimer(event);
event->ctx->time);
task_clock_event_update(event,
list_for_each_entry(pmu,
cpuctx->unique_pmu
*i;
pmu);
snprintf(page,
free_dev:
pmu_dev_alloc(pmu);
pmu->pmu_cpu_context
pmu->start_txn
pmu->commit_txn
pmu->cancel_txn
device_del(pmu->dev);
(pmu->type
PERF_TYPE_MAX)
idr_remove(&pmu_idr,
free_percpu(pmu->pmu_disable_count);
module_put(pmu->module);
srcu_read_lock(&pmus_srcu);
perf_try_init_event(pmu,
srcu_read_unlock(&pmus_srcu,
tick_nohz_full_kick_all();
*group_leader,
group_leader;
get_pid_ns(task_active_pid_ns(current));
parent_event->overflow_handler;
parent_event->overflow_handler_context;
(attr->read_format
setup)
(output_event->cpu
event->cpu)
event->clock)
(output_event)
move_group
f_flags
O_RDWR;
cgroup_fd
PERF_FLAG_PID_CGROUP)
(IS_ERR(event))
account_event(event);
percpu):
(IS_ERR(ctx))
PTR_ERR(ctx);
CPU;
swizzling
perf_event::ctx.
put_ctx(gctx);
(!exclusive_event_installable(event,
Precalculate
sample_data
fdput(group);
&events,
migrate_entry)
list_del(&event->migrate_entry);
perf_install_in_context(dst_ctx,
get_ctx(dst_ctx);
WARN_ON_ONCE(parent_event->ctx->parent_ctx);
free_event(child_event);
perf_event_task(child,
child->perf_event_ctxp[ctxn]
raw_spin_unlock_irqrestore(&child_ctx->lock,
child_ctx,
through:
closes
perf_free_event(event,
*child_ctx)
child_event;
child_ctx;
*leader;
*inherited_all
child->perf_event_ctxp[ctxn];
(!child_ctx)
clone,
parent_ctx
inherit_task_group(event,
&inherited_all);
raw_spin_lock_irqsave(&parent_ctx->lock,
parent_ctx->rotate_disable
raw_spin_unlock_irqrestore(&parent_ctx->lock,
child_ctx->parent_ctx
child_ctx->parent_gen
swhash->online
perf_event_exit_cpu(int
perf_event_exit_cpu(cpu);
INT_MIN,
WARN(ret,
sprintf(page,
kfree(jc);
task_function_call(task,
__perf_cgroup_move,
futexes,
2003,
suggestions,
Dumazet
Darren
loudly
Matthew
avoided
sys_futex(WAIT,
futex_wait(futex,
*futex
sys_futex(WAKE,
futex);
futex_wake(futex);
queue();
forever
waker:
(a)
mb();
(b)
translates
respectively.
CONFIG_HAVE_FUTEX_CMPXCHG
rt_waiter
@bitset:
*lock_ptr;
futex_keys
futex_hashsize;
get_futex_key()
*key1,
*key2)
addressed
(!key->both.ptr)
(key->both.offset
(FUT_OFF_INODE|FUT_OFF_MMSHARED))
FUT_OFF_INODE:
FUT_OFF_MMSHARED:
PROCESS_PRIVATE
mappings,
ro
aligned.
key->private.mm
key->private.address
get_user_pages_fast(address,
(eg.
__split_huge_page_splitting()
compound_head(page);
PG_lock
page_head)
get_page(page_head);
(also
reference,
beneath
unlock_page(page_head);
put_page(page_head);
futex_q's
reside
*this;
&hb->chain,
newval)
sizeof(u32));
atomic_set(&pi_state->refcount,
pi_state->key
head->next;
rt_mutex_unlock(&pi_state->pi_mutex);
states:
exit_robust_list(),
FUTEX_OWNER_DIED
FUTEX_TID_MASK;
non-PI
FUTEX_OWNER_DIED)
out_state;
atomic_inc(&pi_state->refcount);
(unlikely(p->flags
raw_spin_unlock_irq(&p->pi_lock);
attach_to_pi_state(uval,
match->pi_state,
attach_to_pi_owner(uval,
@ps:
@set_waiters:
**ps,
set_waiters)
(get_futex_value_locked(&uval,
lock_pi_update_atomic(uaddr,
hb->lock.
accessed.
TASK_NORMAL);
new_owner
WAITERS
newval))
raw_spin_unlock(&pi_state->pi_mutex.wait_lock);
*hb2)
bitset)
&key,
(++ret
nr_wake)
out_put_key:
*uaddr1,
*hb2;
get_futex_key(uaddr1,
out_put_key1;
hash_futex(&key1);
hash_futex(&key2);
double_lock_hb(hb1,
op_ret
uaddr2);
(unlikely(op_ret
fault_in_user_writeable(uaddr2);
&hb1->chain,
&key1))
out_put_key1:
@hb1:
@hb2:
*hb2,
requeue.
plist_add(&q->list,
hb_waiters_inc(hb2);
q->key
&hb->lock;
*key2,
intends
key2))
top_waiter.
uaddr1
@uaddr2:
requeue_pi)
nr_requeue,
(requeue_pi)
uaddr2)
pi_state,
(refill_pi_state_cache())
predicted
uaddrs
uaddr1);
(task_count
hb1,
crap
lookup_pi_state.
awaiting
futex_unlock_pi().
this->pi_state
queue_me().
__releases(&hb->lock)
futex_wait_requeue_pi()
MAX_RT_PRIO)
it);
spin_lock().
spin_unlock(lock_ptr);
bucket.
spin_unlock(q->lock_ptr);
newtid
Modifying
handle_fault;
futex_wait_restart(struct
anticipated
lock-steal
PI-state
(q->pi_state->owner
fixup_pi_state_owner(uaddr,
futex_wait_queue_me()
hrtimer_sleeper,
completion,
hb);
(uaddr
AFTER
cond(var)
violate
absorb
uaddr);
queue_unlock(*hb);
put_futex_key(&q->key);
q.bitset
(abs_time)
FLAGS_CLOCKRT)
hrtimer_set_expires_range_ns(&to->timer,
current->timer_slack_ns);
futex_wait_setup(uaddr,
&hb);
futex_wait_queue_me(hb,
whatever.
hrtimer_cancel(&to->timer);
(Due
CLOCK_REALTIME,
&q.key,
out_unlock_put_key;
WARN_ON(!q.pi_state);
proprogate
unqueue_me_pi(&q);
attempted
(get_user(uval,
((uval
unlocking.
wakeup:
pi_faulted;
concern
robust-futex
list-head
gracefully.
(nval
futex_wake(uaddr,
futex_offset,
FUTEX_LOCK_PI:
FUTEX_UNLOCK_PI:
FUTEX_TRYLOCK_PI:
FUTEX_WAIT_REQUEUE_PI:
FUTEX_CMP_REQUEUE_PI:
&val3,
futex_lock_pi(uaddr,
'utime'
futex_shift;
futex_hashsize,
semaphores.
down_trylock()
up()
down_interruptible()
down_killable()
wait_list.
__down(struct
__down_interruptible(struct
__down_killable(struct
__down_timeout(struct
__up(struct
mutexes,
down().
constant,
*waiter
list_del(&waiter->list);
Russell.
(think
kthread()
kthreadd.
*done;
(likely(vfork))
&to_kthread(current)->flags);
kthread_should_stop(),
parameter,
kthreads,
speculative
__set_current_state(TASK_PARKED);
&self->flags))
*create
self;
SIGKILLed,
xchg(&create->done,
create->result
complete(done);
do_fork()
create,
CLONE_FS
CLONE_FILES
@threadfn:
signal_pending(current).
@threadfn.
@namefmt:
stopped:
@threadfn()
standalone
number;
DECLARE_COMPLETION_ONSTACK(done);
&done;
wait_for_completion(&done);
(!IS_ERR(task))
namefmt,
set_cpus_allowed_ptr(task,
Park
IS_PARKED
TASK_PARKED);
kthread_should_park()
__kthread_unpark(k,
kthread);
wake_up_process():
threadfn().
wake_up_process(k);
kthread_stop
PF_NOFREEZE;
fwork
KTHREAD_WORK_INIT(fwork.work,
kthread_flush_work_fn),
COMPLETION_INITIALIZER_ONSTACK(fwork.done),
&fwork.work,
wait_for_completion(&fwork.done);
SMPBOOT_H
TRACER_IRQS_OFF
stop_irqsoff_tracer(struct
start_irqsoff_tracer(struct
((trace_type
irqs_disabled());
disturb
found:
irqsoff_set_flag(struct
irqsoff_graph_entry(struct
irqsoff_graph_return(struct
irqsoff_trace_open(struct
irqsoff_trace_close(struct
irqsoff_print_line(struct
(!report_latency(tr,
__trace_stack(tr,
data->critical_sequence
!tracing_is_enabled())
(unlikely(!data)
atomic_read(&data->disabled))
atomic_inc(&data->disabled);
data->critical_start
stop_critical_timing(a0,
start_critical_timing(a0,
caller_addr)
caller_addr);
!irq_trace())
register_irqsoff_function(tr,
unregister_irqsoff_function(tr,
irqsoff_busy
register_irqsoff(trace)
TRACER_PREEMPT_OFF;
register_preemptoff(trace)
defined(CONFIG_IRQSOFF_TRACER)
defined(CONFIG_PREEMPT_TRACER)
register_preemptirqsoff(trace)
<keys/asymmetric-type.h>
<keys/system_keyring.h>
__initconst
system_certificate_list_size;
keyring\n");
((KEY_POS_ALL
~KEY_POS_SETATTR)
KEY_USR_VIEW
key_ref_t
cert
(plen
pr_err("Problem
certificate
PTR_ERR(key));
"../debug_core.h"
kdb_common_init_state(struct
kgdb_info[ks->cpu].debuggerinfo;
kdb_reason_t
(ks->err_code
SIGTRAP)
KDB_REASON_BREAK;
kdb_common_init_state(ks);
KDB_STATE_CLEAR(DOING_SS);
KDB_STATE_SET(PAGER);
(!cpu_online(i))
KDB_FLAG_SET(CATASTROPHIC);
unresponsive
(!kgdb_info[i].enter_kgdb)
db_result,
kdb_common_deinit_state();
KDB_STATE_CLEAR(PAGER);
(KDB_STATE(DOING_SS))
"s");
kgdb_info[ks->cpu].ret_state
(ks->pass_exception)
KDB_CMD_CPU)
emulates
kdb_gdb_state_pass(char
Infrastructure
<linux/bsearch.h>
___r
cond;
___r;
INIT_OPS_HASH(opsname)
.func_hash
.local_hash.regex_lock
__MUTEX_INITIALIZER(opsname.local_hash.regex_lock),
ASSIGN_OPS_HASH(opsname,
FTRACE_OPS_FL_STUB,
stronger
ftrace_ops_recurs_func(unsigned
ARCH_SUPPORTS_FTRACE_OPS
ftrace_ops_list_func(unsigned
ftrace_ops_no_ops(unsigned
rcu_dereference_raw_notrace()
Alpha
Optimized
itself!
update_function_graph_func(void)
set_function_trace_op
update_function_graph_func();
(ftrace_trace_function
ftrace_ops_list_func)
Yes
schedule_on_each_cpu(ftrace_sync);
set_function_trace_op;
rmb
smp_call_function(ftrace_sync_ipi,
(*list
*main_ops,
add_ftrace_ops(&ftrace_ops_list,
main_ops);
remove_ftrace_ops(&ftrace_ops_list,
SAVE_REGS_IF_SUPPORTED
(!core_kernel_data((unsigned
&control_ops,
control_ops
ftrace_stub)
ftrace_profile_stat,
FTRACE_PROFILE_HASH_SIZE
(!pg)
&pg->records[0];
function_stat_cmp(void
(a->time
b->time)
(a->counter
b->counter)
---\n");
mutex_lock(&ftrace_profile_lock);
kallsyms_lookup(rec->ip,
rec->counter);
rec->time;
(rec->counter
Welford's
s^2
(n-1))
(x_i)^2
(\Sum
x_i)^2)
rec->counter
rec->time_squared
rec->time
Divide
trace_seq_puts(&s,
mutex_unlock(&ftrace_profile_lock);
*stat)
stat->start;
highly
stat->start
pg->next
(!pg->next)
&per_cpu(ftrace_profile_stats,
thousand
hit.
stat->hash
FTRACE_PROFILE_HASH_BITS);
this_cpu_ptr(&ftrace_profile_stats);
(!stat->hash
!ftrace_profile_enabled)
trace->rettime
trace->calltime;
register_ftrace_profiler(void)
unregister_ftrace_profiler(void)
ftrace_profile_tracefs(struct
&init_struct_pid;
CONFIG_FTRACE_MCOUNT_RECORD
free_list;
core_kernel_text()
trampolines.
op->trampoline_size)
op->trampoline
ENTRIES_PER_PAGE
10000
hash->size_bits);
&hash->buckets[key];
hlist_for_each_entry_rcu_notrace(entry,
(entry->ip
hash_long(entry->ip,
hlist_del(&entry->hlist);
hash->count--;
*tn;
hash->size_bits;
tn,
(!hash
EMPTY_HASH)
kfree(hash);
*new_hash;
alloc_ftrace_hash(size_bits);
(!new_hash)
new_hash;
entry->ip);
free_ftrace_hash(new_hash);
ftrace_hash_rec_disable_modify(struct
filter_hash);
ftrace_hash_rec_enable_modify(struct
ftrace_hash_ipmodify_update(struct
ftrace_ops_test(struct
pg->index;
rec->ip)
keep_regs
(filter_hash)
other_hash
in_other_hash
decrementing.
(FTRACE_WARN_ON(ftrace_rec_count(rec)
(ftrace_rec_count(rec)
FTRACE_FL_TRAMP;
~FTRACE_FL_TRAMP;
FTRACE_FL_REGS;
inc);
(ops->func_hash
(op->func_hash
ftrace_hash_rec_update_modify(ops,
meanings
*new_hash)
in_old
!!ftrace_lookup_ip(old_hash,
in_new
!!ftrace_lookup_ip(new_hash,
(in_old
in_new)
(in_new)
FTRACE_FL_IPMODIFY;
~FTRACE_FL_IPMODIFY;
*hash
ftrace_find_tramp_ops_any(struct
either:
EPERM
ftrace_rec_count(rec),
R"
ftrace_find_tramp_ops_any(rec);
pr_cont("\ttramp:
%pS",
*)ops->trampoline);
ERROR!");
ftrace_get_addr_curr(rec);
0UL;
(flag)
(!(rec->flags
FTRACE_FL_ENABLED)
@enable:
ftrace_check_record(rec,
(!op->trampoline)
FTRACE_OPS_FL_MODIFYING)
hash_contains_ip(ip,
(!(op->flags
precedence
ops->trampoline;
long)FTRACE_REGS_ADDR;
*)rec->ip);
ftrace_addr;
ftrace_addr
ftrace_addr);
ftrace_bug(failed,
locations
traced.
iter->index
(iter->pg
!iter->pg->index)
(!iter->pg)
iter->pg->index)
(FTRACE_WARN_ON(err))
@command:
FTRACE_WARN_ON(ret);
ftrace_startup_enable(int
(saved_ftrace_func
ftrace_trace_function)
ftrace_trace_function;
(!command
!ftrace_enabled)
ftrace_startup_all(int
update_all_ops
__register_ftrace_function(ops);
probes.
ftrace_start_up--;
ftrace_call
control_ops_free(ops);
removed_ops
command;
ftrace_start_up
(ftrace_start_up)
(ops_traces_mod(ops))
ftrace_now(raw_smp_processor_id());
(test)
Making
*start_pg;
(!num_to_init)
kzalloc(sizeof(*pg),
free_pages:
get_count_order(pg->size
ENTRIES_PER_PAGE);
free_pages((unsigned
long)pg->records,
kfree(pg);
FTRACE_FUNC_HASHSIZE)
iter->hidx++;
(!hnd)
t_hash_next(m,
rec->data);
iter->ops;
iter->func_pos
FTRACE_ITER_PRINTALL)
FTRACE_ITER_ENABLED)
lseek
FTRACE_ITER_NOTRACE
ftrace_pages_start
ptr);
"\ttramp:
&show_ftrace_seq_ops,
FTRACE_BUFF_MAX))
mutex_lock(&ops->func_hash->regex_lock);
trace_parser_put(&iter->parser);
free_ftrace_hash(iter->hash);
mutex_unlock(&ops->func_hash->regex_lock);
ftrace_regex_open(ops,
*regex,
slen;
(slen
add_hash_entry(hash,
*modname;
(ftrace_match_record(rec,
search_len,
enter_record(hash,
not);
match_records(hash,
(strcmp(buff,
buff[0]
*func,
'mod'
FTRACE_HASH_BITS);
&entry->data);
(hhd->first)
registered?
ftrace_probe_registered
**orig_hash
&trace_probe_ops.func_hash->filter_hash;
filter_parse_regex(glob,
strlen(glob),
(WARN_ON(not))
mutex_lock(&trace_probe_ops.func_hash->regex_lock);
entry->ops
ftrace_hash_move(&trace_probe_ops,
mutex_unlock(&trace_probe_ops.func_hash->regex_lock);
PROBE_TEST_FUNC
(glob)
*orig_hash);
list_for_each_entry_safe(entry,
*parser;
&iter->parser;
parser->buffer,
ftrace_regex_write(file,
**orig_hash;
*old_hash;
&old_hash_ops);
ftrace_set_hash(ops,
Notrace
ftrace_set_regex(&global_ops,
ftrace_set_func(unsigned
*array,
*idx,
save_global_trampoline;
ftrace_graph_funcs;
&ftrace_graph_count;
ftrace_graph_notrace_funcs;
&ftrace_graph_notrace_count;
ftrace_set_early_filter(&global_ops,
ftrace_avail_fops
__g_next(m,
fgd;
*fgd;
fgd
kmalloc(sizeof(*fgd),
(fgd
fgd->table
FTRACE_GRAPH_MAX_FUNCS;
fgd->count
fgd->seq_ops
&ftrace_graph_seq_ops;
__ftrace_graph_open(inode,
fgd);
exists;
(trace_parser_get_init(&parser,
trace_get_user(&parser,
trace_parser_loaded((&parser)))
parser.buffer[parser.idx]
trace_parser_put(&parser);
ftrace_graph_write,
ftrace_graph_release,
actualy
ftrace_init_dyn_tracefs(struct
&ftrace_avail_fops);
(*ipa
*ipb)
(!addr)
last_pg
mod->ftrace_callsites
ftrace_module_notify_exit(struct
last_ftrace_enabled
ftrace_startup(ops,
___ret
(!___ret)
(ops)->flags
___ret;
tr->ops->private
trace_test_and_set_recursion(TRACE_LIST_START,
TRACE_LIST_MAX);
saved.
__ftrace_ops_list_func(ip,
protection,
idle_task(cpu);
clear_tsk_trace_trace(p);
set_tsk_trace_trace(p);
do_each_pid_task(pid,
while_each_pid_task(pid,
(fpid->pid
ftrace_update_pid_func();
ftrace_startup_all(0);
set_ftrace_pid"
(trace_func_graph_ret_t)ftrace_stub;
FTRACE_RETSTACK_ALLOC_SIZE;
atomic_set(&t->tracing_graph_pause,
atomic_set(&t->trace_overrun,
trace->func,
do_test
ftrace_graph_entry_test;
ftrace_graph_active--;
ret_stack;
*ret_stack;
graph_init_task(t,
ret_stack);
(secs)
(bin
bin
*d;
tk_debug_account_sleep_time(struct
<linux/clockchips.h>
lock(broadcast_lock);
guts
ce_broadcast_hrtimer
Bind
bc->bound_on
.features
CLOCK_EVT_FEAT_ONESHOT
.rating
IBM,
(johnstul@us.ibm.com)
Jiffies
denominator
coarse
chosen,
34
67
(cycle_t)
JIFFIES_SHIFT,
(hardirq
__synchronize_hardirq(desc);
irq_default_affinity;
irq_set_thread_affinity(struct
irq_can_move_pcntxt(struct
irq_move_pending(struct
irq_copy_pending(struct
irq_get_pending(struct
irq_do_set_affinity(struct
IRQ_SET_MASK_OK:
IRQ_SET_MASK_OK_DONE:
IRQ_SET_MASK_OK_NOCOPY:
(!chip
IRQD_AFFINITY_SET);
desc->affinity_hint
*notify
desc->irq_data.affinity);
@notify:
notification,
desc->affinity_notify
desc->irq_data.node;
irq_do_set_affinity(&desc->irq_data,
irq_select_affinity_usr(unsigned
setup_affinity(irq,
__disable_irq(struct
(!__disable_irq_nosync(irq))
__enable_irq(struct
irq_settings_set_noprobe(desc);
irq_enable(desc);
check_irq_resend(desc,
disable,
set_irq_wake_real(irq,
desc->wake_depth
IRQD_WAKEUP_STATE);
exclusively
canrequest
__irq_set_trigger(struct
IRQF_TRIGGER_*
(chip->flags
request_threaded_irq
Primary
(test_and_clear_bit(IRQTF_RUNTHREAD,
IRQS_ONESHOT))
forever.
irq_wake_thread().
!irqd_irq_disabled(&desc->irq_data)
irqd_irq_masked(&desc->irq_data))
irq_thread_check_affinity(struct
(valid)
explicitely
bh
busses
irq_to_desc(action->irq);
wake_threads_waitq(desc);
handler_fn
irq_thread_dtor);
irq_thread_check_affinity(desc,
action_ret;
IRQ_HANDLED)
(action->dev_id
__irq_wake_thread(desc,
(!force_irqthreads)
IRQF_PERCPU
IRQF_ONESHOT))
(!new->thread_fn)
&new->thread_flags);
new->handler
irq_default_primary_handler;
d->chip;
out_mput;
(IS_ERR(t))
PTR_ERR(t);
new->thread
Drivers
knowledge
ONESHOT
old_ptr
&desc->action;
*old_ptr;
(level,
IRQF_SHARED
ONESHOT.
mismatch;
IRQF_PERCPU)
action->thread_mask
!ONESHOT
IRQF_ONESHOT)
unmasked.
IRQD_PER_CPU);
IRQD_NO_BALANCING);
IRQS_SPURIOUS_DISABLED))
(new->thread)
CONFIG_DEBUG_SHIRQ
WARN(in_interrupt(),
context!\n",
action_ptr
already-free
unregister_handler_proc(irq,
ought
'real'
fake.
(desc
card
dev_id));
@thread_fn.
IRQ_WAKE_THREAD
Dev_id
cookie.
irqaction),
action->handler
action->flags
irqflags;
devname;
kfree(action);
Threaded
IRQC_IS_HARDIRQ
!ret
IRQ_GET_DESC_CHECK_PERCPU);
bad;
irq_settings_is_per_cpu_devid(desc))
request_percpu_irq
irqchip_irq_state
*state)
irq_desc_get_irq_data(desc);
(data);
tick_broadcast_mask;
device:
!curdev
newdev->rating
(!try_module_get(dev->owner))
tick_broadcast_device.evtdev
(dev
__clockevents_update_freq(dev,
(!dev->broadcast)
dev->broadcast
available\n",
*bc
placeholder
tick_device_setup_broadcast_func(dev);
(tick_broadcast_device.mode)
TICKDEV_MODE_ONESHOT:
tick_broadcast_clear_oneshot(cpu);
TICKDEV_MODE_PERIODIC:
misfeature
cpumask_and(tmpmask,
tick_do_broadcast(tmpmask);
tick_do_periodic_broadcast();
CLOCK_EVT_STATE_PERIODIC)
devices,
programmed
dev->next_event;
(!cpumask_test_and_set_cpu(cpu,
tick_broadcast_mask))
(!broadcast)
(bc
tick_unfreeze()
XEN
TICKDEV_MODE_ONESHOT)
tick_broadcast_oneshot_mask;
tick_broadcast_oneshot_control().
(!(bc->features
clockevents_program_event(bc,
oneshot.
next_event;
next_event.tv64
(td->evtdev->next_event.tv64
tmpmask,
next_event,
bc->next_event.tv64)
CLOCK_EVT_STATE_SHUTDOWN);
tick_broadcast_set_event(bc,
dev->next_event,
broadcast_needs_cpu(bc,
ping-pong
fixups.
tick_broadcast_device.mode
(nr_range
(!range[i].end)
range[nr_range
range[j].end)
range[i].end
range[i].start
(r1->start
r2->start)
nr_range
j--)
k--;
sort(range,
range),
cmp_range,
plugins
fast,
scalable,
cpu_clock(),
~1
Simply
trace_clock_struct.prev_time
timings,
kernel/rcu/torture.c.
<paulmck@us.ibm.com>");
stress-test
LOCKTORTURE_RUNNABLE_INIT
*trsp);
lock_torture_cxt
ATOMIC_INIT(0),
BUGGY,
longdelay_us
longdelay_us)))
mdelay(longdelay_us);
__acquires(torture_spinlock)
__releases(torture_spinlock)
torture_spin_lock_write_delay,
(cxt.nrealreaders_stress
torture_rwlock_write_delay,
torture_rwlock_read_delay,
write_unlock_irqrestore(&torture_rwlock,
__acquires(torture_rwsem)
__releases(torture_rwsem)
((torture_random(&rand)
0xfffff)
(WARN_ON_ONCE(lock_is_write_held))
lwsp->n_lock_fail++;
n_stress;
statp[i].n_lock_fail)
statp[i].n_lock_fail;
sum,
lock_torture_stats
8192;
pr_err("lock_torture_stats_print:
need:
__torture_print_stats(buf,
pr_alert("%s",
lock_torture_stats_print();
cxt.nrealwriters_stress,
cxt.nrealreaders_stress,
writer_tasks[i]);
writer_tasks
(nwriters_stress
(strncmp(torture_type,
CONFIG_DEBUG_SPINLOCK
writers.
(onoff_interval
(shutdown_secs
poor
handle_bad_irq
ack_bad_irq(irq);
handler:
"IRQ
__irq_wake_thread(struct
desc->state
IRQS_INPROGRESS)
synchronize_irq()
handle_irq_event_percpu(struct
trace_irq_handler_entry(irq,
trace_irq_handler_exit(irq,
(!noirqdebug)
note_interrupt(irq,
handle_irq_event(struct
handle_irq_event_percpu(desc,
"cpudeadline.h"
dl_time_before(u64
(s64)(a
largest;
cp->size)
Push
IDX_INVALID
cp->elements[idx].dl))
cp->elements[idx].dl
new_dl;
cpudl_heapify(cp,
cpudl_find(struct
cpu_rq(cpu)->lock
cpudl_set(struct
dl,
IDX_INVALID)
invalid.
cp->elements[cpu].idx
IDX_INVALID;
cpudl_change_key(cp,
cpudl.free_cpus
cpudl_set_freecpu(struct
cpudl_clear_freecpu(struct
cpudl_init(struct
memset(cp,
sizeof(*cp));
kcalloc(nr_cpu_ids,
kfree(cp->elements);
cpudl_cleanup(struct
IRQF_MODIFY_MASK
_IRQF_MODIFY_MASK);
_IRQ_PER_CPU;
_IRQ_NO_BALANCING;
_IRQ_LEVEL;
_IRQ_NOREQUEST);
"cpupri.h"
improves
SCHED_LOAD_RESOLUTION
scale_load(w)
((w)
scale_load_down(w)
(w)
SCHED_LOAD_SHIFT
rt_runtime_lock;
where:
dl_total_bw
i-eth
element,
Moreover,
bw,
tsk_bw)
tsk_bw;
arithmetics
problems.
(The
*cfs_b);
*tsk);
*rb_leftmost;
*last,
aggregated
runnable_load_avg
Required
f(tg)
leaf_cfs_rq_list;
CONFIG_IRQ_WORK
overloaded;
push_cpu;
dl_bw;
"RT
rule:
nr_switches;
dl;
sched_info
CONFIG_CPU_IDLE
inspected
cpumask[0];
booted
masking
*task_group(struct
set_task_rq(struct
->cpu
off:
sched_feat_numa(x)
aid
10%
-15
-10
-5
335,
29,
18,
ENQUEUE_WAKING
RETRY_TASK
next_cpu);
idle_set_state(struct
*idle_state)
*idle_get_state(struct
init_sched_dl_class(void);
prev_nr
hrtick_enabled(struct
sched_rt_avg_update(struct
rt_delta)
(likely(rq
!task_on_rq_migrating(p)))
(unlikely(task_on_rq_migrating(p)))
ACQUIRE
(rq->lock)
[S]
task_rq()
RELEASE
double_lock_balance:
expense
_double_lock_balance(struct
__releases(this_rq->lock)
__acquires(busiest->lock)
__acquires(this_rq->lock)
busiest);
spin_lock_nested(l2,
double_rq_lock
__acquires(rq1->lock)
__acquires(rq2->lock)
__acquire(rq2->lock);
Fake
double_rq_unlock
task_rq_unlock,
double_rq_unlock(struct
__releases(rq1->lock)
__releases(rq2->lock)
raw_spin_unlock(&rq1->lock);
__release(rq2->lock);
BUG_ON(rq1
rq2);
DECLARE_PER_CPU(u64,
irq_time_write_begin(void)
__this_cpu_inc(irq_time_seq.sequence);
irq_time_write_end(void)
irq_time_read(int
irq_time;
per_cpu(cpu_softirq_time,
per_cpu(cpu_hardirq_time,
extracted
Inc
BULL
SA.
Mochel's
2001-3
2003-10-10
Simon
Derr.
2003-10-22
Hemminger.
May-July
Jackson.
<linux/cgroupstats.h>
linger
DEFINE_MUTEX(cgroup_mutex);
DECLARE_RWSEM(css_set_rwsem);
SUBSYS(_x)
[_x
_cgrp_id]
<linux/cgroup_subsys.h>
SUBSYS
need_forkexit_callback
rebind_subsystems(struct
*dst_root,
cgroup_destroy_locked(struct
*cgrp);
create_css(struct
css_release(struct
kill_css(struct
cgroup_addrm_files(struct
cfts[],
is_add);
*parent_css
@cgrp->self)
ss->id)))
(cgroup_parent(cgrp)
(cgrp);
init_css_set.subsys[ss->id];
css_get(css);
of->kn->parent->priv;
cgroup_css().
seq_css()
@ancestor.
(cgrp)
cgroup_[tree_]mutex.
for_each_css(css,
CGROUP_SUBSYS_COUNT;
(!((css)
cgroup_e_css(cgrp,
cgroup_release_agent(struct
check_for_release(struct
direction,
naturally
association
improve
.refcount
css_set_count
propagated
refcounts
list_del(&link->cgrp_link);
(list_empty(&cgrp->cset_links))
cgroup_update_populated(cgrp,
put_css_set_locked(cset);
@cset:
@template:
*template[])
l1
l2
BUG_ON(l2
&old_cset->cgrp_links);
Locate
cgrp_link);
template[i]
@tmp_links:
cset_link);
substituted
(cset)
(c->root
link_css_set(&tmp_links,
hash_add(css_set_table,
*kf_root)
*root_cgrp
root->hierarchy_id
root->hierarchy_id);
(root)
rebind_subsystems(&cgrp_dfl_root,
&cgrp->cset_links,
cgroup_exit_root_id(root);
kernfs_destroy_root(root->kf_root);
cgroup_free_root(root);
BUG_ON(!res);
cgroup_populate_dir(struct
subsys_mask);
kernfs_syscall_ops
strncpy(buf,
cft->name,
@subtree_control
cur_ss_mask
(!cgroup_on_dfl(cgrp))
cgroup_calc_child_subsys_mask(cgrp,
cgrp->subtree_control);
@kn:
cgroup_kn_lock_live()
kn->parent->priv;
kernfs_unbreak_active_protection(kn);
cgroup_put(cgrp);
servicing
alive.
active_ref.
liveliness
kernfs_break_active_protection(kn);
cgroup_kn_unlock(kn);
name[CGROUP_FILE_NAME_MAX];
cgroup_file_name(cgrp,
@subsys_mask:
subsys_mask)
*cfts;
(!(subsys_mask
list_for_each_entry(cfts,
&ss->cfts,
(!(ss_mask
&cgrp_dfl_root
dfl_root
tmp_ss_mask
(ss_mask
css->cgroup
hash_for_each(css_set_table,
(ss->bind)
CGRP_ROOT_NOPREFIX)
(test_bit(CGRP_CPUSET_CLONE_CHILDREN,
(strlen(root->name))
*opts)
all_ss
one_ss
CONFIG_CPUSETS
cpuset_cgrp_id);
((token
","))
Mutually
(!strncmp(token,
(opts->release_agent)
'.')
(opts->name)
ss->name))
opts->subsys_mask
CGROUP_SUBSYS_COUNT)
CGRP_ROOT_SANE_BEHAVIOR)
allowed\n");
noprefix
mounting
(opts->subsys_mask
parse_cgroupfs_options(data,
(opts.subsys_mask
root->subsys_mask
rebind_subsystems(root,
strcpy(root->release_agent_path,
kfree(opts.release_agent);
kfree(opts.name);
mount.
(use_task_css_set_links)
cgrp->self.flags
CSS_ONLINE;
cgrp->root
init_cgroup_housekeeping(cgrp);
*base_files;
cgroup_idr_alloc(&root->cgroup_idr,
cancel_ref;
root->kf_root
root_cgrp);
cgroup_dfl_base_files;
cgroup_legacy_base_files;
base_files,
destroy_root;
*fs_type,
*unused_dev_name,
&cgrp_dfl_root;
asynchronous,
name_match
sb
super
deactivate_super(pinned_sb);
thing,
specification
cgroup_put(&root->cgrp);
ref.
*sb)
mounts
.mount
@task's
*path
committed,
->mg_tasks
@tset:
tset->cur_cset
css_set,
mg_node);
tset->cur_task
cgroup_mutex,
old_cset
task_css_set(tsk);
cgroup_taskset_first()
cgroup_migrate()
gained
list_for_each_entry_safe(cset,
preloaded_csets,
@src_cset
cgroup_migrate_finish().
*src_cset,
src_cset->mg_src_cgrp
preloaded_csets);
@dst_cgrp
source,
put_css_set(dst_cset);
@leader:
@threadgroup:
@leader
@leader.
*leader,
disturbing
&tset.src_csets);
(!threadgroup)
while_each_thread(leader,
legitimately
mg_node)
&cset->mg_tasks,
threadgroup);
&pid)
__task_cred(tsk);
GLOBAL_ROOT_UID)
(threadgroup)
no.
threadgroup_unlock(tsk);
__cgroup_procs_write(of,
(printed)
&css->cgroup->cset_links,
cgroup_migrate_add_src(link->cset,
src_csets
src_cset
"cgroup:
thread?
threadgroup_unlock(task);
(%d),
(*tok
unavailable
dependencies.
old_ss
ss))
(!(enable
(css_enable
ssid));
err_undo_css;
subtree.
hide
cgroup_css(child,
cgroup_clear_dir(child,
this_css)
(!css)
&v);
cft));
.atomic_write_len
cgroup_file_write,
cgroup_seqfile_show,
rename
iattr
*kn;
kn
(IS_ERR(kn))
PTR_ERR(kn);
cgroup_kn_set_ugid(kn);
kn;
is_add)
cft);
(cft->max_write_len
cft->max_write_len
cft->kf_ops
cft->ss
cgroup_exit_cftypes(cfts);
ss;
(!cfts
cgroup_apply_cftypes(cfts,
Files
cgroup_rm_cftypes_locked(cfts);
cgroup_add_cftypes()
cft
cgroup_add_cftypes(ss,
dfl
ss->dfl_cftypes
ss->legacy_cftypes)
@parent
unlinked
CSS_RELEASED
inbetween
sibling);
css_next_child(NULL,
css_next_child(pos,
pos->parent);
pos->parent;
@pos.
(pos);
post-order
@it
it->cset_pos;
l->next;
(it->ss)
list_entry(l,
link->cset;
css_advance_task_iter(it);
css_task_iter_start().
moved,
read(),
CGROUP_FILE_TASKS,
tasks)
mutex_lock(&cgrp->pidlist_mutex);
&cgrp->pidlists,
links)
mod_delayed_work(cgroup_pidlist_destroy_wq,
&l->destroy_dwork,
pidlist_free(l->list);
(src
fairly
expectation
one-to-one
disturbs
*(pid_t
*)b;
pid_fry(*(pid_t
task_nsproxy()
l->key.ns
procs
cgroup_pidlist_find(cgrp,
css_task_iter_start(&cgrp->self,
((tsk
CGROUP_FILE_PROCS)
sort(array,
array;
*kn
kernfs_node_from_dentry(dentry);
(dentry->d_sb->s_type
&cgroup_fs_type
!kn
kernfs_type(kn)
@kn->priv
rcu_dereference(kn->priv);
Initially
@of->priv
start()
l->length;
(cgroup_pid_fry(cgrp,
l->list[mid])
*(int
set_bit(CGRP_NOTIFY_ON_RELEASE,
"cgroup.procs",
cgroup_procs_write,
"cgroup.controllers",
Killing
kill_css().
percpu_ref_exit(&css->refcnt);
cgroup_idr_remove(&ss->css_idr,
(cgroup_parent(cgrp))
kfree(cgrp);
list_del_rcu(&css->sibling);
cgroup_idr_replace(&ss->css_idr,
cgrp->id
call_rcu(&css->rcu_head,
css_free_rcu_fn);
*ref)
container_of(ref,
refcnt);
css_serial_nr_next++;
CSS
~CSS_ONLINE;
init_and_link_css(css,
cgrp);
pr_warn("%s
cgroup_clear_dir(css->cgroup,
css->ss->id);
manually.
cgroup_destroy_locked(cgrp);
percpu_ref_kill()
requirements,
s1.
RCU-freed.
empty;
parent/child
early)
refcnting.
CSS_NO_REF;
(early)
BUG_ON(css->id
ss->name
ss->id,
long\n",
(ss->early_init)
cgroup_init_subsys(ss,
BUG_ON(cgroup_init_cftypes(NULL,
ss->id;
ss->dfl_cftypes));
Cap
"%s%s",
(!path)
path);
cg_list
cset);
(need_forkexit_callback)
notify_on_release
put_cset
task_css(tsk,
released,
argv[0]
@dentry
CONFIG_CGROUP_DEBUG
list_for_each_entry(task,
(count++
MAX_TASKS_SHOWN_PER_CSS)
task_pid_vnr(task));
<linux/kmemleak.h>
<linux/jump_label.h>
<linux/pfn.h>
<trace/events/module.h>
CONFIG_DEBUG_SET_MODULE_RONX
debug_align(X)
param_get_bool,
allocation,
module_addr_max
*secstrings,
*strtab;
MODULE_STATE_UNFORMED);
nfsd
do_exit(code);
"ignore
*num)
*num
symversion(base,
*arr,
(*fn)(const
__start___ksymtab,
WILL_BE_GPL_ONLY,
(each_symbol_in_section(arr,
ARRAY_SIZE(arr),
mod->gpl_syms,
mod->gpl_syms
mod->gpl_future_syms,
mod->gpl_future_syms
mod->unused_syms,
mod->unused_syms
mod->unused_gpl_syms,
mod->unused_gpl_syms
gplok;
warn;
symnum,
*fsa
(syms->licence
fsa->warn)
pr_warn("Symbol
fsa->name);
kernel_symbol),
cmp_name);
(sym
(optional)
strlen(name),
*mod_percpu(struct
percpu_modalloc(struct
percpu_modfree(struct
find_pcpusec(struct
percpu_modcopy(struct
is_module_percpu_address(unsigned
*start
ENOMEM
0444
MODULE_REF_BASE
loader.
module_unload_init(struct
Hold
a->name,
%s.\n",
a->name);
ref_module(struct
b))
strong_try_module_get(b);
EXPORT_SYMBOL_GPL(ref_module);
module_unload_free(struct
CONFIG_MODULE_FORCE_UNLOAD
try_force_unload(unsigned
try_force_unload(flags);
free_module(struct
name_user,
(!capable(CAP_SYS_MODULE)
modules_disabled)
MODULE_STATE_LIVE)
Final
free_module(mod);
print_unload_info(struct
module_refcount(mod));
&mod->source_list,
*symbol)
trace_module_get(module,
'O';
check_version(Elf_Shdr
*crc,
num_versions;
modversion_info
--force
maybe_relocated(*crc,
check_modstruct_version(Elf_Shdr
&crc,
same_magic(const
*amagic,
*bmagic,
has_crcs)
strcmp(amagic,
bmagic)
wait_event_interruptible(),
info->index.vers,
getname;
sym;
*ksym;
PTR_ERR(ksym)
mattr;
grp;
container_of(mattr,
mattr);
section;
add_sect_attrs(struct
nloaded
mod->sect_attrs
remove_sect_attrs(struct
sysfs_remove_group(&mod->mkobj.kobj,
Deallocate
*bin_attr,
add_notes_attrs(struct
&mod->mkobj.kobj);
remove_notes_attrs(struct
del_usage_links(struct
1)),
module_remove_modinfo_attrs(struct
(attr->free)
attr->free(mod);
*kobj;
kobj
kset_find_obj(module_kset,
(kobj)
module_kset;
&module_ktype,
mod_sysfs_setup(struct
kparam,
module_param_sysfs_remove(mod);
kobject_put(mod->holders_dir);
mod_sysfs_fini(struct
begin_pfn
begin_pfn)
begin_pfn);
set_section_ro_nx(void
text_size,
total_size)
PFN_UP((unsigned
text_size);
total_size);
unset_module_core_ro_nx(struct
mod->core_size,
set_memory_x);
mod->core_ro_size,
unset_module_init_ro_nx(struct
mod->init_ro_size,
((mod->module_core)
(mod->core_text_size))
((mod->module_init)
(mod->init_text_size))
MODULE_STATE_UNFORMED;
module_arch_cleanup(mod);
module_unload_free(mod);
mod->num_kp);
carefully:
list_del_rcu(&mod->list);
module_bug_cleanup(mod);
module_memfree(mod->module_init);
kfree(mod->args);
percpu_modfree(mod);
lock-classes;
lockdep_free_key_range(mod->module_core,
unset_module_core_ro_nx(mod);
s++)
%s)\n",
st_value
*symsec
&info->sechdrs[info->index.sym];
*sym
*)symsec->sh_addr;
secbase;
symsec->sh_size
info->strtab
symbol:
Ok
sym[i].st_value
STB_WEAK)
Unknown
secbase
info->strtab,
info->index.sym,
SHT_RELA)
section)
sizes,
fields:
SHF_WRITE
order:\n");
ARRAY_SIZE(masks);
++m)
*sname
s->sh_name;
((s->sh_flags
masks[m][0])
masks[m][0]
(s->sh_flags
masks[m][1])
".init"))
sname);
mod->core_size;
RO:
ro-data
mod->init_text_size
mod->init_size;
mod->init_ro_size
string++;
((*secsize)--
padding.
taglen
'=')
modinfo_attrs[i]);
lookup_symbol(name,
SHN_UNDEF)
SHF_EXECINSTR)
ARCH_SHF_SMALL)
'd';
(sechdrs[sym->st_shndx].sh_type
'n';
SHN_UNDEF
CONFIG_KALLSYMS_ALL
layout_symtab(struct
strtab_size
SHF_ALLOC;
INIT_OFFSET_MASK;
(ndst
is_core_symbol(src+i,
info->hdr->e_shnum))
part.
info->symoffs
add_kallsyms(struct
ndst;
mod->symtab
mod->num_symtab
mod->symtab[i].st_info
mod->core_strtab;
long)ret;
module_addr_max)
kmemleak_load_module(const
SHF_EXECINSTR))
module_sig_check(struct
markerlen
!CONFIG_MODULE_SIG
sizeof(*(info->hdr)))
ELFMAG,
SELFMAG)
ET_REL
sizeof(Elf_Shdr))
sizeof(Elf_Shdr)
info->len.
(!info->hdr)
kstat
vfs_getattr(&f.file->f_path,
&stat);
vmalloc,
whines.
vmalloc(stat.size);
kernel_read(f.file,
stat.size
(shdr->sh_type
shdr->sh_addr
info->index.vers
(we'll
strings.
SHT_SYMTAB)
pr_warn("No
*)info->sechdrs[info->index.mod].sh_addr;
staging
quality
CONFIG_CONSTRUCTORS
mod->ctors
sizeof(*mod->ctors),
&mod->num_ctors);
mod->trace_events
memset(ptr,
*dest;
(shdr->sh_entsize
ndiswrapper
needs.
wrongly
flush_icache_range((unsigned
*hdr,
(IS_ERR(mod))
sizes.
this!
relocated
MODULE_STATE_LIVE
request_module()
once;
particular,
modname);
tainting
unlink_mod;
location,
free_unload;
mod->kp,
bug_cleanup;
free_copy(info);
sync_rcu()
may_init_module();
load_module(&info,
annoying
(str[0]
str[0]
1).
(mod->symtab[i].st_value
mod->symtab[i].st_value
*(mod->strtab
mod->symtab[i].st_name)
!is_arm_mapping_symbol(mod->strtab
mod->symtab[i].st_name))
mod->symtab[i].st_value;
mod->symtab[best].st_value;
Careful
**modname,
*namebuf)
*modname
namebuf;
*symname)
strlcpy(modname,
strlcpy(name,
*module_name,
colon
mod_find_symname(mod,
fn(data,
bx
*m_start(struct
*m_next(struct
m_stop(struct
buf[8];
mod->module_core);
module_flags(mod,
buf));
deps
(e)
code?
oopsing.
STATIC_KEY_INIT_FALSE;
unprocessed
top_cpuset.old_mems_allowed
memory_pressure
Retrieve
task_has_mempolicy(struct
@pos_css:
css_cs((pos_css)))))
root_cs)
asynchronously.
cpuset_hotplug_workfn(struct
ugly,
cpusets's
node_states[N_MEMORY].
free_cs;
cs->cpus_allowed);
free_cpus:
free_cpumask_var(trial->cpus_allowed);
free_cs:
kfree(trial);
in-use
cpuset_for_each_child(c,
hiearchy,
b->effective_cpus);
*dattr,
(is_sched_load_balance(cp))
(sched
is_sched_load_balance
Array
(using
'pn'
ptrs
alloc_sched_domains(ndoms);
sched_domain_attr),
csn
(!cpumask_empty(cp->cpus_allowed)
csa[i];
a->pn;
csa[j];
bpn
(nslot
nslot,
dp,
Fallback
'sched_load_balance'
get_online_cpus().
rebuild_sched_domains_locked(void)
cpuset's.
effective_cpus
need_rebuild_sched_domains
parent_cs(cp);
(cgroup_on_dfl(cp->css.cgroup)
cpumask_copy(new_cpus,
(!css_tryget_online(&cp->css))
WARN_ON(!cgroup_on_dfl(cp->css.cgroup)
css_put(&cp->css);
*trialcs,
parsing.
tsk->mems_allowed
guarantee_online_mems(task_cs(tsk),
(unlikely(test_thread_flag(TIF_MEMDIE)))
unchanged
(need_loop)
mpol_rebind_task(tsk,
newmems,
newmems;
mpol_dup()
guarantee_online_mems(cs,
mpol_rebind_mm()
cpuset_change_task_nodemask(task,
is_memory_migrate(cs);
mpol_rebind_mm(mm,
&cs->mems_allowed);
cpuset_migrate_mm(mm,
cs->old_mems_allowed
vmas
effective_mems
*new_mems
parent->effective_mems;
vma's
cs->relax_domain_level
cpuset_update_task_spread_flag(cs,
*trialcs;
trialcs
alloc_trial_cpuset(cs);
(!trialcs)
&trialcs->flags);
free_trial_cpuset(trialcs);
meter.
fmeter_getrate()
fmeter_update()
FM_COEF
happening,
millisecond
fmp->time
FM_SCALE;
spin_lock(&fmp->lock);
fmeter_update(fmp);
spin_unlock(&fmp->lock);
usable;
cpuset_attach_nodemask_to;
FILE_MEMORY_MIGRATE,
FILE_CPULIST,
FILE_MEMLIST,
FILE_EFFECTIVE_CPULIST,
FILE_EFFECTIVE_MEMLIST,
FILE_CPU_EXCLUSIVE,
FILE_MEM_EXCLUSIVE,
FILE_MEM_HARDWALL,
FILE_SCHED_LOAD_BALANCE,
FILE_SCHED_RELAX_DOMAIN_LEVEL,
FILE_MEMORY_PRESSURE_ENABLED,
FILE_MEMORY_PRESSURE,
FILE_SPREAD_PAGE,
FILE_SPREAD_SLAB,
FILE_CPU_EXCLUSIVE:
FILE_MEM_EXCLUSIVE:
FILE_MEM_HARDWALL:
FILE_SCHED_LOAD_BALANCE:
update_flag(CS_SCHED_LOAD_BALANCE,
FILE_MEMORY_MIGRATE:
FILE_MEMORY_PRESSURE_ENABLED:
cpuset_memory_pressure_enabled
FILE_MEMORY_PRESSURE:
FILE_SPREAD_PAGE:
FILE_SPREAD_SLAB:
FILE_SCHED_RELAX_DOMAIN_LEVEL:
"cpus"
"mems"
@cs's
FILE_CPULIST:
trialcs,
FILE_MEMLIST:
css_put(&cs->css);
sequential
cpuset_write_resmask,
(100U
set_bit(CS_SCHED_LOAD_BALANCE,
free_cpumask_var(cs->cpus_allowed);
kfree(cs);
(cgroup_on_dfl(cs->css.cgroup))
refuse
clone()
parent->mems_allowed;
parent->cpus_allowed);
.bind
next-highest
*new_cpus,
*new_mems,
(cpus_updated
update_tasks_cpumask(cs);
update_tasks_nodemask(cs);
parent_cs(cs)->effective_cpus);
(mems_updated)
new_cpus;
mems_updated;
cpus_updated
mems_updated
&new_cpus,
&new_mems,
mems_updated);
transparent
(!on_dfl)
cpumask_copy(top_cpuset.effective_cpus,
top_cpuset.effective_mems
schedule_work(&cpuset_hotplug_work);
Guaranteed
current->mems_allowed);
mem_exclusive
mem_hardwall
unusual
node?
allowed?
node_isset(node,
cpuset_mem_spread_node()
"it
did,
node_random(&current->mems_allowed);
@tsk1's
Prints
Collection
per-cpuset
(direct)
CONFIG_PROC_PID_CPUSET
tsk->cpuset
nodemask_pr_args(&task->mems_allowed));
"#%d",
"/%d",
class->key);
find-mask
ons:
offs:
nr_irq_safe
nr_irq_unsafe
nr_hardirq_safe
nr_hardirq_unsafe
sum_forward_deps
LOCKF_USED_IN_SOFTIRQ)
LOCKF_ENABLED_SOFTIRQ)
LOCKF_USED_IN_SOFTIRQ_READ)
LOCKF_ENABLED_SOFTIRQ_READ)
nr_unused);
*dr
bufsiz,
div
namelen,
snprintf(name+namelen,
"%14lu
ip[32];
40-namelen,
namelen);
snprintf(ip,
sizeof(ip),
"[<%p>]",
*)class->contention_point[i]);
%29s
*)class->contending_point[i]);
'.',
data->iter_end
(get_user(c,
*chip)
desc->irq_data.chip
desc->irq_data.handler_data
irq_offset,
desc->irq_data.msi_desc
desc->irq_data.chip_data
IRQD_IRQ_MASKED);
irq_startup(struct
irq_state_clr_disabled(desc);
irq_shutdown(struct
irq_enable(struct
(desc->irq_data.chip->irq_enable)
desc->irq_data.chip->irq_enable(&desc->irq_data);
irq_disable
irq_disable(struct
irq_percpu_enable(struct
desc->percpu_enabled);
irq_percpu_disable(struct
desc->irq_data.chip->irq_ack(&desc->irq_data);
mask_irq(struct
unmask_irq(struct
unmask_threaded_irq(struct
suspended,
clear,
completely).
irqd_irq_masked(&desc->irq_data)
!desc->threads_oneshot)
preflow_handler(struct
modern
chip);
!desc->action)
(unlikely(!desc->action))
!irqd_irq_disabled(&desc->irq_data));
eoi
(chip->irq_ack)
chip->irq_ack(&desc->irq_data);
(chip->irq_eoi)
handle_bad_irq;
(handle
desc->handle_irq
desc->name
irq_settings_clr_and_set(desc,
IRQD_MOVE_PCNTXT);
irq_cpu_online
for_each_active_irq(irq)
irq_data_get_irq_chip(&desc->irq_data);
IRQCHIP_ONOFFLINE_ENABLED)
!irqd_irq_disabled(&desc->irq_data)))
irq_cpu_offline
dest,
(data->chip
@msg:
opcode;
mutexes[MAX_RT_TEST_MUTEXES];
switch(td->opcode)
MAX_RT_TEST_MUTEXES;
MAX_RT_TEST_MUTEXES)
tid,
MAX_RT_TEST_THREADS;
handle_op(td,
td->opdata
set_freezable();
test_thread_data,
terminated!
\n:
schedpar.sched_priority
sched_setscheduler(threads[tid],
&schedpar);
"rttest",
device_create_file(&thread_data[i].dev,
1995,
Shaver.
1/8/97,
09/08/99,
Carlos
H.
Bauer.
<linux/aio.h>
<linux/bitmap.h>
<linux/limits.h>
<linux/pipe_fs_i.h>
<asm/processor.h>
CONFIG_BSD_PROCESS_ACCT
CONFIG_CHR_DEV_SG
CONFIG_LOCKUP_DETECTOR
pid_max_min,
proc_dointvec_minmax
GID
sysctl_hung_task_timeout_secs
CONFIG_INOTIFY_USER
__hppa__
CONFIG_SYSCTL_ARCH_UNALIGN_ALLOW
CONFIG_SYSCTL_ARCH_UNALIGN_NO_WARN
sysctl_writes_strict
proc_do_cad_pid(struct
proc_taint(struct
proc_dointvec_minmax_sysadmin(struct
proc_dointvec_minmax_coredump(struct
proc_dostring_coredump(struct
CONFIG_EPOLL
HAVE_ARCH_PICK_MMAP_LAYOUT
"vm",
"fs",
"dev",
&min_sched_granularity_ns,
&max_sched_granularity_ns,
sched_rt_handler,
&neg_one,
256,
"hotplug",
"random",
"overflowuid",
"overflowgid",
&show_unhandled_signals,
proc_dointvec_minmax_sysadmin,
User-space
sizeof(dirty_expire_interval),
CONFIG_MEMORY_FAILURE
defined(CONFIG_BINFMT_MISC)
defined(CONFIG_BINFMT_MISC_MODULE)
&inodes_stat,
proc_nr_inodes,
CONFIG_FILE_LOCKING
"inotify",
proc_dointvec
defined(CONFIG_OPTPROBES)
(!data
strlen(data);
*lenp)
proc_dostring(struct
(*size)
@neg:
%0
memcpy(tmp,
*neg
&p,
sprintf(p,
*size)
*lvalp;
(*conv)(bool
vleft,
!table->maxlen
!*lenp
vleft
table->maxlen
(sysctl_writes_strict)
SYSCTL_WRITES_STRICT:
SYSCTL_WRITES_WARN:
vleft--;
proc_wspace_sep,
sizeof(proc_wspace_sep),
(conv(&neg,
'\t');
(!write
!first
proc_dointvec(struct
tmptaint
proc_doulongvec_minmax(&t,
*max;
proc_dointvec_minmax(struct
do_proc_dointvec(table,
core_pattern[0]
"\
validate_coredump_safety();
convdiv)
&val,
proc_doulongvec_minmax(struct
do_proc_doulongvec_minmax(table,
proc_doulongvec_ms_jiffies_minmax(struct
*lvalp);
proc_dointvec_jiffies(struct
1/USER_HZ
proc_dointvec_userhz_jiffies(struct
proc_dointvec_ms_jiffies(struct
new_pid
table->data;
proc_skip_char(&kbuf,
val_a,
val_b
kbuf++;
left--;
val_a
bit_a,
bit_b
bit_a
bitmap_len,
(bit_a
tmp_bitmap,
bitmap_len);
kfree(tmp_bitmap);
poll_spurious_irqs(unsigned
irq_wait_for_poll(struct
irq_settings_is_polled(desc))
asks
(atomic_inc_return(&irq_poll_active)
irq_poll_cpu
atomic_dec(&irq_poll_active);
Racy
mod_timer(&poll_spurious_irq_timer,
POLL_SPURIOUS_IRQ_INTERVAL);
(bad_action_ret(action_ret))
__report_bad_irq(irq,
IRQ_NONE)
traditional
sake
IRQ_WAKE_THREAD)
IRQ_HANDLED.
handled;
31
thread_handled_last
SPURIOUS_DEFERRED;
simplicity
threads_handled_last
desc->last_unhandled
noirqdebug
"Misrouted
<linux/iocontext.h>
<linux/mmu_notifier.h>
<linux/rmap.h>
<linux/user-return-notifier.h>
write_lock_irq(&tasklist_lock)
CONFIG_ARCH_TASK_STRUCT_ALLOCATOR
*alloc_thread_info_node(struct
THREADINFO_GFP,
THREAD_SIZE_ORDER);
free_thread_info(struct
THREAD_SIZE,
free_task_struct(tsk);
max_threads_suggested)
MAX_THREADS;
ARCH_MIN_TASKALIGN
SLAB_NOTRACK,
max_threads/2;
orig);
CONFIG_SECCOMP
then.
tsk->btrace_seq
dup_mmap(struct
*oldmm)
down_write(&oldmm->mmap_sem);
required:
RCU_INIT_POINTER(mm->exe_file,
get_mm_exe_file(oldmm));
rb_link
rb_parent
mpnt
(mpnt->vm_flags
tmp->vm_prev
*mapping
(tmp->vm_flags
*pprev
oldmm,
up_write(&oldmm->mmap_sem);
default_dump_filter
defined(CONFIG_TRANSPARENT_HUGEPAGE)
!USE_SPLIT_PMD_PTLOCKS
mm->flags
mm->def_flags
mm_free_pgd(mm);
free_mm(mm);
pr_alert("BUG:
mm:
allocate_mm();
sizeof(*mm));
BUG_ON(mm
symlink
mutex_unlock(&task->signal->cred_guard_mutex);
killed;
mm_struct,
1998
free_pt;
tsk->nvcsw
tsk->active_mm
that..
*fs
CLONE_FS)
tsk->fs
spin_lock(&fs->lock);
copy_fs_struct(fs);
CLONE_FILES)
&error);
CONFIG_BLOCK
io_context
(!sig)
sighand_cachep
ACCESS_ONCE(sig->rlim[RLIMIT_CPU].rlim_cur);
sig->cputime_expires.prof_exp
tsk->signal
sig->curr_target
task_lock(current->group_leader);
task_unlock(current->group_leader);
(CLONE_NEWNS|CLONE_FS))
(CLONE_NEWUSER|CLONE_FS))
!(clone_flags
VM.
CLONE_VM))
CLONE_PARENT)
fork_out;
bad_fork_free;
~PF_NPROC_EXCEEDED;
p->vfork_done
p->mempolicy
bad_fork_cleanup_policy;
bad_fork_cleanup_io;
&init_struct_pid)
child_tidptr
clear_tsk_thread_flag(p,
TIF_SYSCALL_EMU
p->group_leader
p->tgid
p->real_parent
p->parent_exec_id
trace);
PIDTYPE_SID,
p->signal->flags
PIDTYPE_SID);
threadgroup_change_end(current);
free_pid(pid);
(p->mm)
++type)
ARCH_MIN_MMSTRUCT_ALIGN
unshare_flags)
vm.
(atomic_read(&current->mm->mm_users)
*new_cred
CLONE_FS;
bad_unshare_out;
new_fd
task_lock(current);
(new_fs)
(new_fd)
task_unlock(current);
(new_cred)
expose
*displaced
copy;
adjtime
kernel/time.c
Torsten
Duwe
Ulrich
Windl
'96
"A
Model
Precision
Timekeeping"
<linux/timekeeper_internal.h>
__ARCH_WANT_SYS_TIME
/etc/rc
warp
precisely
Bad,
(sys_tz.tz_minuteswest
adjust.tv_sec
!(MSEC_PER_SEC
1)/(HZ
!(USEC_PER_SEC
Truncate
current_kernel_time()
(gran
23:59:59
mon0,
day,
sec)
mon
hours
)*60
set_normalized_timespec
@sec:
tv_sec
asm()
include/linux/time.h
++sec;
--sec;
(!nsec)
div_s64_rem(nsec,
(unlikely(rem
ts.tv_sec--;
interpret
'infinite
timeout'
timeout:
jiffies_to_msecs(MAX_JIFFY_OFFSET))
wouldn't
(u
(NSEC_JIFFIE_SC
__timespec_to_jiffies(value->tv_sec,
value->tv_sec
div_u64_rem((u64)jiffies
value->tv_usec
clock_t
div_u64((u64)x
div_u64(x,
512);
year)
64.99
120,
(9ull
{m,u}secs_to_jiffies,
u64.
(5^9
2^9)
(1953125
18446744073.709551615
584
div_u64(n
years,
(>=
lhs,
rhs)
lhs.tv_sec
res.tv_sec
timer_stats
developed
Petrini
Instituto
Tecnologia
INdT
folks
effort.
timer_top.c
timer_flag;
nr_entries;
held:
*comm)
**head,
(match_entries(curr,
(likely(!timer_stats_active))
&per_cpu(tstats_lookup_lock,
raw_spin_lock_irqsave(lock,
(!timer_stats_active)
raw_spin_unlock_irqrestore(lock,
print_name_offset(struct
symname[KSYM_NAME_LEN];
ms;
mutex_lock(&show_mutex);
(timer_stats_active)
time_stop
"Timer
%ld.%03ld
s\n",
entry->count,
entry->comm);
events\n",
mutex_unlock(&show_mutex);
'0':
exclusion,
Bloatwatch
rcupdate
"done"
CB.
CBs.
.donetail
&rcu_sched_ctrlblk.rcucblist,
.curtail
RCU_TRACE(.name
&rcu_bh_ctrlblk.rcucblist,
qlen:
NORMAL,
CPUPRI_INVALID;
uncertainty
cpupri_find(struct
CPUPRI_NR_PRIORITIES);
Ideally,
go,
cpupri_set(struct
*currpri
do_mb
CPUPRI_INVALID))
cpupri_init(struct
CPUPRI_NR_PRIORITIES;
free_cpumask_var(cp->pri_to_cpu[i].mask);
cpupri_cleanup(struct
exit_mm(struct
group_dead
lockdep_tasklist_lock_is_held());
posix_cpu_timers_exit_group(tsk);
sig->gtime
sig->min_flt
tsk->min_flt;
sig->maj_flt
tsk->maj_flt;
sig->nvcsw
sig->nivcsw
sig->inblock
sig->oublock
sig->sum_sched_runtime
tsk->se.sum_exec_runtime;
clear_tsk_thread_flag(tsk,
leader's
zap_leader
p->group_leader;
leader->exit_state
ignored_task)
jobs,
3.2.2.2)
father
outside.
candidates.
list_for_each_entry(c,
task_unlock(c);
put_task_struct(c);
already..
father))
find_alive_thread(father);
panic("Attempted
father;
*reaper;
(reaper
EXIT_DEAD))
(do_notify_parent(p,
p->exit_signal))
list_add(&p->ptrace_entry,
things:
&dead);
thread_group_empty(tsk)
do_notify_parent(tsk,
(thread_group_leader(tsk))
EXIT_ZOMBIE;
release_task(p);
check_stack_usage(void)
lowest_to_date
lowest_to_date)
(unlikely(in_interrupt()))
reasons,
code);
validate_creds_for_do_exit(tsk);
PF_EXITPIDONE;
raw_spin_unlock_wait(&tsk->pi_lock);
tsk->mm);
hypervisor
(comp)
complete(comp);
error_code)
(signal_group_exit(sig))
sig->group_exit_code;
0xff)
(!eligible_pid(wo,
(infop)
read_lock(&tasklist_lock)
uninteresting.
(unlikely(wo->wo_flags
p->exit_code;
CLD_DUMPED
thread_group_leader(p))
maxrss;
tgstime;
c*
p->signal
thread_group_cputime_adjusted(p,
task_io_accounting_add(&psig->ioac,
%TASK_STOPPED
@ptrace:
unlock_sig;
(!exit_code)
live,
(!(p->signal->flags
SIGNAL_STOP_CONTINUED))
-ECHILD
security_task_wait(),
(unlikely(exit_state
(likely(!ptrace)
hides
reaping
complex.
WCONTINUED
(wo->wo_flags
wait_consider_task(wo,
__WNOTHREAD)
&wo->child_wait);
-ECHILD;
PIDTYPE_MAX)
upid,
wo;
(options
find_get_pid(upid);
wo.wo_type
wo.wo_pid
wo.wo_flags
options;
wo.wo_info
wo.wo_stat
wo.wo_rusage
do_wait(&wo);
capabilities.
kernel/timer.c
testing,
Morton,
clockids
hrtimer_cpu_base,
HRTIMER_BASE_MONOTONIC,
HRTIMER_BASE_REALTIME,
HRTIMER_BASE_BOOTTIME,
HRTIMER_BASE_TAI,
grained
mono
mono;
locking:
__run_timers/migrate_timers
timer->base))
get_nohz_timer_target(pinned);
base->index;
new_base)
lock_timer_base()
hrtimer_check_target(timer,
raw_spin_unlock(&new_base->cpu_base->lock);
dclc;
sft
dclc
res.tv64
CONFIG_DEBUG_OBJECTS_TIMERS
addr)->function;
debug_hrtimer_init(struct
debug_hrtimer_activate(struct
debug_hrtimer_deactivate(struct
debug_object_deactivate(timer,
__hrtimer_init(struct
debug_object_init_on_stack(timer,
__hrtimer_init(timer,
debug_init(struct
debug_activate(struct
debug_deactivate(struct
clock_was_set()
negative.
(expires_next.tv64
expires_next.tv64
expires_next;
hrtimer_is_hres_enabled(void)
hrtimer_hres_active(void)
hrtimer_force_reprogram(struct
skip_equal)
__hrtimer_get_next_event(cpu_base);
cpu_base->expires_next.tv64)
cpu_base->expires_next.tv64
T2
(cpu_base->hang_detected)
hrtimer_reprogram(struct
reevaluate
cpu_base->expires_next
hrtimer_init_hres(struct
base->hres_active
retrigger_next_event(void
hrtimer_switch_to_hres(void)
retrigger_next_event(NULL);
reprogramm
_all_
clock_was_set_delayed();
(timer->start_site)
memcpy(timer->start_comm,
timer_stats_update_stats(timer,
timer->start_pid,
timer->start_site,
timer->function,
@now:
orun
interval.tv64))
ktime_to_ns(interval);
orun;
ktime_add()
exact:
enqueue_hrtimer
&timer->node);
base->cpu_base->active_bases
(&timer->node
(!timerqueue_getnext(&base->active))
base->index);
CALLBACK
remove_hrtimer(timer,
tim
ktime_add_safe(tim,
CONFIG_TIME_LOW_RES
@tim:
(HRTIMER_MODE_ABS)
(HRTIMER_MODE_REL)
hrtimer_expires_remaining(timer);
mindelta
*cpu_base;
raw_cpu_ptr(&hrtimer_bases);
(clock_id
HRTIMER_MODE_ABS)
memset(timer->start_comm,
timer->function;
HRTIMER_NORESTART)
hrtimer_update_base(cpu_base);
cpu_base->in_hrtirq
cpu_base->clock_base
timerqueue_getnext(&base->active)))
expiration.
__run_hrtimer(timer,
Reprogramming
cpu_base->hang_detected
ns\n",
__hrtimer_peek_ahead_timers(void)
(td
td->evtdev)
hrtimer_peek_ahead_timers
passed.
__hrtimer_peek_ahead_timers();
jiffy,
(hrtimer_hres_active())
xtime_lock
gettime
wake_up_process(task);
!signal_pending(current));
rmt
ktime_to_timespec(rem);
(copy_to_user(rmtp,
&rmt,
sizeof(*rmtp)))
(do_nanosleep(&t,
restart->nanosleep.rmtp;
update_rmtp(&t.timer,
clockid)
hrtimer_set_expires_range_ns(&t.timer,
restart->nanosleep.clockid
restart->nanosleep.expires
*old_base,
old_base
*urn)
TIF_USER_RETURN_NOTIFY);
"console_cmdline.h"
"braille.h"
c->index,
rtn;
(console->flags
_name##_show,
uevent_helper[count-1]
CONFIG_PROFILING
"%lx
&kernel_attr_group);
kernel_config_data
extraction
MAGIC_SIZE
CONFIG_IKCONFIG_PROC
kernel_config_data_size);
S_IFREG
<linux/preempt.h>
b->head
&b->head;
re-initialize
init_srcu_struct_fields(sp);
sleep-RCU
->c[]
__srcu_read_lock()
increment,
__srcu_read_unlock()
srcu_readers_seq_idx(sp,
__srcu_read_lock().
srcu_readers_active_idx(),
suppose
synchronize_srcu_expedited().
microseconds,
SYNCHRONIZE_SRCU_EXP_TRYCOUNT
"func()"
srcu_read_lock(),
srcu_read_unlock()
rcu_batch_queue(&sp->batch_queue,
(!sp->running)
&sp->work,
srcu_advance_batches(struct
trycount);
srcu_reschedule(struct
srcu_advance_batches(sp,
rcu_batch_dequeue(&sp->batch_done);
srcu_reschedule(sp);
synchronize.
indexes.
__synchronize_srcu(sp,
(!try_check_zero(sp,
trycount))
advance,
invocation,
->batch_done.
rcu_batch_move(&sp->batch_done,
&sp->batch_check1);
(rcu_batch_empty(&sp->batch_done)
rcu_batch_empty(&sp->batch_check1)
rcu_batch_empty(&sp->batch_check0)
rcu_batch_empty(&sp->batch_queue))
PID_MAX_LIMIT;
kstrtouint(str,
t->nivcsw;
frozen.
t->pid,
print_tainted(),
trigger_all_cpu_backtrace();
can_cont;
max_count
batch_count
HUNG_TASK_BATCHING;
TASK_KILLABLE
NFS
sysctl_hung_task_timeout_secs;
trace.c
(entry->type)
TRACE_FN:
TRACE_STACK:
TRACE_GRAPH_ENT:
TRACE_GRAPH_RET:
constantly
trace_selftest_test_probe2_cnt
trace_selftest_test_probe3_cnt
len2;
reset_counts();
len2
func1_name,
func2_name,
len2,
ftrace_set_filter(&test_probe3,
(trace_selftest_test_global_cnt
DYN_FTRACE_TEST_NAME2();
func();
trace_selftest_ops(tr,
strlen(func_name);
(trace_selftest_recursion_cnt
(%d)*
trace_selftest_recursion_cnt);
trace_selftest_startup_dynamic_tracing(trace,
trace_selftest_regs_stat
register_ftrace_function(&test_regs_probe);
(!supported)
support*
(ftrace_filter_param)
PASS
msleep
harmlessly
ftrace_dump()
trace_graph_entry(trace);
set_graph_array(tr);
register_ftrace_graph(&trace_graph_return,
recovered
preemptable,
(preempt_count())
"can
CONFIG_SCHED_TRACER
complete(&x->is_ready);
init_completion(&data.is_ready);
wait_for_completion(&data.is_ready);
CONFIG_CONTEXT_SWITCH_TRACER
kernel/power/main.c
pm_notifier_call_chain(unsigned
pm_async_enabled
"core",
"platform",
TEST_FIRST;
TEST_MAX;
level++)
pm_tests[level]);
*(s-1)
"\t\t\t%-s\n",
suspend_step_name(
show()
store()
pm_print_times_enabled
pm_print_times_init(void)
"mem",
"standby",
"freeze"
strings,
PM_SUSPEND_ON;
pm_autosleep_lock();
(pm_autosleep_state()
decode_state(buf,
pm_autosleep_unlock();
aborted.
preparations
pm_states[state]
"error");
PM_SUSPEND_ON
strcmp(buf,
pm_show_wakelocks(buf,
Correct
freeze_timeout_msecs
g[]
attr_group
pm_wq
sysfs_create_group(power_kobj,
&attr_group);
per-user
.ns.inum
.ns.ops
ID's
UIDHASH_BITS)
free_uid()
*hashent)
*up)
spin_unlock_irqrestore(&uidhash_lock,
kmem_cache_free(uid_cachep,
(!up)
++n)
mod_verify_sig(const
System-call
auditsc.c
Durham,
North
Carolina.
Rickard
(Rik)
Faith
<faith@redhat.com>
Security
<net/sock.h>
audit_enabled;
audit_default;
audit_failure
audit_nlk_portid
audit_rate_limit
audit_rate_limit;
(60
AUDIT_BACKLOG_WAIT_TIME;
audit_sig_pid
audit_log_start
socket.
when/if
audit_features
nlmsg_hdr(ab->skb);
*message)
message);
last_check
DEFINE_SPINLOCK(lock);
spin_lock_irqsave(&lock,
spin_unlock_irqrestore(&lock,
stating
print;
*function_name,
deny
allow_changes);
limit);
audit_ever_enabled
audit_log_lost()
daemon,
nlmsg_hdr(skb);
audit_hold_skb(skb);
consume_skb(skb);
multicast
*sock
aunet->nlsk;
mods
unicast
breakage.
Fixing
kauditd
skb_dequeue(&audit_skb_hold_queue);
kauditd_send_skb(skb);
audit_backlog_limit)
wake_up(&audit_backlog_wait);
put_net(net);
kfree(dest);
(!nlh)
out_kfree_skb;
payload,
out_kfree_skb:
kfree(reply);
@request_skb:
replying
reply)
id.
*request_skb,
NETLINK_CB(request_skb).portid;
sock_net(NETLINK_CB(request_skb).sk);
get_net(net);
reply,
msg_type)
ECONNREFUSED
configure
PAM
&init_user_ns)
(msg_type)
AUDIT_GET:
AUDIT_SET:
AUDIT_GET_FEATURE:
AUDIT_SET_FEATURE:
AUDIT_LIST_RULES:
AUDIT_SIGNAL_INFO:
AUDIT_TTY_GET:
AUDIT_TTY_SET:
AUDIT_TRIM:
AUDIT_MAKE_EQUIV:
(!netlink_capable(skb,
AUDIT_USER:
AUDIT_FIRST_USER_MSG
AUDIT_LAST_USER_MSG:
AUDIT_FIRST_USER_MSG2
AUDIT_LAST_USER_MSG2:
**ab,
(!audit_enabled
AUDIT_USER_AVC)
"pid=%d
uid=%u",
AUDIT_LAST_FEATURE;
(!(feature
uaf->mask))
old_feature
new_feature
uaf->features
new_lock
(uaf->lock
af.lock)
old_lock
(new_feature
audit_log_feature_change(i,
invalid,
kauditd_task
audit_status
audit_backlog_wait_time_master;
&s,
memcpy(&s,
sizeof(s),
nlmsg_len(nlh)));
(msg_type
AUDIT_USER_TTY)
nlmsg_len(nlh);
msglen
&msglen,
kfree(old);
audit_tty_status
tsk->signal->audit_tty;
s.log_passwd
tsk->signal->audit_tty_log_passwd;
nlmsg_len
*net)
(audit_initialized
AUDIT_INODE_BUCKETS;
(after
audit_backlog_limit_arg;
kfree_skb(ab->skb);
spin_lock_irqsave(&audit_freelist_lock,
spin_unlock_irqrestore(&audit_freelist_lock,
ab;
audit_buffer_free(ab);
&serial);
*serial)
CURRENT_TIME;
*serial
audit_serial();
(audit_backlog_limit
skb_queue_len(&audit_skb_queue)
sleep_time;
auditable
timeout_start
gfp_mask
(sleep_time
newtail
vsnprintf(skb_tail_pointer(skb),
audit_log_vformat(ab,
(new_len
new_len);
skb_tail_pointer(skb);
enclosed
quotes
'"';
audit_log_n_hex(ab,
*pathname;
pathname,
audit_get_loginuid(current));
cpu_vfs_cap_data
(!dentry)
VFS_CAP_FLAGS_EFFECTIVE);
VFS_CAP_REVISION_MASK)
VFS_CAP_REVISION_SHIFT;
audit_names.
AUDIT_PATH
*call_panic)
name=",
name=");
ouid=%u
osid=%u",
(call_panic)
*call_panic
security_task_getsecid(current,
(!sid)
out_null;
get_mm_exe_file(mm);
pid=%d
uid=%u
audit_log_d_path_exe(ab,
GFP_NOFS);
AUDIT_TYPE_NORMAL;
nlh->nlmsg_len
Log
SELinux
LynuxWorks,
Igor
Manyilov,
Huey
(GPL).
"mutex-debug.h"
SMP_DEBUG_LOCKS_WARN_ON(!spin_is_locked(&lock->wait_lock));
DEBUG_LOCKS_WARN_ON(list_empty(&waiter->list));
ti->task->blocked_on
__mutex_slowpath_needs_to_unlock()
*idata)
*not*
cpumask_clear(desc->pending_mask);
storm.
(!masked)
variables:
IRQ_BITMAP_BITS
Bits
IRQS_REPLAY
replayed
IRQS_WAITING
IRQS_PENDING
irq_lock_sparse(void)
irq_unlock_sparse(void)
__irq_get_desc_lock(unsigned
bus,
__irq_put_desc_unlock(struct
__irq_get_desc_lock(irq,
__irq_put_desc_unlock(desc,
Bojan
Smojver
<bojan@rexursive.com>
<linux/genhd.h>
MAP_PAGE_ENTRIES
sizeof(sector_t)
low_free_pages()
file-alike
crc32;
**new
(*new)
swsusp_extent,
&((*new)->rb_left);
&((*new)->rb_right);
swap_offset;
&swsusp_extents);
alloc_swapdev_block(int
swap_free(swp_entry(swap,
free_all_swap_pages(int
root_swap
*hib_resume_bdev;
Saving
memcpy(swsusp_header->sig,
SF_CRC32_MODE)
set_blocksize(hib_resume_bdev,
(src)
copy_page(src,
hib_wait_on_bio_chain(bio_chain);
-a.\n");
handle->cur_swap
release_swap_writer(handle);
swsusp_close(FMODE_WRITE);
MAP_PAGE_ENTRIES)
write_page(handle->cur,
handle->cur_swap,
free_all_swap_pages(root_swap);
pages/bytes
LZO_UNC_SIZE
buffering.
nr_to_write)
nr_to_write);
nr_to_write
snapshot_read_next(snapshot);
swap_write_page(handle,
%3d%%\n",
nr_to_write,
"Wrote");
unc_len;
cmp_len;
unc[LZO_UNC_SIZE];
cmp[LZO_CMP_SIZE];
LZO.
@handle:
@snapshot:
run_threads,
clamp_val(nr_threads,
LZO_THREADS);
__GFP_HIGH);
page\n");
vmalloc(sizeof(*data)
nr_threads);
data\n");
memset(&data[thr],
kmalloc(sizeof(*crc),
crc\n");
memset(crc,
crc_data,
init_waitqueue_head(&data[thr].go);
init_waitqueue_head(&data[thr].done);
&data[thr],
thr);
(IS_ERR(data[thr].thr))
threads\n");
init_waitqueue_head(&crc->go);
init_waitqueue_head(&crc->done);
handle->crc32
crc->crc32
&handle->crc32;
crc->unc[thr]
data[thr].unc;
crc->unc_len[thr]
&data[thr].unc_len;
kthread_run(crc32_threadfn,
"image_crc32");
(IS_ERR(crc->thr))
thread\n");
thread(s)
nr_threads,
LZO_UNC_SIZE;
"%3d%%\n",
atomic_set(&data[thr].ready,
wake_up(&data[thr].go);
thr;
(run_threads
wait_event(data[thr].done,
atomic_read(&data[thr].stop));
atomic_set(&data[thr].stop,
data[thr].ret;
(unlikely(!data[thr].cmp_len
*(size_t
out_clean:
(crc->thr)
kthread_stop(crc->thr);
kfree(crc);
(data[thr].thr)
kthread_stop(data[thr].thr);
free_swap
swsusp_write(unsigned
*header;
memset(&snapshot,
*)data_of(snapshot);
long)handle->maps->map);
handle->maps;
handle->maps->next;
*flags_p)
(!handle->maps)
hib_bio_read_page(offset,
handle->maps->map;
nr_to_read)
nr_to_read);
nr_to_read
swap_read_page(handle,
snapshot_write_finalize(snapshot);
(!snapshot_image_loaded(snapshot))
nr_to_read,
"Read");
LZO_MAX_RD_PAGES);
(low_free_pages()
snapshot_get_image_size())
ring_size)
asked;
(eof)
(crc->run_threads)
swsusp_read(unsigned
header->pages
Error
Exception
gdb_serial_stub(struct
gdbstub_msg_write(const
gdbstub_state(struct
kdb_parse(const
Alarm
<linux/platform_device.h>
freezer_delta;
CONFIG_RTC_CLASS
*alarmtimer_get_rtcdev(void)
spin_lock_irqsave(&rtcdev_lock,
spin_unlock_irqrestore(&rtcdev_lock,
class_interface
rtcdev
alarmtimer_rtc_timer_init(void)
alarmtimer_rtc_interface_setup(void)
alarmtimer_rtc_interface_remove(void)
timerqueue_del(&base->timerqueue,
fired.
alarm,
alarmtimer_dequeue(base,
hrtimer_set_expires(&alarm->timer,
soonest
alarmtimer_suspend(struct
tm;
spin_lock_irqsave(&freezer_delta_lock,
freezer_delta
spin_unlock_irqrestore(&freezer_delta_lock,
ALARM_NUMTYPE;
__pm_wakeup_event(ws,
&tm);
min);
&alarm_bases[type];
(*function)(struct
alarm_start(alarm,
alarm_forward(alarm,
(clockid
@new_timer:
(!capable(CAP_WAKE_ALARM))
@timr:
(alarm_try_to_cancel(&timr->it.alarm.alarmtimer)
~TIMER_ABSTIME)
necessary)
alarm_timer_nsleep
alarm->data
nsleep
(alarm->data
@rmtp:
alarm;
alarm_init(&alarm,
alarmtimer_nsleep_wakeup);
(alarmtimer_do_nsleep(&alarm,
exp))
(freezing(current))
alarmtimer_freezerset(exp,
update_rmtp(exp,
&alarm_clock);
ws
*rcp);
rcp->donetail
rcp->curtail
&rcp->rcucblist;
Love
2001,
TVN_SIZE
TVR_SIZE
TVR_BITS)
MAX_TVAL
tvec_root
&boot_tvec_bases;
int)(unsigned
__round_jiffies()
skewed
__round_jiffies_relative()
round_jiffies_common(j
j0,
j0;
round_jiffies()
round_jiffies_relative()
slack.
for,
timer->slack
->timer_jiffies
timer->expires;
__internal_add_timer(base,
wheel.
timer_stats_account_timer(struct
del_timer_sync(timer);
TIMER_ENTRY_STATIC)
setup_timer(timer,
stub_timer,
debug_timer_init(struct
debug_timer_activate(struct
debug_timer_deactivate(struct
debug_timer_assert_init(struct
do_init_timer(struct
do_init_timer(timer,
clear_pending)
base->active_timers--;
base->all_timers--;
(timer->expires
timer_stats_timer_set_start_info(timer);
debug_activate(timer,
internal_add_timer(base,
unserialized
expires_limit;
activated)
mod_timer(timer,
del_timer(timer);
networking
->expires
timer->expires);
Double
debug_assert_init(timer);
timer_stats_timer_clear_start_info(timer);
del
(base->running_timer
restarting
(*fn)(unsigned
survive
spin_lock_irq(&base->lock);
(!cascade(base,
list_first_entry(head,
call_timer_fn(timer,
list_for_each_entry(nte,
(tbase_get_deferrable(nte->base))
nte->expires;
bucket(s)?
(found)
(hr_delta.tv64
__this_cpu_read(tvec_bases);
TVR_SIZE;
TVN_SIZE;
timer_register_cpu_notifier(void)
interruptions
@msecs:
msecs_to_jiffies(msecs)
@min:
@max:
spinlock/rwlock
__lock_function
(likely(do_raw_##op##_trylock(lock)))
(!(lock)->break_lock)
(!raw_##op##_can_lock(lock)
(lock)->break_lock)\
arch_##op##_relax(&lock->raw_lock);
_raw_##op##_lock_irqsave(lock);
rwlock);
__local_bh_disable_ip(_RET_IP_,
cpudl_item
loader
December
<rusty@rustcorp.com.au>
usermodehelper_bset
usermodehelper_inheritable
(!argv)
call_usermodehelper_exec(info,
module_name[MODULE_NAME_LEN];
async.
atomic_dec(&kmod_concurrent);
call_usermodehelper_exec().
UMH_NO_WAIT.
keventd,
cap_intersect(usermodehelper_bset,
cap_intersect(usermodehelper_inheritable,
wait_for_helper()
umh_complete
kernel_thread(____call_usermodehelper,
wait4()
call_usermodehelper_exec()
umhelper_sem
usermodehelper_disable()
prepare_to_wait(&usermodehelper_disabled_waitq,
(!usermodehelper_disabled)
finish_wait(&usermodehelper_disabled_waitq,
@depth:
down_write(&umhelper_sem);
up_write(&umhelper_sem);
@argv:
@envp:
gfp
**envp,
UMH_NO_WAIT
exec'ed.
umh_complete()
_KERNEL_CAPABILITY_U32S;
CAP_BSET)
cap_array[i]
CAP_PI)
ulongs
new_cap);
_KERNEL_CAPABILITY_U32S
proc_cap_handler,
swsusp_resume_device;
swsusp_resume_block;
freezer_test_done;
platform_hibernation_ops
transitions.
hibernation_ops
HIBERNATION_PLATFORM;
(hibernation_mode
HIBERNATION_PLATFORM)
entering_platform_hibernation;
debug:
hibernation_test(int
hibernation_ops->end();
configured,
hibernation_ops->finish();
hibernation_ops->recover();
elapsed_centisecs64;
centisecs;
kps
"late"
dpm_suspend_end(PMSG_FREEZE);
hibernation\n");
Power_up;
save_processor_state();
PM_EVENT_HIBERNATE,
restore_processor_state();
events_check_enabled
Power_up:
PMSG_RECOVER
PMSG_THAW)
Quiesce
hibernation_snapshot(int
freeze_kernel_threads();
Cleanup;
!in_suspend)
Cleanup:
restore_highmem();
BUG_ON(!error);
hibernation_restore(int
undefined,
entering_platform_hibernation
dpm_resume_start(PMSG_RESTORE);
dpm_resume_end(PMSG_RESTORE);
hibernation,
power_down();
signature.
printk(KERN_CRIT
Carry
Syncing
initcall,
name_to_dev_t(resume_file);
(!swsusp_resume_device)
discovery
MAJOR(swsusp_resume_device),
MINOR(swsusp_resume_device));
Close_Finish;
Preparing
/sys/power/disk
'platform'
'shutdown'
'reboot'
hibernation_mode)
bracket.
HIBERNATION_FIRST;
HIBERNATION_MAX;
hibernation_modes[i]);
&size)
(noresume)
&offset)
nohibernate
improvements.
__mutex_lock_slowpath(atomic_t
*lock_count);
ww_mutex_lock
mutex_unlock
recheck.
ww_mutex_lock_acquired(lock,
&lock->base.wait_list,
debug_mutex_wake_waiter(&lock->base,
cur);
wake_up_process(cur->task);
reliable.
(lock->owner
READ_ONCE(lock->owner);
mutex_optimistic_spin(struct
(use_ww_ctx
ww_ctx->acquired
*ww;
ww
unlocked.
lock_acquired(&lock->dep_map,
(use_ww_ctx)
osq_unlock(&lock->osq);
__mutex_unlock_slowpath(atomic_t
Unlock
Unlocking
up().
0->1
__mutex_unlock_slowpath);
*ww
hold_ctx))
-EALREADY;
(ctx->stamp
hold_ctx->stamp
ctx->contending_lock
(atomic_xchg(&lock->count,
task_thread_info(task));
(likely(list_empty(&lock->wait_list)))
debug_mutex_free_waiter(&waiter);
mutex_release(&lock->dep_map,
__ww_mutex_lock(struct
&ctx->dep_map,
ctx->acquired
ww_mutex_deadlock_injection(lock,
__ww_mutex_lock_interruptible(struct
duties
__mutex_lock_killable_slowpath(struct
__mutex_lock_interruptible_slowpath(struct
arrives
__mutex_fastpath_lock_retval(&lock->count);
__mutex_fastpath_lock_retval(&lock->base.count);
ww_mutex_set_context_fastpath(lock,
mutex_set_owner(&lock->base);
KGDB_MAX_THREAD_QUERY
gdbstub_read_wait(void)
gdbstub_use_prev_in_buf
buffer[count]
(checksum
dbg_io_ops->write_char('-');
dbg_io_ops->write_char('+');
dbg_io_ops->write_char('$');
dbg_io_ops->write_char(ch);
dbg_io_ops->write_char('#');
dbg_io_ops->write_char(hex_asc_hi(checksum));
dbg_io_ops->write_char(hex_asc_lo(checksum));
packet.
bufptr
wcount
Pack
tmp_raw
*tmp_raw
probe_kernel_write(mem,
hex_val;
(*ptr)++;
c[size]
0x7d)
*gdb_regs,
*)gdb_regs;
(kgdb_hex2long(&ptr,
&addr)
&length)
(binary)
kgdb_hex2mem(ptr,
*pkt,
-error;
hex_asc[(error
10)];
remapped
lzero
pkt
hex_byte_pack(pkt,
*id,
PID,
'?'
remcom_out_buffer[0]
ks->signo);
Pull
gdb_get_regs_helper(ks);
NUMREGBYTES);
regster
&regnum);
gdb_regs,
honor
"Executing
reboot\n");
thref[BUF_THREAD_ID_SIZE];
(remcom_in_buffer[1])
(memcmp(remcom_in_buffer
ks->thr_query
17;
'H'
(!thread
ks->kgdb_usethreadid
'T'
'z'
'1')
Unsupported
Unsupported.
(*(ptr++)
remcom_in_buffer[2]
'9')
memset(remcom_out_buffer,
sizeof(remcom_out_buffer));
'M':
XX
hex)
gdb_cmd_detachkill(ks);
'Z':
Escape
kgdb_arch_handle_exception(ks->ex_vector,
ks->signo,
remcom_in_buffer,
strcpy(remcom_in_buffer,
'$':
buffer[0]
flushed,
bootloader
lock->raw_lock
%08x,
%s/%d,
spin_dump(lock,
(unlikely(cond))
SPIN_BUG_ON(lock->magic
SPINLOCK_MAGIC,
SPIN_BUG_ON(lock->owner
"recursion");
SPIN_BUG_ON(lock->owner_cpu
"cpu
recursion");
owner");
debug_spin_lock_after(lock);
(print_once)
"%s/%d,
RWLOCK_BUG_ON(!ret,
RWLOCK_BUG_ON(lock->owner
RWLOCK_BUG_ON(lock->owner_cpu
debug_write_lock_after(lock);
2008-2009
Mostly
kill_ftrace_graph;
ftrace_graph_stop()
harm.
TRACE_GRAPH_PRINT_OVERRUN)
TRACE_GRAPH_PRINT_OVERHEAD)
print_graph_duration(unsigned
info.*/
current->curr_ret_stack
current->curr_ret_stack;
be.
%ps
trace->calltime
&ret,
max_depth
graph_array;
(trace->depth
graph_array
TRACE_GRAPH_PROCINFO_LENGTH
center
*last_pid;
cpu)->last_pid);
------------------------------------------
data->failed)
&data->ent;
(ring_iter)
(next->ent.type
TRACE_GRAPH_RET)
usecs_rem
print_graph_abs_time(iter->ts,
Proc
print_graph_lat_fmt(s,
msecs
&entry->graph_ent;
Comments
(call->depth
cpu_data->enter_funcs[call->depth]
print_graph_duration(duration,
*)call->func);
FLAGS_FILL_FULL);
TRACE_TYPE_NO_CONSUME;
ent->pid,
print_graph_irq(iter,
funcgraph-interrupts
*depth_irq;
irqs,
(!data))
depth_irq
call->func,
field);
data->failed
func_match
(iter->ent->type)
tracer_flags.val);
"#%.*s
seq_putc(s,
"||||");
gfpflags);
*ignore
graph_trace_open,
.pipe_open
graph_trace_close,
buf[15];
snprintf(NULL,
Keniston
powerpc
SLOT_DIRTY
.mutex
alloc_insn_page,
free_insn_page,
.insn_size
.nr_garbage
collect_garbage_slots(struct
*kip;
mutex_lock(&c->mutex);
list_for_each_entry(kip,
(kip->nused
(kip->slot_used[i]
SLOT_USED;
kip->insns
+/-
kfree(kip);
kip->ngarbage
mutex_unlock(&c->mutex);
*kip,
kip->slot_used[idx]
no-one
collect_one_slot(kip,
kprobe_optinsn_slots
__this_cpu_write(kprobe_instance,
aggr_pre_handler(struct
aggr_pre_handler;
(kp->pre_handler
free_aggr_kprobe(struct
kfree(op);
(!kprobe_aggrprobe(p))
Optimization
kprobe_optimizer(struct
OPTIMIZE_DELAY
(replace
jump)
!kprobes_allow_optimization
get_online_cpus()
&freeing_list);
free_list
list_for_each_entry_safe(op,
&freeing_list,
(kprobe_disabled(&op->kp))
arch_disarm_kprobe(&op->kp);
reclaimed.
(reclaiming
hlist_del_rcu(&op->kp.hlist);
optprobe
(!list_empty(&optimizing_list)
!list_empty(&unoptimizing_list))
p->post_handler)
unoptimizing.
force_unoptimize_kprobe(op);
reuse_unused_kprobe(struct
container_of(ap,
(remove
arch_prepare_optimized_kprobe(op,
*alloc_aggr_kprobe(struct
init_aggr_kprobe(struct
*ap;
optimization,
jump_label_text_reserved()
(!ap)
init_aggr_kprobe(ap,
kicks
optimize_kprobe(p);
wait_for_kprobe_optimizer();
*_p;
_p
get_optimized_kprobe((unsigned
CONFIG_KPROBES_ON_FTRACE
p->addr
ftrace_set_filter_ip(&kprobe_ftrace_ops,
(kprobe_ftrace_enabled
!CONFIG_KPROBES_ON_FTRACE
(unlikely(kprobe_ftrace(kp)))
stop_machine(),
text_mutex.
__this_cpu_read(kprobe_instance);
Walks
inst
INIT_HLIST_NODE(&ri->hlist);
__acquires(hlist_lock)
hash_ptr(tsk,
*hlist_lock;
hlist_lock
raw_spin_lock_irqsave(hlist_lock,
*hlist_lock
__releases(hlist_lock)
raw_spin_unlock_irqrestore(hlist_lock,
(hash
ri->rp
aggr_break_handler;
list_add_rcu(&p->list,
aggr_post_handler;
orig_p;
aggr_kprobe,
&kprobe_blacklist,
ent->start_addr
*)addr)
*list_p;
get_kprobe(p->addr);
list_for_each_entry_rcu(list_p,
*probed_mod
p->addr);
(within_module_init((unsigned
(IS_ERR(addr))
PTR_ERR(addr);
(!kprobes_all_disarmed
(ap
disarmed;
list_del(&p->list);
**kps,
unregister_kprobes(kps,
**jps,
jp
unregister_jprobes(jps,
*jp)
pre_handler_kretprobe(struct
rp->nmissed++;
raw_spin_lock_irqsave(&rp->lock,
NOKPROBE_SYMBOL(pre_handler_kretprobe);
register_kretprobe(struct
(kretprobe_blacklist_size)
kretprobe_blacklist[i].name
rp->maxactive
EXPORT_SYMBOL_GPL(register_kretprobe);
register_kretprobes(struct
unregister_kretprobes(rps,
EXPORT_SYMBOL_GPL(register_kretprobes);
unregister_kretprobe(struct
EXPORT_SYMBOL_GPL(unregister_kretprobe);
unregister_kretprobes(struct
EXPORT_SYMBOL_GPL(unregister_kretprobes);
KPROBE_FLAG_GONE;
kp->flags
&size,
.init.text
Hence,
(p->pre_handler
"%p
kprobe_type,
KPROBE_TABLE_SIZE)
report_probe(pi,
kp,
kprobe_seq_stop,
*user_buf,
'0';
buf[1]
stat_node,
(!stat)
insert_stat(root,
stat,
ts->stat_cmp);
rb_first(&session->stat_root);
rb_next(node);
reset_stat_session(session);
session;
session,
&all_stat_sessions,
session_list)
(node->ts
"ntp_internal.h"
(tk->tkr_mono.xtime_nsec
tk->xtime_sec++;
tk->tkr_mono.shift);
ts->tv_sec;
(u64)ts->tv_nsec
tk_normalize_xtime(tk);
tk->offs_tai
ktime_add(tk->offs_real,
timekeeping_check_update(struct
(%lld)
clock's
WARNING_FREQ)
observed,
possible.\n");
fine.\n");
timekeeping_underflow_seen
timekeeping_overflow_seen
timekeeping_get_delta(struct
tkr->clock->max_cycles;
update_wall_time
@tk:
*clock)
old_clock
tk->tkr_mono.clock
clock->read;
clock->mask;
tk->cycle_interval
tk->xtime_interval;
clocks,
tk->tkr_mono.mult
Timekeeper
requires,
get_arch_timeoffset()
does:
tkf->seq++;
tkr);
consistent.
*tkf)
raw_write_seqcount_latch(&tkf->seq);
sizeof(*base));
observation
tkr
cycles_at_suspend;
timekeeper's
memcpy(&tkr_dummy,
sizeof(tkr_dummy));
tkr_dummy.read
dummy_clock_read;
update_fast_timekeeper(&tkr_dummy,
&tk_fast_mono);
&tk_fast_raw);
CONFIG_GENERIC_TIME_VSYSCALL_OLD
xt
timespec64_to_timespec(tk_xtime(tk));
remainder;
((1ULL
tk->tkr_mono.shift)
pvclock
timedata
update_pvclock_gtod(tk,
(xtime_sec
wtm_sec)
1e9
wtm_nsec
now();
tk->ktime_sec
ntp_clear();
explicitly.
*clock
timespec64_add_ns(ts,
(unlikely(timekeeping_suspended))
tk_offsets
offsets[offs];
tconv;
tk->wall_to_monotonic;
preserved.
@tv:
Users
getnstimeofday64(&now);
now.tv_sec;
tk_set_xtime(tk,
subtracts
ts64
tk_xtime_add(tk,
TAI
tk->tai_offset;
tai_offset)
tk->tai_offset
Accumulates
tk_setup_internals(tk,
module_put(old->owner);
ts64;
Reads
tv_sec=0
tv_nsec=0
unsupported.
*ts64)
*ts64
timespec_to_timespec64(ts);
timekeeping_resume()
sleeptime
sleeptime_injected;
persistent_clock_exists;
timekeeping_init
value!\n"
CMOS/BIOS
settings.\n");
now.tv_sec
boot.tv_nsec
clocksource_default_clock();
tk->raw_time.tv_sec
tk->raw_time.tv_nsec
TK_MIRROR);
*delta)
preference
rtc_resume(),
rtc_suspend(),
__timekeeping_inject_sleeptime(tk,
timekeeping_resume
OS
ULLONG_MAX;
ts_delta
timekeeping_suspend_time);
timekeeping_suspended
persistent_clock
delta_delta
old_delta
(mult
cycles.
adj_2)
So:
xinterval
(tick_error
(tk->ntp_error
11%%
accumulates
accumulation
unconsumed
accumulate_nsecs_to_secs(tk);
cycle_intervals
doubled
trade
@offs_real:
@offs_boot:
boottime
@offs_tai:
*offs_tai)
tk->offs_real;
tk->offs_boot;
tk->offs_tai;
Accessor
ADJ_SETOFFSET)
delta.tv_nsec
(tai
orig_tai)
*phase_ts,
*raw_ts)
advances
rb_page
RUN_TIME
read_start;
producer_nice
consumer_nice
"nice
consumer");
"fifo
ring_buffer_consume(buffer,
EVENT_FOUND;
ring_buffer_alloc_read_page(buffer,
ring_buffer_read_page(buffer,
rpage
(!event->time_delta)
bpage);
read_events
!kill_test);
reader_finish
overruns;
hammer\n");
end_time
runs.
cond_resched
completions
init_completion(&read_start);
(consumer_fifo
nice:
consumer_nice);
Producer
producer_nice);
trace_printk("Read:
entries);
hit);
(long)time;
millisec:
trace_printk("%ld
avg);
wait_to_die();
meg
kthread_stop(consumer);
overflowuid
overflowgid
DEFAULT_FS_OVERFLOWUID;
no_nice;
niceval);
PRIO_USER
PRIO_PROCESS)
PRIO_PROCESS:
find_task_by_vpid(who);
PRIO_PGRP:
find_vpid(who);
do_each_pid_thread(pgrp,
while_each_pid_thread(pgrp,
PRIO_USER:
make_kuid(cred->user_ns,
who);
cred->user;
(!who)
cred->uid;
find_user(uid);
(uid_eq(task_uid(p),
find_user()
Unprivileged
(BSD-style)
privileges,
BSD.
setgid()
IDs.
krgid,
krgid
rgid);
kegid
egid);
((rgid
!gid_valid(krgid))
((egid
!gid_valid(kegid))
(gid_eq(old->gid,
krgid)
gid_eq(old->egid,
krgid;
SysV
SAVED_IDS
(ns_capable(old->user_ns,
(gid_eq(kgid,
new_user
NPROC
setuid
setreuid()
kruid,
kruid
ruid);
keuid
euid);
((ruid
!uid_valid(kruid))
((euid
!uid_valid(keuid))
kruid;
(!uid_eq(old->uid,
kruid)
!uid_eq(old->euid,
!ns_capable(old->user_ns,
(!uid_valid(kuid))
ksuid;
(!ns_capable(old->user_ns,
(suid
ksgid;
(sgid
cred->egid);
change_okay;
change_okay:
from_kgid_munged(current_user_ns(),
tgstime,
(pgid
-DaveM
task_session(group_leader))
pid_vnr(sid);
change_pid(curr,
override_architecture(name)
*rest
utsname(),
(!errno
tmp[__NEW_UTS_LEN];
(!ns_capable(current->nsproxy->uts_ns->user_ns,
__NEW_UTS_LEN)
(!copy_from_user(tmp,
copy_to_user(rlim,
RLIM_NLIMITS)
0x7FFFFFFF)
0x7FFFFFFF;
rlim64
rlim64->rlim_cur
rlim64->rlim_max
rlim->rlim_cur
*rlim;
expiry.
cheat
RLIMIT_CPU.
new_rlim
__task_cred(task);
(uid_eq(cred->uid,
tcred->euid)
tcred->egid)
tcred->sgid)
tcred->gid))
new_rlim,
old_rlim)
(Which
additions
non-current
__exit_signal
p->signal->maxrss;
RUSAGE_CHILDREN)
get_task_mm(p);
RUSAGE_SELF
RUSAGE_CHILDREN
ru);
mm->exe_file
/proc/pid/exe
exit_err:
prctl_mm_map
mmap_max_addr
long)val
mmap_min_addr)
(u32)-1)
opt,
user_auxv[AT_VECTOR_SIZE];
BUILD_BUG_ON(sizeof(user_auxv)
sizeof(mm->saved_auxv));
PR_SET_MM_MAP_SIZE)
(prctl_map.auxv_size)
sizeof(user_auxv));
(copy_from_user(user_auxv,
AT_NULL
AT_NULL;
prctl_set_mm_exe_file(mm,
unmapped
mm->start_code
mm->end_code
mm->start_data
mm->end_data
mm->start_brk
mm->brk
mm->start_stack
mm->arg_start
mm->arg_end
mm->env_start
mm->env_end
memcpy(mm->saved_auxv,
user_auxv,
PR_SET_MM_MAP
arg4);
mm->end_data)
mm->end_data,
mm->start_data))
prctl_get_tid_address(struct
*me,
**tid_addr)
arg2,
arg5);
-ENOSYS)
arg2;
*)arg2);
arg2
*)arg2,
current->timer_slack_ns
(arg2)
me->mm->def_flags
ram
2.2.x
(mem_total
bitcount
mem_unit
bitcount++;
info->mem_unit
compat_sysinfo
uptime;
((cnts
smp_load_acquire((u32
*)&lock->cnts);
cnts;
rspin_until_writer_unlock(lock,
cnts);
arch_spin_lock(&lock->lock);
arch_spin_unlock(&lock->lock);
atomic_read(&lock->cnts);
_QW_WAITING)
nohz)
one-shot
(!dev)
device\n");
@work->func()
task_work_run()
ACCESS_ONCE(task->task_works);
(cmpxchg(&task->task_works,
work->next;
(work);
_KDBPRIVATE_H
0x0002
0x0004
0x0008
0x0010
0x0020
(kdb_flags
KDB_DEBUG_FLAG_SHIFT))
KDB_PLATFORM_ENV
"0x%08lx"
kdb_elfw_addr_fmt
"0x%x"
kdb_elfw_addr_fmt0
kdb_f_count_fmt
"0x%016lx"
kallsyms_symbol_next(char
kallsyms_symbol_complete(char
kdb_getarea_size(void
kdb_putarea_size(unsigned
sizeof((x)))
kdb_getphysword(unsigned
kdb_getword(unsigned
kdb_putword(unsigned
kdbgetularg(const
kdbgetu64arg(const
*kdbgetenv(const
kdbgetsymval(const
kdbnearsym(unsigned
*kdb_strdup(const
kdb_symbol_print(unsigned
kdb_print_state(const
kdb_state;
re-entry
pager
((void)(kdb_state
kdb_cmdflags_t
kdb_grepping_flag;
kdb_grep_leading;
kdb_grep_trailing;
kdb_task_state_string(const
kdb_task_state(const
kdb_ps1(const
kdb_print_nameval(const
*kdb_getstr(char
kdb_symbol_print
(in_interrupt()
*debug_kmalloc(size_t
debug_kfree(void
kdb_set_current_task(struct
*kdb_current_task;
.irq_startup
noop_ret,
.irq_shutdown
.irq_enable
.irq_disable
.irq_ack
(perf_paranoid_tracepoint_raw()
callchains
leak,
(!total_ref_count)
free_percpu(perf_trace_buf[i]);
free_percpu(tp_event->perf_events);
tracepoint_synchronize_unregister();
module_put(tp_event->mod);
perf_trace_event_unreg(p_event);
*p_event,
&event->ftrace_ops;
<linux/uio.h>
CONSOLE_LOGLEVEL_DEFAULT,
console_sem.
mutex_acquire(&console_lock_dep_map,
_RET_IP_);\
userspace-injected
commonly
ifindex
69
65
44
56
49
3d
62
key/value
newline.
notation.
LOG_NOCONS
LOG_PREFIX
LOG_CONT
logbuf_lock
console_unlock()
/proc/kmsg
console_seq;
console_idx;
console_prev;
PREFIX_MAX
LOG_ALIGN
log_buf_len;
*)msg
printk_log);
*log_from_idx(u32
marker.
(!msg->len)
*)log_buf;
log_next(u32
next_idx
empty)
(log_next_idx
log_next_idx,
(logbuf_has_space(msg_size,
text_len,
*pad_len)
*trunc_msg_len,
pad_len);
&pad_len);
(log_make_free_space(size))
log_buf_len)
log_next_idx
msg->text_len
dict_len);
msg->flags
msg->ts_nsec
from_file)
(%d):
security_syslog(type);
default_message_loglevel;
32bit,
wait_event_interruptible(log_wait,
log_next_seq);
fragments
'c'
!(msg->flags
(msg->facility
sprintf(user->buf
"\\x%02x",
SYSLOG_FROM_READER);
/proc/vmcore
utilities
dmesg
Export
new_log_buf_len;
new_log_buf_len
log_buf_add_cpu(void)
setup_arch()
(num_possible_cpus()
cpu_extra
new_log_buf
LOG_ALIGN);
pr_info("debug:
loglevel
boot_delay;
boot_delay
preset_lpj
%ld,
boot_delay_msec(int
rem_nsec
long)ts,
msg->level;
9)
msg_print_text(const
*text
LOG_PREFIX))
next++;
print_prefix(msg,
newline)
*text;
kmalloc(LOG_LINE_MAX
PREFIX_MAX,
(!text)
PREFIX_MAX);
kfree(text);
clear_seq
clear_idx
next_seq
(!buf
FALL
(saved_console_loglevel
minimum_console_loglevel;
log_next_seq
call_console_drivers(int
CON_ENABLED))
oops_timestamp
cons;
flushed:1;
(cont.flushed)
log_store(cont.facility,
cont.level,
cont.buf,
cont.len);
cont.flags
cont.flushed
cont_flush(LOG_CONT);
(!cont.len)
cont.owner
cont.cons
80)
cont_print_text(char
in_sched
recursion_bug
lockdep_off();
raw_spin_lock(&logbuf_lock);
kern_level
(kern_level)
cont_flush(LOG_NEWLINE);
log_store(facility,
lockdep_on();
syslog()
LOGLEVEL_DEFAULT,
DEFINE_PER_CPU(printk_func_t,
*options,
console_cmdline;
c->name[0];
c++)
(strcmp(c->name,
c->index
(!brl_options)
brl_options);
init/main.c
4];
console_suspend_enabled
no_console_suspend
console_suspended
down_console_sem();
@hcpu:
wake_klogd
log_next(console_idx);
console_seq++;
for_each_console(c)
tty_driver
~CON_ENABLED;
(console_drivers)
"console
bootconsole
newcon->name,
newcon->index);
console_drivers->flags
bcon
(preferred_console
selected_console;
(newcon->setup
newcon->setup(newcon,
c->options)
(bcon
(CON_CONSDEV
newcon->next
(newcon->flags
console_sysfs_notify();
xxx
pr_info("%sconsole
[%s%d]
scheduler-internal
(pending
irq_work_queue(this_cpu_ptr(&wake_up_klogd_work));
impossible.
printk_timed_ratelimit()
dumper.
%-EINVAL
spin_lock_irqsave(&dump_list_lock,
dumper->registered
spin_unlock_irqrestore(&dump_list_lock,
dumper->active
(unlocked
@line:
youngest
*line,
(!dumper->active)
*len
next_idx;
dumper->cur_seq;
dumper->cur_idx;
interator
kmsg_dump_get_buffer()
dumper.dump()
dump_stack()
*log_lvl)
show_regs()
Implement
CPUCLOCK_PID(which_clock);
CPUCLOCK_MAX)
(timer->it.cpu.incr
timer->it.cpu.incr;
timer->it.cpu.expires;
check_clock(which_clock);
tp->tv_nsec
exported,
(!cputimer->running)
&sum);
cputimer->running
raw_spin_unlock_irqrestore(&cputimer->lock,
while_each_thread()
cputime_to_expires(cputime.utime
cputime.stime);
&rtn);
sample_to_timespec(which_clock,
CPU-clock
TIMER_RETRY,
firing.)
cleanup_timers_list(++head);
cputime_expires
listpos
expires_to_cputime(exp)))
expires_to_cputime(exp);
cputime_expires->sched_exp
posix_cpu_timer_kick_nohz(void)
(!task_cputime_zero(&tsk->cputime_expires))
(tsk->signal->cputimer.running)
old_expires,
old_incr,
new_expires
timespec_to_sample(timer->it_clock,
old_expires
timer->it.cpu.firing
list_del_init(&timer->it.cpu.entry);
old->it_value.tv_sec
old->it_value.tv_nsec
setting.
firing.
arm_timer(timer);
timer->it_overrun_last
cpu_timer_fire(timer);
itp->it_value.tv_sec
itp->it_value.tv_nsec
thread_group_cputime().
&itp->it_value);
Say
tsk->cpu_timers[N]
*firing)
*timers
check_timers_list(timers,
expires_to_cputime(expires);
USEC_PER_SEC/HZ))
__group_send_sig_info(SIGKILL,
SIGXCPU
__group_send_sig_info(SIGXCPU,
onecputick;
ptime,
sched_expires;
thread_group_cputimer(tsk,
prof_expires
check_cpu_itimer(tsk,
(psecs
sample->utime
&firing);
*newval
*newval))
*newval;
posix_cpu_timer_set(&timer,
posix_cpu_nsleep_restart(struct
do_cpu_nanosleep(which_clock,
copy_to_user(rmtp,
&it.it_value,
*rmtp))
restart_block->nanosleep.expires
timer->it_clock
posix_cpu_timer_create(timer);
proc_sys_poll_notify(table->poll);
uts_kern_table,
Moved
2006.
GPL.
vm_op
mapping.
vma->vm_private_data;
PAGE_SHIFT));
n_pages
depopulate;
relay_free_page_array(buf->page_array);
sizeof(size_t
chan;
free_buf:
kfree(buf->padding);
@kref:
kref_put().
*chan
relay_destroy_buf(buf);
(buf->subbufs_produced
*subbuf,
*prev_subbuf,
prev_padding)
*is_global)
.subbuf_start
.create_buf_file
.remove_buf_file
del_timer_sync(&buf->timer);
buf->finalized
erasing
NOTE.
d_inode(buf->dentry)->i_size
relay_create_buf_file(chan,
buf->cpu
kref_put(&buf->kref,
relay_remove_buf);
chan->cb
*chan;
relay_open_buf(chan,
hotcpu);
@base_filename:
*base_filename,
n_subbufs))
chan
chan->parent
chan->has_base_filename
strlcpy(chan->base_filename,
base_filename,
NAME_MAX);
relay_close_buf(chan->buf[i]);
channel.
(unlikely(length
buf->chan->subbuf_size))
toobig;
buf->padding[old_subbuf];
new_subbuf
buffer's
subbufs_consumed
consumed.
buf->subbufs_produced;
poll_wait(filp,
bytes_consumed)
(buf->bytes_consumed
relay_subbufs_consumed(buf->chan,
bytes_consumed;
(!read_pos)
relay_file_read_consume(buf,
(produced
buf->bytes_consumed;
produced)
write_offset
(read_offset
padding_start
(read_pos
end_pos;
subbuf's
rbuf->bytes_consumed
rbuf->chan->subbuf_size;
*in,
subbuf_pages,
nonpad_end
pidx
(read_start
subbuf_pages;
poff
private;
cur_pos
(this_end
spliced;
nonpad_ret
spliced
__wake_up_locked(&x->wait,
(*action)(long),
spin_unlock_irq(&x->wait.lock);
spin_lock_irq(&x->wait.lock);
x->done--;
__wait_for_common(x,
(w/timeout)
blkio
wait_for_common_io(x,
TASK_KILLABLE);
(!READ_ONCE(x->done))
->done
1999-2006
Status
(R)
keypress
change)
scancode,
scanstatus;
kbd_exists
((inb(KBD_STATUS_REG)
KBD_STAT_OBF)
inb(KBD_DATA_REG);
scanstatus
inb(KBD_STATUS_REG);
(scanstatus
KBD_STAT_MOUSE_OBF)
((scancode&0x7f)
shift_key
ctrl_key
0x9c)
Tab
Del
Home
Down
14;
(keychar
mashed
kernel/power/suspend.c
platform_suspend_ops
platform_freeze_ops
FREEZE_STATE_NONE;
spin_lock_irq(&suspend_freeze_lock);
spin_unlock_irq(&suspend_freeze_lock);
suspend-to-idle\n");
suspend_ops
pm_states[PM_SUSPEND_FREEZE]
PM_SUSPEND_MEM;
pm_states[i]
Platform
.valid()
"suspend"
(!sleep_state_supported(state))
trace_suspend_resume(TPS("freeze_processes"),
pm_notifier_call_chain(PM_POST_SUSPEND);
suspend_test_start();
Recover_platform;
devices");
trace_suspend_resume(TPS("resume_console"),
trace_suspend_resume(TPS("suspend_enter"),
TEST_NONE
trace_suspend_resume(TPS("sync_filesystems"),
sleep\n",
glibc-2.6
(year)
y1
400);
leaps2
Leap
60)
tm
days,
totalsecs
SECS_PER_HOUR;
1970
result->tm_wday
year.
11;
ip[y];
stop->se.exec_start
Simple,
set_flag
TRACE_NOP_OPT_ACCEPT)
TRACE_NOP_OPT_REFUSE)
do!
yourself.
result\n",
rwsem_set_owner(struct
rwsem_clear_owner(struct
shift/mult
1GHz
factors.
range:
clocksource_watchdog_work(struct
clocksource_watchdog_kthread(void
__clocksource_change_rating(struct
rating);
(finished_booting)
clocksource_change_rating
clocksource_mark_unstable(struct
&watchdog_list);
__clocksource_unstable(cs);
csnow,
wdnow,
cslast,
wdlast,
CLOCK_SOURCE_UNSTABLE)
cs->wd_last
wdnow;
cs->cs_last
csnow;
watchdog->mask);
cs->mask);
cs->mult,
%llx\n",
cs->name,
CLOCK_SOURCE_VALID_FOR_HRES)
finished_booting
watchdog_timer.expires
WATCHDOG_INTERVAL;
add_timer_on(&watchdog_timer,
~CLOCK_SOURCE_WATCHDOG;
clocksource_resume_watchdog(void)
clocksource_enqueue_watchdog(struct
CLOCK_SOURCE_MUST_VERIFY)
cs->rating
clocksource_dequeue_watchdog(struct
clocksource_stop_watchdog();
__clocksource_watchdog_kthread(void)
list_for_each_entry_safe(cs,
__clocksource_change_rating(cs,
clocksource_is_watchdog(struct
clocksource(s)
clocksource_resume_watchdog();
maxadj,
max_cycles;
max_nsecs
max_idle_ns
cs->mask,
skipcur)
rating.
(skipcur
CLOCK_SOURCE_VALID_FOR_HRES))
*best,
(strcmp(cs->name,
(curr_clocksource
clocksource_mutex
clocksource_select(void)
clocksource_select_fallback(void)
@scale:
hz
(cycles
second)
*SHOULD
NOT*
directly!
*special*
self-define
scale);
(!sec)
600
600;
clocksource_max_adjustment(cs);
(freq
clocksource_enqueue(cs);
rating)
Unbind
clocksource_unbind(cs);
overriding
name[CS_NAME_LEN];
max((ssize_t)PAGE_SIZE
(ssize_t)0),
"clocksource",
.bus
device_create_file(
&device_clocksource,
str)
Compatibility
clock=
deprecated.
CMD_BUFLEN
kdb_trap_printk;
KDB_STATE_SET(KGDB_TRANS);
bufsize)
vt100
*ped
++f)
ped
(*f)();
(escape_delay)
'\r')
'A':
dropthrough
'4':
escape_data[0]
mapkey;
escape_data[1]
kdb_read
bufsize
tab
dtab_count;
bufsize);
*(--lastchar)
--cp;
\r",
*lastchar++
(!KDB_STATE(KGDB_TRANS))
memset(tmpbuffer,
strlen(kdb_prompt_str)
(lastchar-buffer));
*(tmpbuffer+strlen(kdb_prompt_str)
(lastchar-buffer))
kdb_printf("\r%s\r",
tmpbuffer);
*lastchar
(char)key;
*(lastchar+1)
kdb_printf("%c",
(p_tmp
dtab_count)
*++lastchar
*cp++
(lastchar
strcmp(lastchar
kdb_gdb_state_pass(lastchar
prompt)
flush_delay
(flush_delay)
args.
len2)
saved_loglevel
saved_trap_printk;
"more>
kdb_trap_printk
colcount
kdbgetintenv("LOGGING",
&logging);
sizeof(kdb_buffer);
kdb_parse()
yyy
kdb_print_out;
search.
cphold
replaced_byte;
strcpy(kdb_buffer,
cphold);
(kdb_grepping_flag
printed,
null.
kdb_buffer));
(dbg_io_ops
!dbg_io_ops->is_console)
c->write(c,
c->next;
(kdb_buffer[len]
moreprompt
kdb_input_flush();
Really
"grep
update_pages_handler(struct
special.
RINGBUF_TYPE_DATA_TYPE_LEN_MAX);
+---------------+
+------------------------------+
Turning
DISABLED
RB_ALIGNMENT
8U
RB_FORCE_8BYTE_ALIGNMENT
RB_ARCH_ALIGNMENT
RINGBUF_TYPE_DATA
extend.
rb_event_data(event);
write;
BUF_PAGE_HDR_SIZE)
commit),
int)is_signed_type(long));
record_disabled;
*head_page;
*tail_page;
commits;
read_stamp;
irq_work;
@full:
@buffer's
&buffer->irq_work;
&cpu_buffer->irq_work;
work->waiters_pending
!ring_buffer_empty(buffer))
!ring_buffer_empty_cpu(buffer,
pagebusy;
pagebusy
waiters_pending
TIME_EXTENTS
normalization
DEBUG_SHIFT;
rb_time_stamp(buffer);
LSB
+----+
~RB_FLAG_MASK)
RB_PAGE_MOVED;
RB_FLAG_MASK;
rb_list_head(list->next)
*)&list->next;
rb_set_list_to_head(cpu_buffer,
RB_PAGE_UPDATE);
RB_PAGE_NORMAL);
*bpage
(rb_is_head_page(cpu_buffer,
*next_page)
local_add_return(RB_WRITE_INTCNT,
old_entries
(rb_check_list(cpu_buffer,
kzalloc_node(ALIGN(sizeof(*bpage),
bpage->page
rb_init_page(bpage->page);
rb_wake_up_waiters);
fail_free_buffer;
fail_free_reader;
INIT_LIST_HEAD(&cpu_buffer->reader_page->list);
list_entry(cpu_buffer->pages,
cpu_buffer;
free_buffer_page(cpu_buffer->reader_page);
fail_free_buffer:
kfree(cpu_buffer);
rb_cpu_notify(struct
buffer->clock
cpumask_copy(buffer->cpumask,
buffer->buffers[cpu]
rb_allocate_cpu_buffer(buffer,
(!buffer->buffers[cpu])
rb_free_cpu_buffer(buffer->buffers[cpu]);
kfree(buffer->buffers);
free_cpumask_var(buffer->cpumask);
rb_reset_cpu(struct
local_read(&bpage->write)
*next_page;
*last_page,
*first_page;
raw_spin_lock_irq(&cpu_buffer->reader_lock);
to_remove
first_page
nr_removed
rb_list_head(to_remove)->next;
next_page.
next_page;
raw_spin_unlock_irq(&cpu_buffer->reader_lock);
last_page
&cpu_buffer->overrun);
local_sub(BUF_PAGE_SIZE,
*pages
*r;
resizing
mutex_lock(&buffer->mutex);
cpu_buffer->nr_pages;
Fire
(!cpu_buffer->nr_pages_to_update)
&cpu_buffer->update_pages_work);
wait_for_completion(&cpu_buffer->update_done);
atomic_inc(&buffer->record_disabled);
atomic_dec(&buffer->record_disabled);
rb_event_index(event);
local_set(&cpu_buffer->commit_page->page->commit,
rb_page_write(cpu_buffer->commit_page));
cpu_buffer->commit_page->page->time_stamp;
iter->head_page->page->time_stamp;
add_timestamp,
(unlikely(add_timestamp))
RB_FORCE_8BYTE_ALIGNMENT)
RB_PAGE_HEAD:
RB_PAGE_NORMAL:
HEAD.
new_head,
rb_head_page_set_normal(cpu_buffer,
tail_page->real_end
__rb_page_index(tail_page,
tail);
kmemcheck_annotate_bitfield(event,
bitfield);
tail)
ts)
cpu_buffer->buffer;
local_inc(&cpu_buffer->commit_overrun);
fighting
reader_page,
reread
rb_reset_tail(cpu_buffer,
cpu_buffer->tail_page;
old_index
(bpage->page
old_index)
write_mask;
local_inc(&cpu_buffer->committing);
local_dec(&cpu_buffer->committing);
add_timestamp
current_context
SoftIRQ
101
1010
1001
ring_buffer_lock_reserve
header)
body
out_nocheck;
(atomic_read(&cpu_buffer->record_disabled))
BUF_MAX_DATA_SIZE)
rb_reserve_next_event(buffer,
cpu_buffer,
rb_update_write_stamp(cpu_buffer,
irq_work_queue(&cpu_buffer->irq_work.work);
rb_commit(cpu_buffer,
rb_wakeups(buffer,
cpu_buffer);
local_dec(&bpage->entries);
&bpage);
Sometimes
RB_WARN_ON(buffer,
*reader
reader->read
(much
preempt_disable).
new_rd;
atomic_read(&buffer->record_disabled);
new_rd
(atomic_cmpxchg(&buffer->record_disabled,
rd,
new_rd)
rb_num_of_entries(cpu_buffer);
local_read(&cpu_buffer->overrun);
cpu_buffer->read;
entries)
iter->cache_reader_page
cpu_buffer->read_stamp;
local_set(&cpu_buffer->reader_page->write,
local_set(&cpu_buffer->reader_page->entries,
local_set(&cpu_buffer->reader_page->page->commit,
reader->list.prev;
cpu_buffer->last_overrun
rb_iter_head_event(iter);
cpu_buffer->lost_events;
cpu_buffer->cpu,
@lost_events:
rb_buffer_peek(cpu_buffer,
rb_iter_peek(iter,
ring_buffer_read_start
ring_buffer_read_finish.
atomic_inc(&buffer->resize_disabled);
ring_buffer_read_prepare_sync
cpu_buffer->read_bytes
rb_per_cpu_empty(cpu_buffer);
out_dec;
ring_buffer_alloc_read_page
rb_init_page(bpage);
ring_buffer_alloc_read_page.
save_timestamp;
*data_page;
reader->read;
read))
rpos
&bpage->commit);
nr_pages_same
CONFIG_RING_BUFFER_STARTUP_TEST
*item;
68
rb_item);
data->bytes_dropped
event_len
item->size
rb_string,
data->bytes_alloc
data->bytes_written
rb_write_something(data,
&rb_data[cpu];
pr_cont("FAILED\n");
total_read
total_alloc
item->size,
total_read);
Shailabh
Nagar,
delayacct_on
ktime_get_ns()
delayacct_end(&current->delays->blkio_start,
t2,
t3;
task_cputime_scaled(tsk,
&utimescaled,
&stimescaled);
t2;
spin_lock_irqsave(&tsk->delays->lock,
d->blkio_delay_total
d->swapin_delay_total
d->freepages_delay_total
spin_unlock_irqrestore(&tsk->delays->lock,
serialization,
NET
RX
WARN_ON_ONCE(in_irq());
Were
SOFTIRQ_MASK))
MAX_SOFTIRQ_TIME
lockdep_softirq_start(void)
trace_hardirq_exit();
in_hardirq;
lockdep_softirq_end(bool
in_hardirq)
pending;
softirq_bit;
PF_MEMALLOC
softirq_vec;
account_irq_exit_time(current);
do_softirq_own_stack();
__do_softirq();
tasklet_head,
raise_softirq_irqoff(TASKLET_SOFTIRQ);
raise_softirq_irqoff(HI_SOFTIRQ);
__this_cpu_read(tasklet_hi_vec.head);
__this_cpu_write(tasklet_hi_vec.head,
__raise_softirq_irqoff(HI_SOFTIRQ);
(tasklet_trylock(t))
(!atomic_read(&t->count))
(!test_and_clear_bit(TASKLET_STATE_SCHED,
t->func(t->data);
(*func)(unsigned
&t->state));
*ttimer
&per_cpu(tasklet_hi_vec,
(*i
cpu).tail)
cpu).tail);
do_for_each_event_file_safe(tr,
list_for_each_entry_safe(file,
filter_type)
GFP_TRACE);
(!id)
fbuffer->flags,
fbuffer->pc);
fbuffer->entry;
*fbuffer)
tracepoint_probe_register(call->tp,
call->class->probe,
tracepoint_probe_unregister(call->tp,
call->class->perf_probe,
call);
(!(file->flags
set_bit(FTRACE_EVENT_FL_RECORDED_CMD_BIT,
clear_bit(FTRACE_EVENT_FL_RECORDED_CMD_BIT,
soft_disable)
tracepoint,
(soft_disable)
call->class->reg(call,
FTRACE_EVENT_FL_SOFT_MODE)
WARN_ON_ONCE(system_refcount(system)
(system->ref_count
SYSTEM_FL_FREE_NAME)
kfree(system->name);
kfree(system);
WARN_ON_ONCE(dir->ref_count
dir->ref_count
__put_system_dir(dir);
unneeded
*sub,
(!name
TRACE_EVENT_FL_IGNORE_ENABLE)
strcmp(match,
strcmp(sub,
strcmp(event,
__ftrace_set_clr_event_nolock(tr,
*(buf
EVENT_BUF_SIZE
list_for_each_entry_continue(file,
call->class->reg)
list_entry(&tr->events,
ftrace_event_file,
(strcmp(call->class->system,
TRACE_SYSTEM)
(likely(file))
strlen(buf));
strcmp(call->class->system,
long)v)
FORMAT_HEADER:
FORMAT_FIELD_SEPERATOR:
FORMAT_PRINTFMT:
field:TYPE
array_descriptor
field->type,
field->name,
field->offset,
!!field->is_signed);
tracing_open_generic(inode,
dir->tr
dir->subsystem
ftrace_event_avail_open(struct
ftrace_event_set_open(struct
ftrace_event_release(struct
subsystem_open,
system_enable_read,
system_enable_write,
*seq_ops
ftrace_event_open(inode,
system->name
system;
file->system
dir->entry;
dir->entry,
*d_events;
(!d_events)
events/%s\n",
remove_event_file_dir(file);
do_for_each_event_file_safe()
event_init(call);
list_add(&call->list,
&ftrace_events);
elen;
elen
snprintf(ptr,
"%ld",
map->enum_value);
between.
last_i;
list_for_each_entry_safe(call,
last_i
(call->class->system
trace_create_new_event(call,
event_create_dir(tr->event_dir,
__add_event_to_tracers(struct
__trace_remove_event_call(call);
clear_trace
list_for_each_entry(call,
__trace_add_new_event(call,
&data->file->flags);
data->enable
event_enable_probe,
event_enable_count_probe,
event_enable_func,
register_event_cmds(void)
&ftrace_show_header_fops);
tr->event_dir
create_event_toplevel_files(parent,
token,
early_enable_events(tr,
event_trace_enable()
events:\n");
system->name,
event_trace_self_test_with_function(void)
event_trace_self_tests();
1usec
rem.tv64
cval,
*it
&tsk->signal->it[clock_id];
cinterval
CPUCLOCK_VIRT
(cval
cputime_to_timeval(cval,
cputime_to_timeval(cinterval,
ITIMER_REAL:
ktime_to_timeval(tsk->signal->it_real_incr);
ITIMER_VIRTUAL:
get_cpu_itimer(tsk,
CPUCLOCK_VIRT,
ITIMER_PROF:
get_buffer;
&get_buffer,
sizeof(get_buffer)))
real_ns)
cpu_ns
ovalue)
ninterval;
incr_error;
nval
(ovalue)
set_cpu_itimer(tsk,
ovalue);
F_printk
F_printk(fmt,
args...)
____ftrace_##name
container.item),
sizeof(field.container.item),
BUILD_BUG_ON(len
__entry
FTRACE_ENTRY_REG(call,
probe-based
<masami.hiramatsu.pt@hitachi.com>
generic:
2010-2011
FIELD_STRING_FUNC
get_rloc_offs(*dl);
PRINT_TYPE_FUNC_NAME(type)(struct
*ent);
PRINT_TYPE_FMT_NAME(type)[]
fetch_reg_string
fetch_reg_string_size
fetch_retval_string
fetch_retval_string_size
DECLARE_FETCH_FUNC(symbol,
string_size);
DECLARE_FETCH_FUNC(deref,
fetch_bitfield_string
fetch_bitfield_string_size
__DEFAULT_FETCH_TYPE(t)
ptype,
_size,
{.name
sign)
CONFIG_KPROBE_EVENT
*sc);
fetch;
fetch_size;
!!(tp->flags
(!isalpha(*name)
traceprobe_parse_probe_arg(char
*parg,
traceprobe_conflict_field_name(const
traceprobe_update_arg(struct
traceprobe_free_probe_arg(struct
traceprobe_split_symbol_offset(char
traceprobe_probes_write(struct
char**));
traceprobe_command(const
(unlikely(tp->args[i].fetch_size.fn))
call_fetch(&tp->args[i].fetch,
get_rloc_len(*dl);
set_print_fmt(struct
Dependents
Watchers
pm_qos_objects
requests,
PM_QOS_MIN,
pm_qos_power_write(struct
*f_pos);
pm_qos_power_read(struct
pm_qos_power_open(struct
pm_qos_power_release(struct
pm_qos_get_value(struct
total_value
(plist_head_empty(&c->list))
(c->type)
PM_QOS_MIN:
PM_QOS_MAX:
PM_QOS_SUM:
pm_qos_req_action
pm_qos_get_value(c);
PM_QOS_REMOVE_REQ:
plist_del(node,
PM_QOS_UPDATE_REQ:
PM_QOS_ADD_REQ:
curr_value
@pqf:
pm_qos_flags
*pqf,
pm_qos_flags_request
pqf->effective_flags
curr_value;
irqflags);
list_empty(&pqf->list)
pqf->effective_flags;
pm_qos_flags_remove_req(pqf,
req);
req->pm_qos_class
(new_value
req->node.prio)
pm_qos_update_target(
pm_qos_array[req->pm_qos_class]->constraints,
PM_QOS_UPDATE_REQ,
pm_qos_update_request_timeout
__pm_qos_update_request(req,
@req
pm_qos
new_value,
pm_qos_array[pm_qos_class]->constraints->notifiers,
notifier);
(pm_qos_class
PM_QOS_CPU_DMA_LATENCY;
PM_QOS_NUM_CLASSES;
req
*f_pos)
last_seed;
std
bm_last
bm_max
bm_min
bm_cnt
"last=%llu
first=%llu
max=%llu
min=%llu
avg=%u
std=%d
std^2=%lld",
bm_last,
bm_first,
bm_max,
bm_min,
last_seed
do_div(seed,
bm_std
bm_avg
bm_stddev
cpu_present_mask
cpu_online_mask
mutex_lock(&cpu_hotplug.lock);
cpuhp_lock_release();
cpu_hotplug.lock
cpu_hotplug.active_writer
raw_notifier_chain_register(&cpu_chain,
raw_notifier_chain_unregister(&cpu_chain,
tasks_frozen)
*hcpu
*)(long)cpu;
tasks_frozen
CPU_TASKS_FROZEN
cpu_hotplug_begin();
nr_calls--;
nr_calls,
out_release;
smpboot
everyone.
smpboot_unpark_threads(cpu);
cpu_hotplug_done();
(cpu_hotplug_disabled)
_cpu_down(cpu,
(cpu_online(cpu)
out_notify;
_cpu_up(cpu,
cpumask_clear(frozen_cpus);
trace_suspend_resume(TPS("CPU_OFF"),
CPU%d
trace_suspend_resume(TPS("CPU_ON"),
cpu_hotplug_enable();
cpu_hotplug_pm_callback
CPU_BITS_ALL;
DECLARE_BITMAP(cpu_possible_bits,
to_cpumask(cpu_possible_bits));
to_cpumask(cpu_present_bits));
to_cpumask(cpu_online_bits));
obfuscation
xor
product
truncated,
than,
t2)
*m1,
*m2)
mutex_unlock(m1);
inspect
&task2->signal->cred_guard_mutex);
async_synchronize_cookie()
async_entry,
%lli_%pF
%i\n",
long)entry->cookie,
entry->func,
long)ktime_to_ns(delta)
newcookie
next_cookie++;
__async_schedule(func,
&async_dfl_domain);
globally.
async_synchronize_full
@cookie:
checkpoint
@cookie
system-call
7500
(unlikely(!ctx))
AUDIT_PERM_READ)
AUDIT_PERM_ATTR)
(!ctx->prio)
ctx->tree_count;
audit_put_chunk(q->c[n]);
q->c[n]
(audit_tree_match(p->c[n],
tree))
audit_uid_comparator(uid,
audit_gid_comparator(gid,
(f->val)
audit_uid_comparator(cred->suid,
audit_gid_comparator(cred->sgid,
need_sid
&rule->fields[i];
task_pid_nr(tsk);
audit_comparator(pid,
AUDIT_PPID:
AUDIT_PERS:
AUDIT_EXIT:
ctx->return_valid)
AUDIT_SUCCESS:
audit_comparator(ctx->return_valid,
AUDIT_DEVMAJOR:
AUDIT_DEVMINOR:
security_task_getsecid(tsk,
security_audit_rule_match(sid,
AUDIT_ARG0:
AUDIT_ARG1:
AUDIT_ARG2:
AUDIT_ARG3:
AUDIT_PERM:
AUDIT_FILETYPE:
AUDIT_FIELD_COMPARE:
(rule->action)
AUDIT_NEVER:
AUDIT_ALWAYS:
AUDIT_BUILD_CONTEXT;
0xffffffff)
tsk->tgid
(audit_in_mask(&e->rule,
ctx->major)
audit_filter_rules(tsk,
return_code)
(return_code
context->return_code
audit_filter_syscall(tsk,
context->proctitle.value
context->proctitle.len
((aux
aux->next;
kfree(aux);
context->aux_pids
context->state
~0ULL
TIF_SYSCALL_AUDIT);
context->filterkey
audit_free_names(context);
audit_free_aux(context);
kfree(context->filterkey);
auid),
uid),
len_sent
arg_num,
max_execve_audit_len
MAX_EXECVE_AUDIT_LEN;
too_long
len_left;
copy_from_user(buf,
buf[to_send]
string");
(context->type)
mode=%#ho",
mq_flags=0x%lx
mq_maxmsg=%ld
context->name_count);
context->aux;
aux->next)
*axs
*)aux;
&call_panic);
audit_take_context(tsk,
audit_log_exit(context,
(!list_empty(&context->killed_trees))
audit_kill_trees(&context->killed_trees);
context->in_syscall
context->target_pid
context->sockaddr_len
context->fds[0]
context->trees;
context->tree_count;
audit_tree_lookup(inode);
chunk)))
reference\n");
audit_set_auditable(context);
aname
AUDIT_TYPE_UNKNOWN);
name->refcnt++;
d_backing_inode(dentry);
AUDIT_TYPE_PARENT)
n->dev
AUDIT_TYPE_PARENT;
audit_copy_inode(n,
dentry,
Syscalls
hooked
(inode)
(!n->name
AUDIT_TYPE_UNKNOWN))
!audit_compare_dname_path(dname,
found_parent
found_child
(!found_child)
loginuid)
unset,
oldloginuid,
loginuid;
oldloginuid
loginuid,
audit_get_loginuid(current);
sessionid;
mq_attr));
send/receive
timespec));
context->mq_notify.sigev_signo
get/set
context->ipc.has_perm
@uid:
@gid:
linux_binprm
task_uid(t);
task_tgid_nr(t);
t_uid;
axp
new->cap_permitted;
sys_capset
task_pid_nr(current);
current->mm);
audit_log_task(ab);
(unlikely(!sched_ref))
unregister_trace_sched_wakeup_new(probe_sched_wakeup,
unregister_trace_sched_wakeup(probe_sched_wakeup,
mutex_lock(&sched_register_mutex);
mutex_unlock(&sched_register_mutex);
Protection
*ce;
~0ULL;
set_mode()
Legacy
state-specific
clockevent
(dev->tick_resume)
MIN_DELTA_LIMIT)
"CE:
dev->name
clockevents_program_min_delta
clockevents_program_min_delta(struct
dev->min_delta_ns;
ktime_add_ns(ktime_get(),
dev->retries++;
dev->set_next_event((unsigned
clockevents_program_min_delta(dev)
(int64_t)
dev->max_delta_ns);
list_add(&dev->list,
&clockevent_devices);
tick_check_new_device(dev);
list_for_each_entry(dev,
ced
CLOCK_EVT_STATE_DETACHED)
(newdev)
newdev
list_del_init(&ced->list);
clockevents_mutex
track.
mutex_lock(&clockevents_mutex);
mutex_unlock(&clockevents_mutex);
dev->tick_resume);
CLOCK_EVT_FEAT_PERIODIC)
CLOCK_EVT_STATE_DETACHED;
raw_spin_lock_irqsave(&clockevents_lock,
raw_spin_unlock_irqrestore(&clockevents_lock,
dev->max_delta_ticks
clockevents_config(dev,
CLOCK_EVT_STATE_PERIODIC);
list_for_each_entry_safe(dev,
raw_spin_lock_irq(&clockevents_lock);
raw_spin_unlock_irq(&clockevents_lock);
tick_broadcast_init_sysfs(void)
&dev_attr_current_device);
device_create_file(dev,
rt_mutex_deadlock_account_lock(struct
rt_mutex_deadlock_account_unlock(struct
debug_rt_mutex_init_waiter(struct
debug_rt_mutex_free_waiter(struct
debug_rt_mutex_init(struct
debug_rt_mutex_lock(struct
debug_rt_mutex_unlock(struct
debug_rt_mutex_proxy_lock(struct
debug_rt_mutex_proxy_unlock(struct
debug_rt_mutex_deadlock(enum
debug_rt_mutex_print_deadlock(struct
cpu_pm
write_lock_irqsave(&cpu_pm_notifier_lock,
write_unlock_irqrestore(&cpu_pm_notifier_lock,
nr_calls;
(nr_calls
tick_usec
(nsecs):
tick_length;
NSEC_PER_USEC)
NTP_INTERVAL_FREQ)
time_freq;
time_reftime;
time_adjust;
ntp_tick_adj;
pps_shift
PPS_MAXWANDER
median
pps_jitter;
pps_shift;
pps_freq;
ns/s)
pps_stabil;
pps_calcnt;
pps_jitcnt;
pps_stbcnt;
pps_errcnt;
ntp_offset_chunk(s64
shift_right(offset,
SHIFT_PLL
pps_reset_freq_interval(void)
pps_clear(void)
pps_reset_freq_interval();
pps_tf[1]
pps_tf[2]
pps_valid
pps_dec_valid(void)
STA_PPSWANDER
STA_PPSERROR);
pps_clear();
pps_set_freq(s64
is_error_status(int
(STA_PPSTIME|STA_PPSJITTER))
STA_PPSFREQ)
pps_fill_timex(struct
txc->ppsfreq
PPM_SCALE_INV_SHIFT)
PPM_SCALE_INV,
txc->shift
txc->stabil
txc->jitcnt
txc->calcnt
txc->errcnt
txc->stbcnt
NSEC_PER_USEC
tick_nsec
div_u64(second_length,
time_reftime
MAXFREQ_SCALED);
-MAXFREQ_SCALED);
second_overflow(unsigned
86400
printk(KERN_NOTICE
"Clock:
UTC\n");
TIME_WAIT;
(time_adjust
MAX_TICKADJ;
MAX_TICKADJ_SCALED;
CONFIG_GENERIC_CMOS_UPDATE
sync_cmos_clock(struct
60);
(next.tv_nsec
next.tv_sec
&sync_cmos_work,
ntp_notify_cmos_timer(void)
((time_status
STA_PLL)
*time_tai)
ADJ_FREQUENCY)
txc->freq
PPM_SCALE;
txc->constant;
txc->constant
ntp_validate_timex(struct
ADJ_ADJTIME)
!capable(CAP_SYS_TIME))
txc->tick
((txc->modes
PPM_SCALE
txc->freq)
__do_adjtimex(struct
txc->time.tv_usec
-NSEC_PER_SEC
*jitter
pps_tf[1];
pps_tf[0];
(pps_shift
ftemp;
(freq_norm.sec
pps_shift))
pps_dec_freq_interval();
printk_deferred(KERN_ERR
freq_norm.sec);
delta_mod
REALTIME
(jitter
(pps_jitter
PPSJITTER:
STA_PPSJITTER;
oscillator
__hardpps(const
(freq_norm.nsec
#########################
BIG
FAT
WARNING
##########################
cpu_clock(j)
!!
####################################################################
sub-architectures
sched_clock_running;
sched_clock_init()
(!sched_clock_running)
__set_sched_clock_stable();
sched_clock_init(void)
cpu_sdc(cpu);
scd->tick_raw
ktime_now;
scd->clock
{set,clear}_sched_clock_stable()
(s64)(x
*scd)
old_clock,
scd->clock;
TICK_NSEC);
this_scd();
old_val,
sched_clock_local()
low32bit
portion.
this_clock
sched_clock_local(my_scd);
remote_clock
old_val
this_clock;
sched_clock_cpu(int
*scd;
sched_clock_local(scd);
now_gtod;
disabled):
cpu_clock(int
local_clock(void)
crash,
FETCH_FUNC_NAME(deref,
*dprm
call_fetch(&dprm->orig,
dprm->offset;
NOKPROBE_SYMBOL(FETCH_FUNC_NAME(deref,
update_deref_fetch_param(data->orig.data);
update_symbol_cache(data->orig.data);
free_deref_fetch_param(data->orig.data);
free_symbol_cache(data->orig.data);
Bitfield
'b')
param;
(is_kprobe)
kstrtoul(arg
*)param;
t->fetch[FETCH_MTD_memory];
'(');
parse_probe_arg(arg,
bo;
&tail,
bf
bprm->hi_shift
parg->type,
(CHECK_FETCH_FUNCS(bitfield,
**))
WRITE_BUFSIZE
4096
*kbuf,
(done
strchr(kbuf,
"REC->"
tp->args[i].name);
__set_print_fmt(tp,
nla_policy
NLA_STRING
&family,
@skb
genlmsghdr
*genlhdr
genlmsg_data(genlhdr);
genlmsg_end(skb,
reply);
skb_next
s->valid
invalidated
list_for_each_entry_safe(s,
list_del(&s->list);
taskstsats
per-task-foo(stats,
delayacct_add_tsk(stats,
stats->version
TASKSTATS_VERSION;
stats->nvcsw
stats->nivcsw
memcpy(stats,
*listeners;
cpu_possible_mask))
&per_cpu(listener_array,
*na,
(na
NLA
(nla_put(skb,
(!na)
nla_nest_cancel(skb,
nla_total_size(sizeof(struct
cgroupstats));
add_del_listener(info->snd_portid,
nla_total_size(0);
TASKSTATS_TYPE_PID,
TASKSTATS_TYPE_TGID,
STATS
&init_pid_ns));
.cmd
.doit
.policy
mechanisms
<linux/ioport.h>
<crypto/hash.h>
vmcoreinfo_max_size
kexec_in_progress
kexec_calculate_store_digests(struct
"Crash
necessary,
KIMAGE_NO_DEST
kimage_is_destination_range(struct
*kimage_alloc_page(struct
nr_segments
RAM.
((mstart
pstart
(image->type
KEXEC_TYPE_CRASH)
image->segment[i].memsz
crashk_res.end))
image->entry
image->last_entry
kimage_free_page_list(struct
**rimage,
do_kimage_alloc_init();
image->start
sanity_check_segment_list(image);
KEXEC_TYPE_CRASH;
image->control_code_page
get_order(KEXEC_CONTROL_PAGE_SIZE));
(!image->control_code_page)
control_code_buffer\n");
(!kexec_on_panic)
image->swap_page
(!image->swap_page)
buffer\n");
out_free_control_pages;
*rimage
out_free_control_pages:
out_free_image:
vfree(*buf);
*ehdr,
relsec)
unsupported.\n");
elsewhere.
kimage_file_post_load_cleanup(struct
image->cmdline_buf
vfree(pi->purgatory_buf);
pi->purgatory_buf
vfree(pi->sechdrs);
pi->sechdrs
Above
image->image_loader_data
*cmdline_ptr,
image->kernel_buf,
image->kernel_buf_len);
pr_debug("kernel
out_free_post_load_bufs;
mstart)
mend))
lru);
list_del(&page->lru);
kimage_free_pages(page);
intermediaries
O(N)
epfn
PAGE_SHIFT))
list_add(&pages->lru,
region,
~(size
hole_end
(hole_end
(image->type)
KEXEC_TYPE_DEFAULT:
KEXEC_TYPE_CRASH:
(*image->entry
kimage_alloc_page(image,
ind_page
kimage_add_entry(image,
ind
(ind
kimage_free_entry(ind);
IND_SOURCE)
list_add(&page->lru,
old_page);
maddr;
ubytes,
mbytes;
segment->kbuf;
segment->buf;
segment->bufsz;
segment->memsz;
segment->mem;
maddr);
(mbytes)
kmap(page);
mbytes,
(maddr
uchunk
min(ubytes,
mchunk);
kexec,
copy_from_user(ptr,
kunmap(page);
uchunk;
destination.
segment);
**dest_image,
(!capable(CAP_SYS_BOOT)
kexec_load_disabled)
artificial
&kexec_image;
&kexec_crash_image;
kimage_free(xchg(&kexec_crash_image,
NULL));
kimage_alloc_init(&image,
crash_map_reserved_pages();
machine_kexec_prepare(image);
kimage_load_segment(image,
&image->segment[i]);
kimage_terminate(image);
crash_unmap_reserved_pages();
xchg(dest_image,
image);
kimage_free(image);
compat_kexec_segment
sizeof(out));
KEXEC_FILE_ON_CRASH)
&image->segment[i];
kexec_mutex
(kexec_crash_image)
mutex_lock(&kexec_mutex);
crashk_res.end;
(new_size
old_size)
KEXEC_CRASH_MEM_ALIGN);
RAM";
elf_note
note;
note.n_namesz
note.n_descsz
note.n_type
&note,
sizeof(note));
sizeof(prstatus));
append_elf_note(buf,
final_note(buf);
':'
pr_warn("Memory
cmdline;
memparse(cmdline,
(cmdline
suffix,
strlen(suffix)))
strlen(suffix);
(!strncmp(q,
(!ck_cmdline)
suffix);
syntax
first_colon
strchr(ck_cmdline,
crash_base);
vmcoreinfo_size);
update_vmcoreinfo_note();
VMCOREINFO_OFFSET(list_head,
VMCOREINFO_OFFSET(vmap_area,
*kbuf)
*image
kbuf->image;
temp_end;
kbuf->memsz;
(kimage_is_destination_range(image,
temp_end))
kbuf->mem
temp_start;
Success,
navigating
kbuf->buf_max)
kbuf->buf_min
with-in
kbuf);
*load_addr)
locate_mem_hole_callback);
ksegment->memsz
*load_addr
ksegment->mem;
crypto_shash
*tfm;
shash_desc
desc_size,
nullsz;
tfm
(IS_ERR(tfm))
desc_size
crypto_shash_descsize(tfm)
sizeof(*desc);
desc->tfm
tfm;
desc->flags
crypto_shash_init(desc);
out_free_sha_regions;
purgatory.
crypto_shash_update(desc,
nullsz
kexec_purgatory_get_set_symbol(image,
buf_sz,
bss_pad;
entry_sidx
sechdrs_c
->sh_addr
sizeof(Elf_Shdr));
pi->ehdr->e_shnum
relocated.
sechdrs[i].sh_offset
sechdrs[i].sh_offset;
relocatable
sechdrs[i].sh_addralign;
(buf_align
(buf_sz
purgatory_buf;
load_addr
pi->purgatory_load_addr;
sechdrs[i].sh_addr
pi->ehdr->e_shnum)
sechdrs,
(kexec_purgatory_size
ehdr->e_shnum)
strtab
kexec_purgatory_find_symbol(pi,
CONFIG_KEXEC_JUMP
(kexec_image->preserve_context)
wq_busy
elapsed_msecs64;
sleep_usecs
(!user_only)
freeze_processes
pr_info("Freezing
pm_freezing
pr_cont("done.");
BUG_ON(in_atomic());
trace_suspend_resume(TPS("thaw_processes"),
pr_info("Restarting
thaw_workqueues();
PF_SUSPEND_TASK));
__thaw_task(p);
pr_cont("done.\n");
platform_mode);
!CONFIG_HIBERNATION
snapshot_read_next()
snapshot_write_next().
swap);
!CONFIG_SUSPEND
!CONFIG_PM_TEST_SUSPEND
suspend_freeze_processes(void)
thaws
suspend_thaw_processes(void)
!CONFIG_PM_AUTOSLEEP
all_var
in_gate_area_no_mm(addr);
(all_var)
stream.
skipped_first
byte.
data++;
ARRAY_SIZE(namebuf));
kallsyms_addresses[i];
symbol_start
(high
symbol_start;
symname,
symbol_offset;
kallsyms_lookup(address,
(name
sprintf(buffer
@address.
address);
get_symbol_offset
iter->nameoff;
iter->nameoff
reset_iter(iter,
(!update_iter(m->private,
"%pK
*)iter->value,
_TICK_SCHED_H
tick_device_mode
tick_nohz_mode
sleeps.
div_factor))
div_factor;
.symbol_name
.pre_handler
.post_handler
handler\n");
.entry
j_kprobe_target,
"kprobe_target"
"kprobe_target2"
handler2
regs_return_value(regs);
pr_err("call
.entry_handler
entry_handler,
pr_err("register_kretprobe
context.\n");
backtrace_test_saved(void)
pr_info("====[
spin-lock
mcs_unlock
mcs_lock.
**lock,
trace_print_flags
delim)
delim);
*symbol_array)
symbol_array[i].name;
symbol_array[i].mask)
symbol_array[i].name);
*)(trace_seq_buffer_ptr(p)))
el_size;
*kretprobed(const
kretprobed(str);
vmstart
TRACE_ITER_SYM_ADDR)
<"
(!ip)
(sym_flags
TRACE_FLAG_PREEMPT_RESCHED:
MARK
abs_ts
rel_ts
(verbose
in_ns)
iter->ts,
abs_ts,
[%03d]
TRACE_ITER_VERBOSE);
(EVENT_HASHSIZE
(event->type
&ftrace_event_list;
FTRACE_MAX_EVENT)
types.
(!event->type)
%5d:%3d:%c
field->prev_pid,
field->prev_prio,
field->next_cpu,
field->next_pid,
field->next_prio,
trace_ctxwake_print(iter,
S)
(!S)
T);
trace_ctxwake_raw(iter,
field->prev_pid);
field->prev_prio);
field->next_cpu);
field->next_pid);
field->next_prio);
trace_ctxwake_hex(iter,
trace_ctxwake_bin,
trace>\n");
events[i];
workqueue.c.
current_work's
tick_periodic(cpu);
tick_periodic()
TICKDEV_MODE_PERIODIC;
td->evtdev
tick_setup_device(td,
tick_oneshot_notify();
newdev->cpumask))
cpumask_of(cpu)))
(curdev
(!tick_check_percpu(curdev,
tick_suspend_local();
tick_resume_local();
(possibly)
%tick_unfreeze().
raw_spin_lock(&tick_freeze_lock);
(tick_freeze_depth
num_online_cpus())
raw_spin_unlock(&tick_freeze_lock);
%tick_freeze().
x...)
printk(x);
((long
long)nsec
group_path,
%9Ld.%06ld
SPLIT_NS(p->se.vruntime),
%15Ld.%06ld
0L,
rq_cpu)
MIN_vruntime
"load",
print_cpu(struct
#n,
rq->n);
"%-40s:
long)(x))
SPLIT_NS(x))
print_cpu(m,
print_cpu(NULL,
__P(F)
long)p->F)
__PN(F)
long)p->F))
pol
(pol
nr_faults
cpu_current,
-1LL;
cpu_clock(this_cpu);
"../locking/rtmutex_common.h"
rcu_nocb_mask
Offload
CONFIG_RCU_FANOUT
*rcu_state_p
&rcu_preempt_state;
rcu_preempted_readers_exp(struct
rcu_report_exp_rnp(struct
rcu_bootup_announce(void)
implementation.\n");
rcu_bootup_announce_oddness();
blkd_tasks
rcu_preempt_note_context_switch(void)
t->rcu_blocked_node
list_add(&t->rcu_node_entry,
&t->rcu_node_entry;
rnp->gp_tasks;
*np;
rcu_read_unlock(),
drop_boost_mutex
t->rcu_read_unlock_special.b.need_qs
!rcu_preempted_readers_exp(rnp);
rt_mutex_unlock(&rnp->boost_mtx);
(!rcu_preempt_blocked_readers_cgp(rnp))
list_entry(rnp->gp_tasks,
list_for_each_entry_continue(t,
&rnp->blkd_tasks,
rcu_node_entry)
rcu_print_detail_task_stall_rnp(rnp);
rcu_print_task_stall_begin(struct
rcu_print_task_stall_end(void)
rnp's
rnp->blkd_tasks.next;
rcu_preempt_check_callbacks(void)
(!rcu_preempt_has_tasks(rnp))
rnp_up
synchronize_rcu_expedited(void)
ACCESS_ONCE(sync_rcu_preempt_exp_count)
(ULONG_CMP_LT(snap,
ACCESS_ONCE(sync_rcu_preempt_exp_count)))
Others
EXPORT_SYMBOL_GPL(synchronize_rcu_expedited);
rcu_barrier(void)
EXPORT_SYMBOL_GPL(rcu_barrier);
__rcu_init_preempt(void)
exit_rcu(void)
rcu_initiate_boost_trace(struct
rnp->n_balk_exp_gp_tasks++;
ULONG_CMP_LT(jiffies,
ACCESS_ONCE(rnp->boost_tasks)
tb
RCU_KTHREAD_RUNNING;
(spincnt
RCU_KTHREAD_YIELDING;
schedule_timeout_interruptible(2);
rnp->boost_kthread_task;
invoke_rcu_callbacks_kthread(void)
rcu_is_callbacks_kthread(void)
*workp
rcu_spawn_boost_kthreads(void)
(void)rcu_spawn_one_boost_kthread(rcu_state_p,
(rcu_scheduler_fully_active)
RCU_FAST_NO_HZ,
rcu_needs_cpu(unsigned
rcu_cleanup_after_idle(void)
rcu_prepare_for_idle(void)
rcu_idle_count_callbacks_posted(void)
rcu_prepare_for_idle()
energy-efficiency
energy
efficiency,
RCU_IDLE_LAZY_GP_DELAY
Roughly
cbs_ready
rdtp->nonlazy_posted_snap
rdtp->nonlazy_posted;
(rcu_try_advance_all_cbs())
tne;
rdtp->tick_nohz_enabled_snap
(rcu_is_nocb_cpu(smp_processor_id()))
rdtp->nonlazy_posted
rdtp->all_lazy
rcu_oom_notify
uncertain
Unconditionally
print_cpu_stall_fast_no_hz(char
0xffff,
stall-info
print_cpu_stall_info_begin(void)
rdp->dynticks;
ticks_title
ticks_value
kstat_softirqs_cpu(RCU_SOFTIRQ,
print_cpu_stall_info_end(void)
->ticks_this_gp
increment_cpu_stall_ticks(void)
rcu_nocb_poll
have_rcu_nocb_mask
CPUs'
nrq)
*rdp_leader
ACCESS_ONCE(rdp->nocb_head);
(!rhp)
ACCESS_ONCE(rdp->nocb_follower_head);
&rdp->nocb_q_count);
xchg(&rdp->nocb_tail,
&rdp->nocb_q_count_lazy);
(!irqs_disabled_flags(flags))
rdp->nocb_defer_wakeup
__call_rcu(),
(!rcu_is_nocb_cpu(rdp->cpu))
-atomic_long_read(&rdp->nocb_q_count_lazy),
-atomic_long_read(&rdp->nocb_q_count));
drown
"Poll"!
"Poll");
gotcbs
(!rdp->nocb_gp_head)
"WokeEmpty");
_sleep
scan.
(ACCESS_ONCE(rdp->nocb_head))
wait_again;
CBs,
xchg(&rdp->nocb_follower_tail,
enqueuing
ACCESS_ONCE(rdp->nocb_defer_wakeup);
rcu_nocb_kthread().
ndw
CONFIG_RCU_NOCB_CPU_NONE
(!have_rcu_nocb_mask)
CONFIG_RCU_NOCB_CPU_ZERO
0\n");
rcu_nocb_mask)
rdp->nocb_tail
flavor,
rdp_last
rdp_old_leader;
rdp_spawn)
rdp_last->nocb_next_follower
rdp->nocb_next_follower;
rcu_spawn_nocb_kthreads(void)
rcu_nocb_leader_stride
ls
ls;
adaptive-ticks
respond
RCU_SYSIDLE_FULL
full-system
nesting,
(irq)
atomic_inc(&rdtp->dynticks_idle);
newoldstate;
RCU_SYSIDLE_SHORT)
newoldstate
cmpxchg(&full_sysidle_state,
RCU_SYSIDLE_FULL_NOTED)
non-timekeeping
ACCESS_ONCE(full_sysidle_state)
rcu_sysidle_delay()))
(void)cmpxchg(&full_sysidle_state,
maxj)
*rdtp)
ACCESS_ONCE(current->rcu_tasks_idle_cpu)
_TRACE_TEST_H
h),
symname
module/symbol/section,
*symtab)
symtab=%p\n",
symtab);
memset(symtab,
sizeof(*symtab));
2.6
symbolsize
knt1)
symtab->sym_name,
ARRAY_SIZE(kdb_name_table);
sizeof(kdb_name_table[0])
debug_kfree(knt1);
kdb_name_table[i];
kdb_name_table[i]
prefix_len
((name
kdb_walk_kallsyms(&pos)))
(strncmp(name,
prefix_name,
symtab_p2
kdbnearsym(addr,
KDB_SP_VALUE)
KDB_SP_PAREN)
symtab_p2->sym_start);
probe_kernel_read((char
*)res,
(!KDB_STATE(SUPPRESS))
KDB_STATE_SET(SUPPRESS);
Physical
sizeof(*word))
treats
sizeof(word))
kdb_task_state_string
TASK_*
UNRUNNABLE
EXIT_*
IDLE
(*s)
TASK_STOPPED;
probe_kernel_read(&tmp,
long)))
kdb_process_cpu(p);
kdb_initial_cpu)
'I';
kdb_task_state_char(p),
&symtab,
fit.
dah_first_call
sizeof(debug_alloc_pool_aligned)
(h->size
h->next);
dah_overhead)
*)best
best->size;
best->next;
dah_used
sizeof(debug_alloc_pool_aligned))
h->size;
*)h
debug_alloc_pool;
(h_offset
prev_offset;
h_offset)
prev->size
prev->next;
h_free
*)h_free
h_free->size);
h_used->size,
h_used->caller);
BUG_ON(kdb_flags_index
kdb_flags;
Crude
P_ns(x)
P_ns
Device:
print_tickdevice(m,
%08lx\n",
iter->now);
(!iter->second_pass)
iter->second_pass
file->private;
move_iter(iter,
required)
(tmp->dev
present,
(!pm_vt_switch())
(orig_fgconsole
module_signature
Digest
signer_len;
key_id_len;
*pks;
desc_size;
hashing
operational
pks
sizeof(*pks)
*)pks
crypto_free_shash(tfm);
kfree(pks);
RSA
mpi;
((const
nbytes);
signer_len,
ERR_PTR(-ENOKEY);
modlen);
(modlen
modlen;
-ENOPKG;
ms.signer_len,
rq_sched_info_arrive(struct
rq->rq_sched_info.run_delay
rq_sched_info_depart(struct
rq_sched_info_dequeued(struct
schedstat_add(rq,
amt)
schedstat_set(var,
t->sched_info.last_queued
rq_clock(rq),
(t->sched_info.last_queued)
t->sched_info.last_queued;
sched_info_reset_dequeued(t);
t->sched_info.run_delay
involuntarily
sched_info_depart(rq,
sched_info_arrive(rq,
pci_dev
prev_overruns;
overrun_detected
prev_overruns
mmio_trace_array
pci_resource_to_user(dev,
&dev->resource[i],
&start,
%llx",
hiter->dev
pci_get_device(PCI_ANY_ID,
PCI_ANY_ID,
*hiter
destroy_header_iter(hiter);
"MARK
hiter->dev);
trace_mmiotrace_rw
*rw;
rw->width,
rw->value,
what?\n");
trace_mmiotrace_map
m->map_id,
*rw)
atomic_inc(&dropped_count);
mmio_trace_array;
Buffer
rcu_head;
page_order(struct
perf_callchain(struct
CONFIG_HAVE_PERF_USER_STACK_DUMP
arch_perf_have_user_stack_dump(void)
perf_user_stack_pointer(regs)
ctl_name;
CTL_UUID,
"overflowuid"
"overflowgid"
"warnings"
{},
"gc_thresh"
"max_size"
"gc_min_interval"
"gc_min_interval_ms"
"gc_timeout"
"gc_elasticity"
"mtu_expires"
"min_adv_mss"
"accept_redirects"
"accept_source_route"
NET_PROTO_CONF_ALL,
"all",
bin_net_neigh_vars_table
"neigh",
bin_net_neigh_table
"route",
"netfilter",
2088
"routing_control"
"restart_request_timeout"
"call_request_timeout"
"reset_request_timeout"
"clear_request_timeout"
"x25",
NET_LLC2,
"nf_conntrack_max"
lastp))
kmalloc(BUFSZ,
*end++
&str,
(isspace(*str))
(put_user(value,
(!isdigit(*str))
BUFSZ;
(get_user(value,
"%lu\t",
out_kfree:
buf[result]
__le16
dnaddr;
(__le16
le16_to_cpu(dnaddr)
ERR_PTR(-ENOTDIR);
binary_sysctl(const
out_putname;
nlen)
collision:
nlen);
__sysctl_args
(copy_from_user(&tmp,
(tmp.oldval
!tmp.oldlenp)
(tmp.oldlenp
get_user(oldlen,
tmp.oldlenp))
tmp.nlen,
tmp.newlen);
put_user(oldlen,
(compat_oldlenp
compat_oldlenp))
parking
million
RESERVED_PIDS
pid_max_min
find_next_offset(map,
BITS_PER_PAGE,
upid->nr;
MAXUINT)
MAXUINT
pid_ns->last_pid
'pid'
last_write
pid_max)
RESERVED_PIDS;
map->page
kmem_cache_free(ns->pid_cachep,
put_pid_ns(ns);
pid->level;
pid->numbers
free_pidmap(pid->numbers
*upid;
find_pid_ns(nr,
write-held.
&task->pids[type];
__change_pid(task,
pidhash_size;
max_t(int,
pid_max,
CAP_EMPTY_SET;
file_caps_enabled
name[sizeof(current->comm)];
pr_info_once("warning:
get_task_comm(name,
&header->version))
*tocopy
pEp,
pIp,
pPp);
@header:
effective,
inheritable
cap_user_header_t,
cap_user_data_t,
cap_validate_magic(header,
&tocopy);
((dataptr
(get_user(pid,
&header->pid))
__user_cap_data_struct
kdata[_KERNEL_CAPABILITY_U32S];
capget/modify/capset
effective.cap[i]
permitted.cap[i]
inheritable.cap[i]
(unaudited)
usernamespace
opener
"PCI
iomem_resource
(*alignf)(void
0x10000
width,
r->name
&resource_op);
spin_lock(&bootmem_resource_lock);
res->sibling
bootmem_resource_free;
bootmem_resource_free
spin_unlock(&bootmem_resource_lock);
tmp->start
new->sibling
&tmp->sibling;
(tmp->end
tmp->sibling;
tmp->end
request_resource_conflict
request_resource_conflict(root,
__release_resource(old);
sibling_only
res->start;
iomem_resource.child;
strcmp(p->name,
p->start)
(res->end
p->end)
ranges.
(*func)(u64,
(!find_next_iomem_res(&res,
(*func)(res.start,
res.end,
IORESOUCE_BUSY.
RAM",
root->child;
root->start;
this->sibling;
tmp.end
root->end;
tmp.start)
constraint->align);
alloc.start
new->end
__find_resource(root,
newsize,
new.start;
old->end
new.end;
&constraint);
((first->start
(first->end
new->end))
next->sibling)
next->sibling;
next->sibling
parent->child;
Inserts
(conflict
(conflict->end
res->parent;
parent->start)
parent->end))
resource's
@res:
Existing
alloc_resource(GFP_ATOMIC);
(!next_res)
next_res
(res->flags
alloc_resource(GFP_KERNEL);
IORESOURCE_MUXED)
&parent->child;
&res->child;
&res->sibling;
CONFIG_MEMORY_HOTREMOVE
new_res
res->start);
devm_release_resource()
"resource
reserved;
(get_option
r_next(NULL,
IORESOURCE_IO
attribute?
callchain_cpus_entries,
kfree(entries->cpu_entries[cpu]);
kfree(entries);
(!entries)
mutex_unlock(&callchain_mutex);
exit_put;
perf_callchain_store(entry,
tracing_start_function_trace(struct
tracing_stop_function_trace(struct
function_trace_call(unsigned
*pt_regs);
function_stack_trace_call(unsigned
TRACE_FUNC_OPT_STACK)
function_stack_trace_call;
op->private;
(unlikely(!tr->function_enabled))
(!old_count)
unlimited?
(old_count
update_traceon_count(data,
new_count
(new_count
(update_count(data))
ftrace_traceon_print,
ftrace_traceoff_print,
ftrace_stacktrace_print,
ftrace_trace_onoff_callback,
init_func_cmd_traceon(void)
MB;
KB;
stats->read_bytes
stats->write_bytes
stats->cancelled_write_bytes
tsk->acct_timexpd
tsk->acct_rss_mem1
tsk->acct_vm_mem1
__acct_update_integrals(tsk,
clocksource_delta(cycle_t
0600);
KDB_BASE_CMD_MAX;
"Command
BYTESPERWORD
value"),
full"),
KDBMSG(TOOMANYDBREGS,
"More
address"),
altered
KDB_ENVBUFSIZE
represntation
*ep;
integer-valued
simple_strtoul(arg,
simple_strtoull(arg,
argv[1]="var",
debugflags
simple_strtoul(argv[2],
argv[2]);
vallen
*)0)
argv[1],
(__env[i]
__env[i]
KDB_BADREG;
(preceded
*nextarg
unparsed
argv[]
KDB_NOPERM;
*)argv[*nextarg];
(symname[0]
symtab.sym_start;
((*nextarg
'\0'))
(argv[*nextarg][0]
diag)
kdb_defcmd,
kdb_defcmd2
kdb_defcmd
kdb_parse
kdb_exec_defcmd(int
defcmd_set_count
defcmd_in_progress
s->usage,
defcmd_set,
defcmd
++s)
s->count;
s->command[i]);
'pipe',
*cp2
kdb_grep_leading
tokenize
MAXARGC
cbuf;
(cpp
cbuf
CMD_BUFLEN)
kdb_printf("kdb_parse:
ignored\n%s\n",
*cpp++
*cp++;
argv[0][1]
for_each_kdbcmd(tp,
(tp->cmd_name)
(strncmp(argv[0],
tp->cmd_name,
md1c20
KDB_CMD_GO)
(cmd_head
strncpy(cmd_cur,
cmd_hist[cmdptr],
cmd_head)
kdb_reboot
(kdb_task_has_cpu(p))
kdb_local
kdb()
hardware-defined
fault/breakpoint.
1",
kdb_printf("\nEntering
kdb_current,
kdb_printf("on
4",
Entry\n");
NonMaskable
dismiss
cmd_cur;
snprintf(kdb_prompt_str,
(*cmdbuf
strncpy(cmd_hist[cmd_head],
cmd_cur,
kdb_cpu
mdr
<addr
arg>,<byte
[<addr
arg>
[<line
[<radix>]]]
symbolic,
nosect,
repeat,
kdb_printf(kdb_machreg_fmt0
(kdb_getphysword(&word,
bytesperword))
(kdb_getword(&word,
__c
(bytesperword)
last_addr;
last_repeat;
KDB_WORD_SIZE,
fmtstr[64];
&bytesperword);
"mdr")
last_bytesperword;
last_bytesperword
((repeat
last_repeat
"mds")
KDB_WORD_SIZE)
KDB_WORD_SIZE;
word)
kdb_md_line(fmtstr,
(z
(argv[0][2]
(nextarg
(kdb_continue_catastrophic
continue\n");
*rname;
reg32;
reg16;
reg8;
switch(dbg_reg_def[i].size
&reg8,
&reg16,
&reg32,
&reg64,
defined(CONFIG_MAGIC_SYSRQ)
defined(CONFIG_MODULES)
0x%p
kmsg_dump_rewind_nolock(&dumper);
(kmsg_dump_get_line_nolock(&dumper,
(lines
abs(lines);
first_print
(!first_print)
kdb_printf(",
kdb_printf("%d",
start_cpu);
i-1)
kdb_printf("-%d",
i-1);
kdb_printf("(%c)",
prev_state);
cpunum;
(int)(2*sizeof(void
*))+2,
kdb_printf("%-15.15s
(*endp)
mon_day[1]
val.uptime
(24*60*60);
((x)
%ld.%02ld
whichcpu
kdb_printf("%5d
\"pat
tern\"
\"^pat
kdb_register_flags
kdb_command_extend
*usage,
*help,
minlen,
(strcmp(kp->cmd_name,
sizeof(*new));
"<vaddr>
<bytes>",
Memory",
<contents>",
"Modify
KDB_ENABLE_ALWAYS_SAFE_NO_ARGS);
Registers",
KDB_ENABLE_MEM_READ);
KDB_ENABLE_INSPECT_NO_ARGS);
<pid>",
cpu",
kdb_help,
Message",
"Switch
kdb_init_lvl
lvl;
filterlist.
(unlikely(!entry))
field's
**bufp,
fields,
AUDIT_SYSCALL_CLASSES
(audit_match_class_bits(AUDIT_CLASS_SIGNAL,
listnr;
rule->flags
~AUDIT_FILTER_PREPEND;
(rule->action
AUDIT_ALWAYS)
entry->rule.flags
datasz)
*bufp;
(IS_ERR(entry))
data->buf;
data->field_count;
f->type
pr_warn("audit
\'%s\'
f->op);
(entry->rule.watch)
audit_rule_data.
(unlikely(!data))
data->fields[i]
lsm_str
old->field_count;
(!audit_compare_rule(&entry->rule,
&e->rule))
entry->rule.watch;
*tree
entry->rule.tree;
AUDIT_FILTER_USER
AUDIT_FILTER_TYPE)
audit_find_rule(entry,
audit_add_tree_rule()
(entry->rule.flags
AUDIT_FILTER_PREPEND)
&audit_rules_list[entry->rule.listnr]);
(!dont_count)
(!audit_match_signal(entry))
&audit_rules_list[i],
AUDIT_LIST_RULES,
skb_queue_tail(q,
(serial)
&entry->rule,
strlen(path);
slashes
((*p
path))
(pathlen
dlen)
parentlen;
audit_comparator(type,
unlock_and_return;
rt.c
preempt-rt
printk("..
printk(
task_pid_nr(task));
printk("\n%s/%d's
stackdump:\n\n",
work->flags
~IRQ_WORK_PENDING;
(!irq_work_claim(work))
(work->flags
arch_irq_work_raise();
this_cpu_ptr(&raised_list);
llnode
irq_work_run_list(this_cpu_ptr(&lazy_list));
UINSNS_PER_PAGE
copy_insn()
orig_ret_vaddr;
mangled
(is_register)
vma->vm_start
((loff_t)vma->vm_pgoff
vma->vm_mm;
vma->vm_mm,
mmun_start,
mmun_end);
is_swbp_insn
@insn
UPROBE_SWBP_INSN;
is_trap_insn
*kaddr
~PAGE_MASK),
uprobe_write_opcode
mm->mmap_sem
get_user_pages(NULL,
&opcode,
@auprobe:
@mm,
*auprobe,
uprobe_write_opcode(mm,
(l->inode
r->inode)
(l->offset
r->offset)
uprobes_tree.rb_node;
rb_entry(n,
atomic_inc(&uprobe->ref);
&uprobes_tree);
(access
kfree(uprobe);
uprobe->offset
@uc
(con
*mapping,
(offs
(test_bit(UPROBE_COPY_INSN,
&uprobe->flags))
(first_uprobe)
set_bit(MMF_HAS_UPROBES,
clear_bit(MMF_HAS_UPROBES,
uprobe_is_active()
GFP_NOWAIT
map_info),
info->vaddr
down_write(&mm->mmap_sem);
file_inode(vma->vm_file)
uprobe->inode)
install_breakpoint(uprobe,
(test_bit(MMF_HAS_UPROBES,
remove_breakpoint(uprobe,
uprobe_unregister
__uprobe_unregister(uprobe,
(WARN_ON(!uprobe))
uprobe->offset);
u->inode)
u->offset)
find_node_in_range(inode,
rb_entry(t,
(u->inode
u->offset
list_add(&u->pending_list,
(no_uprobe_events()
(!test_bit(MMF_HAS_UPROBES,
(vma_has_uprobes(vma,
*area)
(area->vaddr
area->bitmap);
kfree(area->bitmap);
kfree(area);
&newmm->flags);
slot_nr;
UINSNS_PER_PAGE);
UINSNS_PER_PAGE)
xol_vaddr
@regs:
t->utask;
ri->next;
uprobe_task),
return_instance),
uprobe_warn(current,
area");
uprobe_warn(t,
trampoline_vaddr;
get_utask();
(!ri)
orig_ret_vaddr
(orig_ret_vaddr
utask->return_instances
bp_vaddr)
bp_vaddr;
xol_free_insn_slot(current);
utask->active_uprobe
non-fatal
uprobe_unregister()
*uprobe
*is_swbp
need_prep
down_read(&uprobe->register_rwsem);
(uc->ret_handler)
up_read(&uprobe->register_rwsem);
_register
(utask->state
set_thread_flag(TIF_UPROBE);
conforms
(child->jobctl
task->state
@ignore_state
WARN_ON(child->state
release_task()
CAP_SYS_PTRACE);
dumpable
(same_thread_group(task,
__ptrace_may_access(task,
(seize)
long)PTRACE_O_MASK)
PT_PTRACED;
unlock_tasklist;
PT_SEIZED;
JOBCTL_TRAP_STOP
(!valid_signal(data))
child->exit_code
(copied)
(lock_task_sighand(child,
(likely(child->last_siginfo
(arg.flags
*uinfo
compat_ptr(data);
&info)
__put_user(info.si_code,
&uinfo->si_code))
PTRACE_SINGLESTEP
is_singlestep(request)
PTRACE_SINGLEBLOCK
is_singleblock(request)
is_sysemu_singlestep(request)
PTRACE_SYSEMU_SINGLESTEP)
set_tsk_thread_flag(child,
(need_siglock)
user_regset_view
regset
view,
regset_no,
kiov->iov_len,
kiov->iov_base);
datavp;
(request)
PTRACE_PEEKTEXT:
PTRACE_PEEKDATA:
PTRACE_POKETEXT:
PTRACE_POKEDATA:
PTRACE_GETEVENTMSG:
datalp);
PTRACE_GETSIGINFO:
ptrace_getsiginfo(child,
PTRACE_SETSIGINFO:
datavp,
ptrace_setsiginfo(child,
recalc_sigpending()
(unlikely(!seized
!lock_task_sighand(child,
LISTEN
put_user(tmp,
ptrace_resume(child,
PTRACE_GETREGSET:
PTRACE_SETREGSET:
kiov;
*uiov
uiov,
sizeof(*uiov)))
&uiov->iov_base)
&uiov->iov_len))
ptrace_regset(child,
&kiov);
__put_user(kiov.iov_len,
&uiov->iov_len);
PTRACE_TRACEME)
ptrace_traceme();
ptrace_get_task_struct(pid);
(IS_ERR(child))
PTR_ERR(child);
PTRACE_ATTACH
PTRACE_SEIZE)
ptrace_attach(child,
book-keeping
arch_ptrace_attach(child);
ptrace_check_attach(child,
PTRACE_KILL
PTRACE_INTERRUPT);
PTRACE_DETACH)
ptrace_unfreeze_traced(child);
out_put_task_struct:
(copied
sizeof(data),
access_process_vm(child,
datap);
compat_iovec
count'
list_empty(&sem->wait_list))
new_idmap_permitted(const
kmem_cache_free(user_ns_cachep,
ns->flags
id2
kuid,
from_kuid
@kuid:
kuid)
kgid.
from_kgid
@kgid:
kgid)
from_kprojid
kprojid
@kprojid:
kprojid)
projid;
upper_first
prev_upper_first
prev_lower_first
prev->count
extent?
extents.
((*ppos
kbuf[count]
kbuf;
next_line
(!isspace(*pos))
extent->lower_first
junk
((extent->first
extent->count)
new_map.nr_extents;
lower_first;
(cap_setid
(!(ns->flags
USERNS_SETGROUPS_ALLOWED)
(strncmp(pos,
setgroups_allowed
lockdep_stats
__debug_atomic_inc(ptr)
debug_atomic_inc(ptr)
debug_atomic_dec(ptr)
debug_atomic_read(ptr)
__total
51
Franklin
Fifth
Floor,
02110-1301
minimalistic
TRACE_BLK_OPT_CLASSIC)
blk_tracer_enabled;
blk_tr->trace_buffer.buffer;
record_it;
(!bt->rchan)
relay_reserve(bt->rchan,
t->magic
BLK_IO_TRACE_VERSION;
t->time
record_it:
t->device
bt->dev;
t->pid
t->cpu
t->pdu_len
sizeof(*t),
(unlikely(bt->trace_state
Blktrace_running
BLK_TC_SHIFT)
what)
(sector
pdu_len,
pdu_len);
(atomic_dec_and_test(&blk_probes_ref))
blk_unregister_tracepoints();
Blktrace_running)
simple_open,
kfree(msg);
*old_bt,
underscores
'_';
kzalloc(sizeof(*bt),
bt->msg_data
__alloc_percpu(BLK_TN_MAX_MSG,
__alignof__(char));
(!bt->msg_data)
(!blk_tree_root)
mutex_unlock(&blk_tree_mutex);
bt->dev
bt,
blk_trace_setup_lba(bt,
old_bt
old_bt);
(atomic_inc_return(&blk_probes_ref)
blk_register_tracepoints();
buts;
do_blk_trace_setup(q,
&buts);
(copy_to_user(arg,
defined(CONFIG_COMPAT)
defined(CONFIG_X86_64)
spin_lock_irq(&running_trace_lock);
spin_unlock_irq(&running_trace_lock);
bdev_get_queue(bdev);
bdevname(bdev,
bdev->bd_dev,
blk_trace_startstop(q,
oriented
@what:
rq->cmd_flags,
@bio:
BLK_TA_GETRQ,
BLK_TA_SLEEPRQ,
__be64
rpdu
sizeof(rpdu),
&rpdu);
BIO_UPTODATE),
@ignore:
used)
mapper
r.device_from
cpu_to_be32(dev);
r.device_to
r.sector_from
cpu_to_be64(from);
BLK_TA_REMAP,
sizeof(r),
BLK_TA_DRV_DATA,
*rwbs,
(t->action
BLK_TN_MESSAGE)
'R';
'A';
rwbs[i]
rwbs[RWBS_LEN];
fill_rwbs(rwbs,
"%3d,%-3d
%2s
%3s
MAJOR(t->device),
MINOR(t->device),
rwbs);
end--)
end++;
(t_action(ent)
BLK_TC_ACT(BLK_TC_PC))
blk_log_dump_pdu(s,
(t_sec(ent))
[%s]\n",
[%d]\n",
t->pdu_len);
(!(blk_tracer_flags.val
TRACE_BLK_OPT_CLASSIC))
"queue"
"requeue"
blk_log_with_error
"issue"
"complete"
blk_log_unplug
long_act
log_action(iter,
print_one_line(iter,
free_bt;
sysfs_blk_trace_attr_show(struct
sysfs_blk_trace_attr_store(struct
ARRAY_SIZE(mask_maps);
dev_to_part(dev);
*bdev;
bdev
bdget(part_devt(p));
(bdev
blk_trace_get_queue(bdev);
out_bdput;
&dev_attr_enable)
out_unlock_bdev;
(q->blk_trace
&dev_attr_pid)
&dev_attr_start_lba)
&dev_attr_end_lba)
out_unlock_bdev:
out_bdput:
bdput(bdev);
&value)
blk_trace_setup_queue(q,
&blk_trace_attr_group);
*pd,
target_cpu;
pd->cpumask.pcpu);
cpu_index
cpumask_weight(pd->cpumask.pcpu);
padata_index_to_cpu(pd,
LIST_HEAD(local_list);
pd->pinst;
&local_list);
(!list_empty(&local_list))
list_entry(local_list.next,
padata_do_parallel
@padata:
BHs
(!(pinst->flags
PADATA_INVALID)
PADATA_RESET))
cb_cpu;
pinst->wq,
num_cpus;
padata_list
next_nr
*pinst
del_timer(&pd->timer);
spin_unlock_bh(&pd->lock);
spin_lock(&squeue->serial.lock);
spin_unlock(&squeue->serial.lock);
padata_do_serial
free_cpumask_var(pd->cpumask.cbcpu);
pd->cpumask.cbcpu)
pd->cpumask.pcpu)
pinst;
free_percpu(pd->squeue);
free_percpu(pd->pqueue);
kfree(pd);
rcu_assign_pointer(pinst->pd,
PADATA_CPU_PARALLEL;
padata_register_cpumask_notifier
*nblock)
nblock);
cpumask_copy(pinst->cpumask.pcpu,
cpumask_copy(pinst->cpumask.cbcpu,
@pcpumask:
@cbcpumask:
__padata_set_cpumasks(pinst,
serial_mask
pinst->cpumask.cbcpu;
parallel_mask
pinst->cpumask.pcpu;
pinst->cpumask.pcpu,
pinst->cpumask.cbcpu))
(PADATA_CPU_SERIAL
PADATA_CPU_PARALLEL)))
PADATA_CPU_SERIAL)
PADATA_CPU_PARALLEL)
pinst->cpumask.pcpu);
__padata_add_cpu(pinst,
(!padata_validate_cpumask(pinst,
!padata_validate_cpumask(pinst,
__padata_remove_cpu(pinst,
padata_instance,
(!pinst_has_cpu(pinst,
notifier_from_errno(err);
free_cpumask_var(pinst->cpumask.cbcpu);
kfree(pinst);
kobj)
attr)
_store_name)
show_cpumask,
store_cpumask);
[RW]
*pentry;
pentry
attr2pentry(attr);
(pentry->show)
sysfs_ops
err_free_inst;
err_free_masks;
FULLSTOP_RMMOD;
online-offline
n_offline_attempts;
maxcpu
(min_offline
max_offline
(min_online
max_online
TORTURE_RANDOM_MULT
shuffling.
interactions
&shuffle_task_list,
st_l)
Shuffle
FULLSTOP_SHUTDOWN)
title);
jiffies_snap;
VERBOSE_TOROUT_STRING("torture_shutdown
jiffies_snap
shutdown_time
shutdown_task
pr_warn("Concurrent
illegal!\n");
(ACCESS_ONCE(stutter_pause_test)
torture_shutdown_absorb(title);
FULLSTOP_DONTSTOP;
stop?
**tp)
VERBOSE_TOROUT_STRING(m);
kp->name);
*doing,
*doing))
num_params;
pr_debug("doing
(args[i]
Chew
(*args)
**)kp->arg
*(bool
kp;
kp.arg
(*num
arguments\n",
BUG_ON(!mutex_is_locked(&param_lock));
kp->arr;
(arr->num
*arr->num
arr->max);
arr->elemsize
kparam_string
*kps
kp->str;
kp->name,
module_param_attrs
*attribute
to_param_attr(mattr);
attribute->param);
@kparam:
(!mk->mp)
mk->mp->grp.attrs
(mk->mp->num
mk->mp->attrs[mk->mp->num].mattr.store
(mk->mp)
&mod->mkobj.mp->grp);
sysfs_create_file(&mk->kobj,
&mk->mp->grp);
BUG_ON(err);
"module"
name_len
name_len);
*vattr
*attribute;
to_module_attr(attr);
mk,
kset
init_irq_default_affinity(void)
alloc_masks(struct
free_cpumask_var(desc->irq_data.affinity);
desc_smp_init(struct
desc_node(struct
desc->owner
*irq_to_desc(unsigned
EXPORT_SYMBOL(irq_to_desc);
free_masks(struct
desc->kstat_irqs
&irq_desc_lock_class);
desc_set_defaults(irq,
free_percpu(desc->kstat_irqs);
free_desc(unsigned
alloc_descs(unsigned
bitmap_clear(allocated_irqs,
irq_expand_nr_irqs(unsigned
first_online_node;
init_irq_default_affinity();
initcnt
IRQ_BITMAP_BITS))
IRQ_BITMAP_BITS;
initcnt;
arch_early_irq_init();
bitmap_set(allocated_irqs,
irq_set_status_flags(i,
arch_teardown_hwirq(i);
(bus)
Repeat
(!context_tracking_is_enabled())
rcu_user_exit()
(__this_cpu_read(context_tracking.active))
CONTEXT_USER)
__this_cpu_write(context_tracking.state,
tracking.
TIF_NOHZ);
